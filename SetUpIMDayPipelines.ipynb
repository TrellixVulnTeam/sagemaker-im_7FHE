{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a Helper file for SageMaker Pipelines Immersion day run\n",
    "This avoids any copy paste issues. To run this file do the following steps\n",
    "\n",
    "    Copy this file into the Build repository after it has been cloned.\n",
    "    Make sure to copy at the same location as where the codebuild-buildspec.yml file is\n",
    "    Change the directory to customer_churn as mentioned in the Immersion day\n",
    "    Run the cells to create the correct versions of the files\n",
    "    IMPORTANT: Before runing the cell having the file pipeline.py REPLACE the value with the S3 location obtained in the cell just above that \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codebuild-buildspec.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile codebuild-buildspec.yml\n",
    "\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      python: 3.8\n",
    "    commands:\n",
    "      - pip install --upgrade --force-reinstall . \"awscli>1.20.30\"\n",
    "  \n",
    "  build:\n",
    "    commands:\n",
    "      - export PYTHONUNBUFFERED=TRUE\n",
    "      - export SAGEMAKER_PROJECT_NAME_ID=\"${SAGEMAKER_PROJECT_NAME}-${SAGEMAKER_PROJECT_ID}\"\n",
    "      - |\n",
    "        run-pipeline --module-name pipelines.customer_churn.pipeline \\\n",
    "          --role-arn $SAGEMAKER_PIPELINE_ROLE_ARN \\\n",
    "          --tags \"[{\\\"Key\\\":\\\"sagemaker:project-name\\\", \\\"Value\\\":\\\"${SAGEMAKER_PROJECT_NAME}\\\"}, {\\\"Key\\\":\\\"sagemaker:project-id\\\", \\\"Value\\\":\\\"${SAGEMAKER_PROJECT_ID}\\\"}]\" \\\n",
    "          --kwargs \"{\\\"region\\\":\\\"${AWS_REGION}\\\",\\\"sagemaker_project_arn\\\":\\\"${SAGEMAKER_PROJECT_ARN}\\\",\\\"role\\\":\\\"${SAGEMAKER_PIPELINE_ROLE_ARN}\\\",\\\"default_bucket\\\":\\\"${ARTIFACT_BUCKET}\\\",\\\"pipeline_name\\\":\\\"${SAGEMAKER_PROJECT_NAME_ID}\\\",\\\"model_package_group_name\\\":\\\"${SAGEMAKER_PROJECT_NAME_ID}\\\",\\\"base_job_prefix\\\":\\\"${SAGEMAKER_PROJECT_NAME_ID}\\\"}\"\n",
    "      - echo \"Create/Update of the SageMaker Pipeline and execution completed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipelines/customer_churn/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipelines/customer_churn/preprocess.py\n",
    "\n",
    "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
    "# may not use this file except in compliance with the License. A copy of\n",
    "# the License is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is\n",
    "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
    "# ANY KIND, either express or implied. See the License for the specific\n",
    "# language governing permissions and limitations under the License.\n",
    "\"\"\"Feature engineers the customer churn dataset.\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)\n",
    "    input_data = args.input_data\n",
    "    print(input_data)\n",
    "    bucket = input_data.split(\"/\")[2]\n",
    "    key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "    fn = f\"{base_dir}/data/raw-data.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, fn)\n",
    "\n",
    "    logger.info(\"Reading downloaded data.\")\n",
    "\n",
    "    # read in csv\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    # drop the \"Phone\" feature column\n",
    "    df = df.drop([\"Phone\"], axis=1)\n",
    "\n",
    "    # Change the data type of \"Area Code\"\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "    # Drop several other columns\n",
    "    df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "    # Convert categorical variables into dummy/indicator variables.\n",
    "    model_data = pd.get_dummies(df)\n",
    "\n",
    "    # Create one binary classification target column\n",
    "    model_data = pd.concat(\n",
    "        [\n",
    "            model_data[\"Churn?_True.\"],\n",
    "            model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Split the data\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        model_data.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(model_data)), int(0.9 * len(model_data))],\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(train_data).to_csv(\n",
    "        f\"{base_dir}/train/train.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(validation_data).to_csv(\n",
    "        f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test_data).to_csv(\n",
    "        f\"{base_dir}/test/test.csv\", header=False, index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n",
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "prefix = 'sagemaker/DEMO-xgboost-churn'\n",
    "region = boto3.Session().region_name\n",
    "default_bucket = sagemaker.session.Session().default_bucket()\n",
    "RawData = boto3.Session().resource('s3')\\\n",
    ".Bucket(default_bucket).Object(os.path.join(prefix, 'data/RawData.csv'))\\\n",
    ".upload_file('./churn.txt')\n",
    "print(os.path.join(\"s3://\",default_bucket, prefix, 'data/RawData.csv'))\n",
    "\n",
    "input_data_url_val = os.path.join(\"s3://\",default_bucket, prefix, 'data/RawData.csv')\n",
    "\n",
    "print(input_data_url_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "Note the value of the s3 url and replace where it says input_data = ParameterString with this value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipelines/customer_churn/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipelines/customer_churn/pipeline.py\n",
    "\n",
    "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
    "# may not use this file except in compliance with the License. A copy of\n",
    "# the License is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is\n",
    "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
    "# ANY KIND, either express or implied. See the License for the specific\n",
    "# language governing permissions and limitations under the License.\n",
    "\"\"\"Example workflow pipeline script for CustomerChurn pipeline.\n",
    "                                               . -RegisterModel\n",
    "                                              .\n",
    "    Process-> Train -> Evaluate -> Condition .\n",
    "                                              .\n",
    "                                               . -(stop)\n",
    "Implements a get_pipeline(**kwargs) method.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "\n",
    "def get_session(region, default_bucket):\n",
    "    \"\"\"Gets the sagemaker session based on the region.\n",
    "    Args:\n",
    "        region: the aws region to start the session\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "    return sagemaker.session.Session(\n",
    "        boto_session=boto_session,\n",
    "        sagemaker_client=sagemaker_client,\n",
    "        sagemaker_runtime_client=runtime_client,\n",
    "        default_bucket=default_bucket,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    sagemaker_project_arn=None,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"CustomerChurnPackageGroup\",  # Choose any name\n",
    "    pipeline_name=\"CustomerChurnDemo-p-ewf8t7lvhivm\",  # You can find your pipeline name in the Studio UI (project -> Pipelines -> name)\n",
    "    base_job_prefix=\"CustomerChurn\",  # Choose any name\n",
    "):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with on CustomerChurn data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "    sagemaker_session = get_session(region, default_bucket)\n",
    "    if role is None:\n",
    "        role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "\n",
    "    # Parameters for pipeline execution\n",
    "    processing_instance_count = ParameterInteger(\n",
    "        name=\"ProcessingInstanceCount\", default_value=1\n",
    "    )\n",
    "    processing_instance_type = \"ml.m5.xlarge\"\n",
    "    training_instance_type = \"ml.m5.xlarge\"\n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )\n",
    "    input_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value='s3://sagemaker-us-east-1-470200736873/sagemaker/DEMO-xgboost-churn/data/RawData.csv',  \n",
    "        # Change this to point to the s3 location of your raw input data.\n",
    "    )\n",
    "\n",
    "    # Processing step for feature engineering\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"0.23-1\",\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=processing_instance_count,\n",
    "        base_job_name=f\"{base_job_prefix}/sklearn-CustomerChurn-preprocess\",  # choose any name\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    step_process = ProcessingStep(\n",
    "        name=\"CustomerChurnProcess\",  # choose any name\n",
    "        processor=sklearn_processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\", source=\"/opt/ml/processing/validation\"\n",
    "            ),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ],\n",
    "        code=os.path.join(BASE_DIR, \"preprocess.py\"),\n",
    "        job_arguments=[\"--input-data\", input_data],\n",
    "    )\n",
    "\n",
    "    # Training step for generating model artifacts\n",
    "    model_path = f\"s3://{sagemaker_session.default_bucket()}/{base_job_prefix}/CustomerChurnTrain\"\n",
    "    image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",  # we are using the Sagemaker built in xgboost algorithm\n",
    "        region=region,\n",
    "        version=\"1.0-1\",\n",
    "        py_version=\"py3\",\n",
    "        instance_type=training_instance_type,\n",
    "    )\n",
    "    xgb_train = Estimator(\n",
    "        image_uri=image_uri,\n",
    "        instance_type=training_instance_type,\n",
    "        instance_count=1,\n",
    "        output_path=model_path,\n",
    "        base_job_name=f\"{base_job_prefix}/CustomerChurn-train\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    xgb_train.set_hyperparameters(\n",
    "        objective=\"binary:logistic\",\n",
    "        num_round=50,\n",
    "        max_depth=5,\n",
    "        eta=0.2,\n",
    "        gamma=4,\n",
    "        min_child_weight=6,\n",
    "        subsample=0.7,\n",
    "        silent=0,\n",
    "    )\n",
    "    step_train = TrainingStep(\n",
    "        name=\"CustomerChurnTrain\",\n",
    "        estimator=xgb_train,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Processing step for evaluation\n",
    "    script_eval = ScriptProcessor(\n",
    "        image_uri=image_uri,\n",
    "        command=[\"python3\"],\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=1,\n",
    "        base_job_name=f\"{base_job_prefix}/script-CustomerChurn-eval\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    "    )\n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\",\n",
    "    )\n",
    "    step_eval = ProcessingStep(\n",
    "        name=\"CustomerChurnEval\",\n",
    "        processor=script_eval,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"\n",
    "            ),\n",
    "        ],\n",
    "        code=os.path.join(BASE_DIR, \"evaluate.py\"),\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "\n",
    "    # Register model step that will be conditionally executed\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "                    \"S3Uri\"\n",
    "                ]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Register model step that will be conditionally executed\n",
    "    step_register = RegisterModel(\n",
    "        name=\"CustomerChurnRegisterModel\",\n",
    "        estimator=xgb_train,\n",
    "        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=model_package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "\n",
    "    # Condition step for evaluating model quality and branching execution\n",
    "    cond_lte = ConditionGreaterThanOrEqualTo(  # You can change the condition here\n",
    "        left=JsonGet(\n",
    "            step=step_eval,\n",
    "            property_file=evaluation_report,\n",
    "            json_path=\"binary_classification_metrics.accuracy.value\",  # This should follow the structure of your report_dict defined in the evaluate.py file.\n",
    "        ),\n",
    "        right=0.8,  # You can change the threshold here\n",
    "    )\n",
    "    step_cond = ConditionStep(\n",
    "        name=\"CustomerChurnAccuracyCond\",\n",
    "        conditions=[cond_lte],\n",
    "        if_steps=[step_register],\n",
    "        else_steps=[],\n",
    "    )\n",
    "\n",
    "    # Pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            model_approval_status,\n",
    "            input_data,\n",
    "        ],\n",
    "        steps=[step_process, step_train, step_eval, step_cond],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipelines/customer_churn/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipelines/customer_churn/evaluate.py\n",
    "\n",
    "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
    "# may not use this file except in compliance with the License. A copy of\n",
    "# the License is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is\n",
    "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
    "# ANY KIND, either express or implied. See the License for the specific\n",
    "# language governing permissions and limitations under the License.\n",
    "\"\"\"Evaluation script for measuring model accuracy.\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# May need to import additional metrics depending on what you are measuring.\n",
    "# See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\"..\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    print(\"Loading test input data\")\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Creating classification evaluation report\")\n",
    "    acc = accuracy_score(y_test, predictions.round())\n",
    "    auc = roc_auc_score(y_test, predictions.round())\n",
    "\n",
    "    # The metrics reported can change based on the model used, but it must be a specific name per (https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": acc,\n",
    "                \"standard_deviation\": \"NaN\",\n",
    "            },\n",
    "            \"auc\": {\"value\": auc, \"standard_deviation\": \"NaN\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"Classification report:\\n{}\".format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join(\n",
    "        \"/opt/ml/processing/evaluation\", \"evaluation.json\"\n",
    "    )\n",
    "    print(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done\n"
     ]
    }
   ],
   "source": [
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
