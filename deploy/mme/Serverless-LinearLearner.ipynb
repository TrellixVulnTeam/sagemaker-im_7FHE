{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Serverless using Linear Learner\n",
    "With [Amazon SageMaker Serverless ](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html), which is a purpose-built inference option that makes it easy for you to deploy and scale ML models. Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling.\n",
    "\n",
    "With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or unpredictable traffic pattern. During times when there are no requests, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs. \n",
    "\n",
    "You can integrate Serverless Inference with your MLOps Pipelines to streamline your ML workflow, and you can use a serverless endpoint to host a model registered with Model Registry.\n",
    "\n",
    "This notebook showcases these capabilities\n",
    "\n",
    "To demonstrate these capabilities, the notebook discusses the use case of predicting house prices in multiple cities using linear regression.  House prices are predicted based on features like number of bedrooms, number of garages, square footage etc.  Depending on the city, the features affect the house price differently.  For example, small changes in the square footage cause a drastic change in house prices in New York when compared to price changes in Houston.  For accurate house price predictions, we will train multiple linear regression models, a unique location specific model per city.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Generate synthetic data for housing models](#Generate-synthetic-data-for-housing-models)\n",
    "1. [Preprocess the raw housing data using Scikit Learn model](#Preprocess-synthetic-housing-data-using-scikit-learn)\n",
    "1. [Train multiple house value prediction models for multiple cities](#Train-multiple-house-value-prediction-models)\n",
    "1. [Create model entity with multi model support](#Create-sagemaker-multi-model-support)\n",
    "1. [Create an inference pipeline with sklearn model and MME linear learner model](#Create-inference-pipeline)\n",
    "1. [Exercise the inference pipeline - Get predictions from the different  linear learner models](#Exercise-inference-pipeline)\n",
    "1. [Update Multi Model Endpoint with new models](#update-models)\n",
    "1. [Explore granular access to the target models of MME](#Finegrain-control-invoke-models)\n",
    "1. [Endpoint CloudWatch Metrics Analysis](#CW-metric-analysis)\n",
    "1. [Clean up](#CleanUp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 1 - Generate synthetic data for housing models <a id='Generate-synthetic-data-for-housing-models'></a>\n",
    "\n",
    "In this section, you will generate synthetic data that will be used to train the linear learner models.  The data generated consists of 6 numerical features - the year the house was built in, house size in square feet, number of bedrooms, number of bathroom, the lot size and number of garages and two categorial features - deck and front_porch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from random import choice\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "from sagemaker.multidatamodel import MULTI_MODEL_CONTAINER_MODE\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    \"\"\"Generate price based on features of the house\"\"\"\n",
    "    \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        garage = 1\n",
    "    else:\n",
    "        garage = 0\n",
    "        \n",
    "    if house['FRONT_PORCH'] == 'y':\n",
    "        front_porch = 1\n",
    "    else:\n",
    "        front_porch = 0\n",
    "        \n",
    "    price = int(150 * house['SQUARE_FEET'] + \\\n",
    "                10000 * house['NUM_BEDROOMS'] + \\\n",
    "                15000 * house['NUM_BATHROOMS'] + \\\n",
    "                15000 * house['LOT_ACRES'] + \\\n",
    "                10000 * garage + \\\n",
    "                10000 * front_porch + \\\n",
    "                15000 * house['GARAGE_SPACES'] - \\\n",
    "                5000 * (MAX_YEAR - house['YEAR_BUILT']))\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_yes_no():\n",
    "    \"\"\"Generate values (y/n) for categorical features\"\"\"\n",
    "    answer = choice(['y', 'n'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    \"\"\"Generate a row of data (single house information)\"\"\"\n",
    "    house = {'SQUARE_FEET':    np.random.normal(3000, 750),\n",
    "             'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "             'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "             'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "             'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "             'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "             'FRONT_PORCH':   gen_yes_no(),\n",
    "             'DECK':          gen_yes_no()\n",
    "            }\n",
    "    \n",
    "    price = gen_price(house)\n",
    "    \n",
    "    return [house['YEAR_BUILT'],   \n",
    "            house['SQUARE_FEET'], \n",
    "            house['NUM_BEDROOMS'], \n",
    "            house['NUM_BATHROOMS'], \n",
    "            house['LOT_ACRES'],    \n",
    "            house['GARAGE_SPACES'],\n",
    "            house['FRONT_PORCH'],    \n",
    "            house['DECK'], \n",
    "            price]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    \"\"\"Generate housing dataset\"\"\"\n",
    "    house_list = []\n",
    "    \n",
    "    for _ in range(num_houses):\n",
    "        house_list.append(gen_random_house())\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        house_list, \n",
    "        columns=[\n",
    "            'YEAR_BUILT',    \n",
    "            'SQUARE_FEET',  \n",
    "            'NUM_BEDROOMS',            \n",
    "            'NUM_BATHROOMS',\n",
    "            'LOT_ACRES',\n",
    "            'GARAGE_SPACES',\n",
    "            'FRONT_PORCH',\n",
    "            'DECK', \n",
    "            'PRICE']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, test): \n",
    "    \"\"\"Save the housing data locally\"\"\"\n",
    "    os.makedirs('data/{0}/train'.format(location), exist_ok=True)\n",
    "    train.to_csv('data/{0}/train/train.csv'.format(location), sep=',', header=False, index=False)\n",
    "       \n",
    "    os.makedirs('data/{0}/test'.format(location), exist_ok=True)\n",
    "    test.to_csv('data/{0}/test/test.csv'.format(location), sep=',', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate housing data for multiple locations.\n",
    "#Change \"PARALLEL_TRAINING_JOBS \" to a lower number to limit the number of training jobs and models. Or to a higher value to experiment with more models.\n",
    "\n",
    "PARALLEL_TRAINING_JOBS = 1\n",
    "\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "    houses = gen_houses(NUM_HOUSES_PER_LOCATION)\n",
    "    \n",
    "    #Spliting data into train and test in 90:10 ratio\n",
    "    #Not splitting the train data into train and val because its not preprocessed yet\n",
    "    train, test = train_test_split(houses, test_size=0.1)\n",
    "    save_data_locally(loc, train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the first few lines of data.\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2a ) Bring your own Model as tar ball in S3 <a id='Bring-your-own-model-as-tarball'></a>\n",
    "Here we will use the Tar ball as is and then create all the required artifacts from scratch\n",
    "    First we upload the Model tar ball to S3 to be used in our Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET :  sagemaker-us-east-1-622343165275\n",
      "ROLE :  arn:aws:iam::622343165275:role/service-role/AmazonSageMaker-ExecutionRole-20220208T115633\n"
     ]
    }
   ],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "BUCKET  = sagemaker_session.default_bucket()\n",
    "print(\"BUCKET : \", BUCKET)\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"ROLE : \", role)\n",
    "\n",
    "ACCOUNT_ID = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION = boto3.Session().region_name\n",
    "\n",
    "DATA_PREFIX = 'DEMO_MME_LINEAR_LEARNER'\n",
    "HOUSING_MODEL_NAME = 'housing'\n",
    "MULTI_MODEL_ARTIFACTS = 'multi_model_artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the RAW data set to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL Test data uploaded to :  s3://sagemaker-us-east-1-622343165275/data/realtime/transform/use_for_test.csv\n"
     ]
    }
   ],
   "source": [
    "#Upload the raw training data to S3 bucket, to be accessed by SKLearn\n",
    "test_inputs = []\n",
    "PARALLEL_TRAINING_JOBS=1\n",
    "for loc in LOCATIONS[:PARALLEL_TRAINING_JOBS]:\n",
    "\n",
    "    test_input = sagemaker_session.upload_data(\n",
    "        path='./data/transform_output/use_for_test.csv'.format(loc),\n",
    "        bucket=BUCKET,\n",
    "        key_prefix='data/realtime/transform'\n",
    "    )\n",
    "    \n",
    "    test_inputs.append(test_input)\n",
    "    print(\"FINAL Test data uploaded to : \", test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Model as tar ball into S3 location for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/byom/scikit-learnestimator/model/realtime\n",
      "s3://sagemaker-us-east-1-622343165275/byom/scikit-learnestimator/model/realtime/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# - UPLOAD the MODEL to S3\n",
    "desired_model_s3 = 's3://{}/{}'.format(sagemaker_session.default_bucket(),'byom/scikit-learnestimator/model/realtime')\n",
    "print(desired_model_s3)\n",
    "model_s3_upload=sagemaker.s3.S3Uploader().upload(local_path='./models/realtime_linearlearner/model.tar.gz', desired_s3_uri=desired_model_s3,sagemaker_session=sagemaker_session)  \n",
    "print(model_s3_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the REAL TIME Jobs:\n",
    "\n",
    "The steps to create the Job is straight forward\n",
    "* Create a Model object from the S3 location with the Image\n",
    "* Create a Model and a Real Time Serverless \n",
    "* Create a Serverless End Point\n",
    "* Run the Metrics test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Container image\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework=\"sklearn\", version=\"0.20.0\") # 0.23-1\"\n",
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='linear-learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the model from s3=s3://sagemaker-us-east-1-622343165275/byom/scikit-learnestimator/model/realtime/model.tar.gz:\n"
     ]
    }
   ],
   "source": [
    "print(f\"using the model from s3={model_s3_upload}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<sagemaker.model.Model object at 0x7fb1836e3370>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "linear_model_name = \"DEMO-SERVERLESS-LINEAR-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "linear_model = Model(\n",
    "    name=linear_model_name,\n",
    "    model_data=model_s3_upload,  \n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    #entry_point=\"scripts/sklearn_preprocessor_batch.py\",\n",
    "    #framework_version=\"0.20.0\", #\"0.23-1\", #\"0.20.0\",\n",
    "    image_uri=container,\n",
    "    #source_dir=\"scripts\",\n",
    ")\n",
    "print(linear_model.source_dir)\n",
    "print(linear_model.entry_point)\n",
    "print(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Predictor\n",
    "we will use the predictor object to run our inference on the end points and \n",
    "also DEPOY the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "csv_serializer = sagemaker.serializers.CSVSerializer()\n",
    "csv_deserializer = sagemaker.deserializers.CSVDeserializer()\n",
    "\n",
    "\n",
    "serverless_config = sagemaker.serverless.ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=4096, \n",
    "    max_concurrency=3\n",
    ")\n",
    "\n",
    "predictor = linear_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=csv_serializer,\n",
    "    deserializer=csv_deserializer,\n",
    "    endpoint_name=linear_model_name, # use the same name \n",
    "    wait=True,\n",
    "    #async_inference_config=None,\n",
    "    serverless_inference_config=serverless_config,\n",
    ")\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Test Data set\n",
    "To be used to test for our Serverless interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.744936</td>\n",
       "      <td>-0.831786</td>\n",
       "      <td>-0.009418</td>\n",
       "      <td>-1.420279</td>\n",
       "      <td>0.106776</td>\n",
       "      <td>-1.312254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.136893</td>\n",
       "      <td>0.188842</td>\n",
       "      <td>-1.422126</td>\n",
       "      <td>-1.420279</td>\n",
       "      <td>-0.045943</td>\n",
       "      <td>0.451792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5    6    7    8  \\\n",
       "0 -0.744936 -0.831786 -0.009418 -1.420279  0.106776 -1.312254  1.0  0.0  1.0   \n",
       "1 -1.136893  0.188842 -1.422126 -1.420279 -0.045943  0.451792  0.0  1.0  1.0   \n",
       "\n",
       "     9  \n",
       "0  0.0  \n",
       "1  0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv('./data/transform_output/use_for_test.csv', header=None) # Has the Price column Removed -- ONLY Features \n",
    "print(test_data_df.shape)\n",
    "test_data_df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value_serverless(features, predictor_to_use):\n",
    "    #print('SERVERLESS:to predict price of this house: {}'.format(features))\n",
    "    body = ','.join(map(str, features)) + '\\n'\n",
    "    start_time = time.time()\n",
    "     \n",
    "    response = predictor_to_use.predict(features)\n",
    "    #response_json = json.loads(response)\n",
    "    #predicted_value = response_json['predictions'][0]['score']   \n",
    "    predicted_value = float(response[0][0])\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print('SEVERLESS:Price:of:house:${:.2f}, took {:,d} ms\\n'.format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke 1st time\n",
    "we will see the time to invoke 1st time will be much longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor:created:=<sagemaker.predictor.Predictor object at 0x7fb181ef4220>:\n"
     ]
    }
   ],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer,\n",
    "    deserializer=csv_deserializer,\n",
    "    endpoint_name=linear_model_name, # use the same name \n",
    "\n",
    ")\n",
    "print(f\"Predictor:created:={predictor}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEVERLESS:Price:of:house:$273472.94, took 103 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cell 15\n",
    "# INVOKE the Predictor for the 1st time\n",
    "\n",
    "predict_one_house_value_serverless(\n",
    "    test_data_df.values.tolist()[0],  \n",
    "    predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now call it for the rest of all 900 rows to see the values\n",
    "we will iterate and send all 900 rows in but 1 by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEVERLESS:Price:of:house:$273472.94, took 86 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$394512.62, took 48 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$443970.53, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$651414.62, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$358996.16, took 45 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$458998.72, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$339078.88, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$305665.94, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$596214.25, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$242778.41, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$434828.34, took 54 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$573561.00, took 45 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$274597.91, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$396197.97, took 595 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$387287.69, took 53 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$533519.12, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$187328.31, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$579758.44, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$486177.62, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$483680.66, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$441660.66, took 37 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$475851.41, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$453352.53, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$387209.56, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$618228.62, took 46 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$457132.56, took 667 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$640030.69, took 51 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$622637.44, took 45 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$525288.31, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$242597.45, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$542602.38, took 51 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$325793.19, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$408599.88, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$234835.97, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$310207.38, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$257306.88, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$390752.69, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$402483.47, took 46 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$449262.72, took 982 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$308879.94, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$628439.50, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$220137.12, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$462059.50, took 51 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$562241.31, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$472224.34, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$438311.34, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$570243.88, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$705674.62, took 47 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$497707.03, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$651868.44, took 61 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$610175.88, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$436696.22, took 997 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$536979.56, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$717770.00, took 45 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$482160.38, took 62 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$614858.00, took 50 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$644951.50, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$575607.44, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$357767.66, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$452281.28, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$371354.91, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$511984.03, took 36 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$651048.50, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$335039.31, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$331901.25, took 780 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$354281.84, took 60 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$559598.75, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$361568.84, took 52 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$452559.84, took 37 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$330268.94, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$544143.69, took 47 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$283463.06, took 49 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$461367.53, took 46 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$660761.25, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$560439.00, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$181267.69, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$452529.31, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$257819.50, took 823 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$329288.66, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$528752.88, took 49 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$309406.16, took 42 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$690584.12, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$321368.06, took 141 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$312909.31, took 40 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$370569.41, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$166371.88, took 46 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$770863.62, took 43 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$424380.59, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$407474.00, took 44 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$429067.59, took 39 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for one_row in test_data_df.values.tolist():\n",
    "    predict_one_house_value_serverless(one_row,  predictor)\n",
    "    time.sleep(0.005) # MINIMUM time to sleep -- so we should see the scale up of the serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This completes the serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Invoke using the Boto3 API model <a id='invoke-boto3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Boto3 client and invoke the end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 273472.9375}]}\n",
      "The Serverless:returned:Price:=273472.9375\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"text/csv\"\n",
    "features = test_data_df.values.tolist()[0]\n",
    "payload = ','.join(map(str, features)) + '\\n'\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=linear_model_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)\n",
    "print(f\"The Serverless:returned:Price:={result['predictions'][0]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 - Endpoint CloudWatch Metrics Analysis <a id='CW-metric-analysis'></a>\n",
    "\n",
    "\n",
    "Amazon SageMaker provides CloudWatch metrics for  endpoints so you can determine the endpoint usage and the cache hit rate and optimize your endpoint.  To analyze the endpoint and the container behavior, you will invoke the model:\n",
    "\n",
    "    a. Create a Full 900 row data set\n",
    "    b. Invoke one by one row to generate traffic with no stop\n",
    "    c. Check cloud watch for instances and invocations\n",
    "\n",
    "We use this order of invocations to observe the behavior of the CloudWatch metrics - LoadedModelCount, MemoryUtilization and ModelCacheHit.  You are encouraged to experiment with loading varying number of models to use the CloudWatch charts to help make ongoing decisions on the optimal choice of instance type, instance count, and number of models that a given endpoint should host.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Invoke multiple models in a loop\n",
    "def invoke_multiple_model(model_range_low, model_range_high, features_list):\n",
    "    for i in range(model_range_low, model_range_high):\n",
    "        predict_one_house_value_serverless(features_list[i],  predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEVERLESS:Price:of:house:$434828.34, took 141 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$573561.00, took 39 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$274597.91, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$396197.97, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$387287.69, took 64 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$533519.12, took 45 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$187328.31, took 41 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$579758.44, took 38 ms\n",
      "\n",
      "SEVERLESS:Price:of:house:$486177.62, took 38 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Starting with no models loaded into the container\n",
    "##Invoke the first 100 models\n",
    "invoke_multiple_model(10, 19, test_data_df.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up<a id='CleanUp'></a>\n",
    "Clean up the endpoint to avoid unneccessary costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the endpoint and underlying model\n",
    "predictor.delete_model() \n",
    "predictor.delete_endpoint()\n",
    "for t in preprocessor_transformers:\n",
    "    t.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the IAM Role\n",
    "iam_client.detach_role_policy(\n",
    "    PolicyArn=policy_arn,\n",
    "    RoleName=role_name\n",
    ")\n",
    "iam_client.delete_role(RoleName=role_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the IAM Policy\n",
    "iam_client.delete_policy(PolicyArn=policy_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
