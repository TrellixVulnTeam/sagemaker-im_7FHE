{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8945539a",
   "metadata": {},
   "source": [
    "#### Model Monitoring\n",
    "\n",
    "**Data drift in NLP**\n",
    "\n",
    "Data drift can be classified into three categories depending on whether the distribution shift is happening on the input or on the output side, or whether the relationship between the input and the output has changed.\n",
    "Covariate shift\n",
    "\n",
    "In a covariate shift, the distribution of inputs changes over time, but the conditional distribution P(y|x) doesn’t change. This type of drift is called covariate shift because the problem arises due to a shift in the distribution of the covariates (features). For example, in an email spam classification model, distribution of training data (email corpora) may diverge from the distribution of data during scoring.\n",
    "\n",
    "**Label shift**\n",
    "\n",
    "While covariate shift focuses on changes in the feature distribution, label shift focuses on changes in the distribution of the class variable. This type of shifting is essentially the reverse of covariate shift. An intuitive way to think about it might be to consider an unbalanced dataset. If the spam to non-spam ratio of emails in our training set is 50%, but in reality 10% of our emails are non-spam, then the target label distribution has shifted.\n",
    "\n",
    "**Concept shift**\n",
    "\n",
    "Concept shift is different from covariate and label shift in that it’s not related to the data distribution or the class distribution, but instead is related to the relationship between the two variables. For example, email spammers often use a variety of concepts to pass the spam filter models, and the concept of emails used during training may change as time goes by.\n",
    "\n",
    "**Model monitoring in SageMaker**\n",
    "\n",
    "![](./images/model-monitor-architecture.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eedd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need torch 1.3.1 for elastic inference\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install torch --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "model_prefix = \"sagemaker/nlp-data-drift-bert-model\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p nlp_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./nlp_drift/cola_public_1.1.zip\"):\n",
    "    !curl -o ./nlp_drift/cola_public_1.1.zip https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
    "if not os.path.exists(\"./nlp_drift/cola_public/\"):\n",
    "    !cd nlp_drift && unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aecb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./nlp_drift/cola_public/raw/in_domain_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[20:25])\n",
    "print(labels[20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa32f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "train.to_csv(\"./nlp_drift/cola_public/train.csv\", index=False)\n",
    "test.to_csv(\"./nlp_drift/cola_public/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./nlp_drift/cola_public/train.csv\", bucket=bucket, key_prefix=model_prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"./nlp_drift/cola_public/test.csv\", bucket=bucket, key_prefix=model_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p nlp_drift/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabe6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nlp_drift/code/requirements.txt\n",
    "tqdm\n",
    "requests==2.22.0\n",
    "regex\n",
    "sentencepiece\n",
    "sacremoses\n",
    "transformers==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nlp_drift/code/train_deploy.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "MAX_LEN = 64  # this is the max length of the sentence\n",
    "\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir, is_distributed):\n",
    "    logger.info(\"Get train data loader\")\n",
    "\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    if is_distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "    else:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def _get_test_data_loader(test_batch_size, training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    logger.debug(\"NLP_DRIFT:Distributed training - %s\", is_distributed)\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    logger.debug(\"NLP_DRIFT:Number of gpus available - %d\", args.num_gpus)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(args.hosts)\n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        host_rank = args.hosts.index(args.current_host)\n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "        logger.info(\n",
    "            \"NLP_DRIFT:Initialized the distributed environment: '%s' backend on %d nodes. \"\n",
    "            \"NLP_DRIFT:Current host rank is %d. Number of gpus: %d\",\n",
    "            args.backend, dist.get_world_size(),\n",
    "            dist.get_rank(), args.num_gpus\n",
    "        )\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed)\n",
    "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\n",
    "\n",
    "    logger.debug(\n",
    "        \"NLP_DRIFT:Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.debug(\n",
    "        \"NLP_DRIFT:Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Starting BertForSequenceClassification\\n\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=args.num_labels,  # The number of output labels--2 for binary classification.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    if is_distributed and use_cuda:\n",
    "        # multi-machine multi-gpu case\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "        eps=1e-8,  # args.adam_epsilon - default is 1e-8.\n",
    "    )\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:End of defining BertForSequenceClassification\\n\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step % args.log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"NLP_DRIFT:Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        step * len(batch[0]),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * step / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        logger.info(\"NLP_DRIFT:Average training loss: %f\\n\", total_loss / len(train_loader))\n",
    "\n",
    "        test(model, test_loader, device)\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Saving tuned model.\")\n",
    "    model_2_save = model.module if hasattr(model, \"module\") else model\n",
    "    model_2_save.save_pretrained(save_directory=args.model_dir)\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Test set: Accuracy: %f\\n\", tmp_eval_accuracy)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"NLP_DRIFT:================ objects in model_dir ===================\")\n",
    "    print(os.listdir(model_dir))\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    print(\"NLP_DRIFT:================ model loaded ===========================\")\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        print(\"NLP_DRIFT:================ input sentences ===============\")\n",
    "        print(data)\n",
    "        \n",
    "        if isinstance(data, str):\n",
    "            data = [data]\n",
    "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Input type can be a string or an non-empty list. \\\n",
    "                             I got {}\".format(data))\n",
    "                       \n",
    "        #encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
    "        #encoded = tokenizer(data, add_special_tokens=True) \n",
    "        \n",
    "        # for backward compatibility use the following way to encode \n",
    "        # https://github.com/huggingface/transformers/issues/5580\n",
    "        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
    "        \n",
    "        print(\"NLP_DRIFT:================ encoded sentences ==============\")\n",
    "        print(input_ids)\n",
    "\n",
    "        # pad shorter sentence\n",
    "        padded =  torch.zeros(len(input_ids), MAX_LEN) \n",
    "        for i, p in enumerate(input_ids):\n",
    "            padded[i, :len(p)] = torch.tensor(p)\n",
    "     \n",
    "        # create mask\n",
    "        mask = (padded != 0)\n",
    "        \n",
    "        print(\"NLP_DRIFT:================= padded input and attention mask ================\")\n",
    "        print(padded, '\\n', mask)\n",
    "\n",
    "        return padded.long(), mask.long()\n",
    "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
    "    \n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    input_id, input_mask = input_data\n",
    "    input_id = input_id.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    print(\"NLP_DRIFT:============== encoded data =================\")\n",
    "    print(input_id, input_mask)\n",
    "    with torch.no_grad():\n",
    "        y = model(input_id, attention_mask=input_mask)[0]\n",
    "        print(\"NLP_DRIFT:=============== inference result =================\")\n",
    "        print(y)\n",
    "    return y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--num_labels\", type=int, default=2, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=64, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\", type=int, default=1000, metavar=\"N\", help=\"input batch size for testing (default: 1000)\"\n",
    "    )\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2, metavar=\"N\", help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backend\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    train(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{model_prefix}\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"nlp_drift/code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,  # this script only support distributed training for GPU instances.\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True, # disable debugger\n",
    ")\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf580af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "#s3_capture_upload_path = f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture'\n",
    "prefix = \"sagemaker/CustomModelMonitor\"\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "endpoint_name='nlp-data-drift-bert-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a379223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name='nlp-data-drift-bert-endpoint'\n",
    "predictor = estimator.deploy(endpoint_name=endpoint_name,\n",
    "                             initial_instance_count=1, \n",
    "                             instance_type=\"ml.m4.xlarge\",\n",
    "                             data_capture_config=data_capture_config)\n",
    "print(endpoint_name)\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6adb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c533f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch inference \n",
    "\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "result = predictor.predict([\n",
    "    \"CLI to download the zip file\", \n",
    "    \"Thanks so much for driving me home\",\n",
    "    \"construct the sub-embeddings and corresponding baselines\",\n",
    "    \"our Bert model and interpret what the model\",\n",
    "    \"Bert models using Captum library\",\n",
    "    \"case study we focus on a fine-tuned Question Answering model on SQUAD datase\",\n",
    "    \"we pretrain the model, we can load \",\n",
    "    \"need to define baselines / references, nu\",\n",
    "    \"defines numericalized special tokens \",\n",
    "    \"Thanks so much for cooking dinner. I really appreciate it\",\n",
    "    \"let's define the ground truth for prediction's start and en\",\n",
    "    \"pre-computation of embeddings for the second option is necessary because\",\n",
    "    \"to summarize attributions for each word token in the sequence.\",\n",
    "    \"Nice to meet you, Sergio. So, where are you from\"\n",
    "])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted class: \", np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee4ae7",
   "metadata": {},
   "source": [
    "#### View Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: It takes a few minutes for the capture data to appear in S3\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810aea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64541b37",
   "metadata": {},
   "source": [
    "#### Create the base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])\n",
    "        hidden_states = outputs[2]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        sentence_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18754efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_list = []\n",
    "\n",
    "for i in sentence_embeddings:\n",
    "    sentence_embeddings_list.append(i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42807083",
   "metadata": {},
   "source": [
    "#### Save sentence as a npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nlp_drift/embeddings.npy', sentence_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f71424",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp nlp_drift/embeddings.npy s3://{bucket}/{model_prefix}/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b5e4a",
   "metadata": {},
   "source": [
    "#### Dockerfile create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-nlp/Dockerfile\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install sagemaker\n",
    "RUN pip3 install scipy\n",
    "RUN pip3 install transformers\n",
    "RUN pip3 install torch\n",
    "RUN pip3 install s3fs\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ADD evaluation.py /\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/evaluation.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile nlp_drift/code/evaluate.py\n",
    "\"\"\"Custom Model Monitoring script for Detecting Data Drift in NLP using SageMaker Model Monitor\n",
    "\"\"\"\n",
    "\n",
    "# Python Built-Ins:\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# External Dependencies:\n",
    "import numpy as np\n",
    "import boto3\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_environment():\n",
    "    \"\"\"Load configuration variables for SM Model Monitoring job\n",
    "\n",
    "    See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-inputs.html\n",
    "    \"\"\"\n",
    "    print(f\"nlp-drift::get_environment()::\")\n",
    "    try:\n",
    "        with open(\"/opt/ml/config/processingjobconfig.json\", \"r\") as conffile:\n",
    "            defaults = json.loads(conffile.read())[\"Environment\"]\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Unable to read environment vars from SM processing config file\")\n",
    "        defaults = {}\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        dataset_format=os.environ.get(\"dataset_format\", defaults.get(\"dataset_format\")),\n",
    "        dataset_source=os.environ.get(\n",
    "            \"dataset_source\",\n",
    "            defaults.get(\"dataset_source\", \"/opt/ml/processing/input/endpoint\"),\n",
    "        ),\n",
    "        end_time=os.environ.get(\"end_time\", defaults.get(\"end_time\")),\n",
    "        output_path=os.environ.get(\n",
    "            \"output_path\",\n",
    "            defaults.get(\"output_path\", \"/opt/ml/processing/resultdata\"),\n",
    "        ),\n",
    "        publish_cloudwatch_metrics=os.environ.get(\n",
    "            \"publish_cloudwatch_metrics\",\n",
    "            defaults.get(\"publish_cloudwatch_metrics\", \"Enabled\"),\n",
    "        ),\n",
    "        sagemaker_endpoint_name=os.environ.get(\n",
    "            \"sagemaker_endpoint_name\",\n",
    "            defaults.get(\"sagemaker_endpoint_name\"),\n",
    "        ),\n",
    "        sagemaker_monitoring_schedule_name=os.environ.get(\n",
    "            \"sagemaker_monitoring_schedule_name\",\n",
    "            defaults.get(\"sagemaker_monitoring_schedule_name\"),\n",
    "        ),\n",
    "        start_time=os.environ.get(\n",
    "            \"start_time\", \n",
    "            defaults.get(\"start_time\")),\n",
    "        max_ratio_threshold=float(os.environ.get(\n",
    "            \"THRESHOLD\", \n",
    "             defaults.get(\"THRESHOLD\", \"nan\"))),\n",
    "        bucket=os.environ.get(\n",
    "            \"bucket\",\n",
    "            defaults.get(\"bucket\", \"None\")),\n",
    "    )\n",
    "\n",
    "\n",
    "def download_embeddings_file():\n",
    "    \n",
    "    env = get_environment()\n",
    "    print(f\"nlp-drift::Starting s3fs: download\")\n",
    "    \n",
    "    from s3fs.core import S3FileSystem\n",
    "    s3 = S3FileSystem()\n",
    "    \n",
    "    key = 'sagemaker/nlp-data-drift-bert-model/embeddings/embeddings.npy'\n",
    "    bucket = env.bucket\n",
    "    print(f\"nlp-drift::S3 bucket name is={bucket}\")\n",
    "\n",
    "    return np.load(s3.open('{}/{}'.format(bucket, key)))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    env = get_environment()\n",
    "    print(f\"nlp-drift::Starting evaluation with config\\n{env}\")\n",
    "\n",
    "    print(f\"nlp-drift::Downloading Embedding File\")\n",
    "    \n",
    "    #download BERT embedding file used for fine-tuning BertForSequenceClassification\n",
    "    embedding_list = download_embeddings_file()\n",
    "    \n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                      output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                      )\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    sent_cosine_dict = {}\n",
    "    violations = []\n",
    "    \n",
    "    total_record_count = 0  # Including error predictions that we can't read the response for\n",
    "    error_record_count = 0\n",
    "    counts = defaultdict(int)  # dict defaulting to 0 when unseen keys are requested\n",
    "    print(f\"nlp-drift::counts={counts}::\")\n",
    "    \n",
    "    for path, directories, filenames in os.walk(env.dataset_source):\n",
    "        for filename in filter(lambda f: f.lower().endswith(\".jsonl\"), filenames):\n",
    "            print(f\"nlp-drift::starting:DRIFT:Analysis:filename={filename}:\")\n",
    "            \n",
    "            with open(os.path.join(path, filename), \"r\") as file:\n",
    "                for entry in file:\n",
    "                    total_record_count += 1\n",
    "                    try:\n",
    "                        response = json.loads(json.loads(entry)[\"captureData\"][\"endpointInput\"][\"data\"])\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                    for record in response:\n",
    "                        encoded_dict = tokenizer.encode_plus(\n",
    "                            record, \n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 64,\n",
    "                            padding= True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                            truncation=True,\n",
    "                            )\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])\n",
    "                            hidden_states = outputs[2]\n",
    "                            token_vecs = hidden_states[-2][0]\n",
    "                            input_sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "                        \n",
    "                        cosine_score = 0\n",
    "                        \n",
    "                        for embed_item in embedding_list:\n",
    "                            cosine_score += (1 - cosine(input_sentence_embedding, embed_item))\n",
    "                            print(f\"nlp-drift::cos:={cosine_score}\")\n",
    "                            \n",
    "                        cosine_score_avg = cosine_score/(len(embedding_list))\n",
    "                        if cosine_score_avg < env.max_ratio_threshold:\n",
    "                            error_record_count += 1\n",
    "                            sent_cosine_dict[record] = cosine_score_avg\n",
    "                            violations.append({\n",
    "                                    \"sentence\": record,\n",
    "                                    \"avg_cosine_score\": cosine_score_avg,\n",
    "                                    \"feature_name\": \"sent_cosine_score\",\n",
    "                                    \"constraint_check_type\": \"baseline_drift_check\",\n",
    "                                    \"endpoint_name\" : env.sagemaker_endpoint_name,\n",
    "                                    \"monitoring_schedule_name\": env.sagemaker_monitoring_schedule_name\n",
    "                                })\n",
    "        \n",
    "    print(f\"nlp-drift::Checking for constraint violations...\")\n",
    "    print(f\"nlp-drift::Violations: {violations if len(violations) else 'None'}\")\n",
    "\n",
    "    print(f\"nlp-drift::Writing violations file...\")\n",
    "    with open(os.path.join(env.output_path, \"constraints_violations.json\"), \"w\") as outfile:\n",
    "        outfile.write(json.dumps(\n",
    "            { \"violations\": violations },\n",
    "            indent=4,\n",
    "        ))\n",
    "    \n",
    "    print(f\"nlp-drift::Writing overall status output...\")\n",
    "    with open(\"/opt/ml/output/message\", \"w\") as outfile:\n",
    "        if len(violations):\n",
    "            msg = ''\n",
    "            for v in violations:\n",
    "                msg += f\"CompletedWithViolations: {v['sentence']}\"\n",
    "                msg +=\"\\n\"\n",
    "        else:\n",
    "            msg = \"Completed: Job completed successfully with no violations.\"\n",
    "        outfile.write(msg)\n",
    "        print(msg)\n",
    "\n",
    "    if True:\n",
    "    #if env.publish_cloudwatch_metrics:\n",
    "        print(f\"nlp-drift::Writing CloudWatch metrics...\")\n",
    "        with open(\"/opt/ml/output/metrics/cloudwatch/cloudwatch_metrics.jsonl\", \"a+\") as outfile:\n",
    "            # One metric per line (JSONLines list of dictionaries)\n",
    "            # Remember these metrics are aggregated in graphs, so we report them as statistics on our dataset\n",
    "            outfile.write(json.dumps(\n",
    "            { \"violations\": violations },\n",
    "            indent=4,\n",
    "            ))\n",
    "    print(f\"nlp-drift::Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572abafa",
   "metadata": {},
   "source": [
    "#### Build Custom Container for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8113c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'nlp-data-drift-bert-v1'\n",
    "tag = ':latest'\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1114637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ecr_repository + tag)\n",
    "print(processing_repository_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=nlp-data-drift-bert-test\n",
    "\n",
    "cd docker-nlp\n",
    "\n",
    "echo \"Repository name is $algorithm_name\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo \"account got =$account\"\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "echo \"region got=$region \"\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "\n",
    "echo \"Docker image created repo:name:or:algorithm_name=$algorithm_name fullName= $fullname\"\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "echo \"Docker push full name image=$fullname\"\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e528a",
   "metadata": {},
   "source": [
    "#### Now create the monitoring schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "monitor = ModelMonitor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5', 'bucket': bucket },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session.default_bucket()}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "\n",
    "processing_output = ProcessingOutput(\n",
    "    output_name='result',\n",
    "    source='/opt/ml/processing/resultdata',\n",
    "    destination=destination,\n",
    ")\n",
    "output = MonitoringOutput(source=processing_output.source, destination=processing_output.destination)\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='nlp-data-drift-bert-schedule',\n",
    "    output=output,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a184793",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3566b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = monitor.list_executions()\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(jobs) > 0:\n",
    "    last_execution_desc = monitor.list_executions()[-1].describe()\n",
    "    print(last_execution_desc)\n",
    "    print(f'\\nExit Message: {last_execution_desc.get(\"ExitMessage\", \"None\")}')\n",
    "else:\n",
    "    print(\"\"\"No processing job has been executed yet. \n",
    "    This means that one hour has not passed yet. \n",
    "    You can go to the next code cell and run the processing job manually\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_uri_processing = sagemaker.image_uris.retrieve('processing_repository_uri', 'us-east-1')\n",
    "container = \"{}.dkr.ecr.{}.amazonaws.com/nlp-data-drift-bert-v1:latest\".format(\n",
    "    account_id, region\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f532b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"{}.dkr.ecr.{}.amazonaws.com/nlp-data-drift-bert-test:latest\".format(\n",
    "    account_id, region\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9acb8",
   "metadata": {},
   "source": [
    "### MANUALLY run the PROCESSING JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session.default_bucket()}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "processor = Processor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=container, #processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5','bucket': bucket },\n",
    ")\n",
    "    \n",
    "processor.run(\n",
    "    [ProcessingInput(\n",
    "        input_name='endpointdata',\n",
    "        source = \"s3://{}/{}/{}\".format(bucket, data_capture_prefix,endpoint_name),\n",
    "        #source=f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture',\n",
    "        destination = '/opt/ml/processing/input/endpoint',\n",
    "    )],\n",
    "    [ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/resultdata',\n",
    "        destination=destination,\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3d8c0",
   "metadata": {},
   "source": [
    "#### Clean up Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7557f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the monitoring schedule\n",
    "monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e872a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "import time\n",
    "for i in range(1,100):\n",
    "    result = predictor.predict([\n",
    "        \"CLI to download the zip file\", \n",
    "        \"Thanks so much for driving me home\",\n",
    "        \"construct the sub-embeddings and corresponding baselines\",\n",
    "        \"our Bert model and interpret what the model\",\n",
    "        \"Bert models using Captum library\",\n",
    "        \"case study we focus on a fine-tuned Question Answering model on SQUAD datase\",\n",
    "        \"we pretrain the model, we can load \",\n",
    "        \"need to define baselines / references, nu\",\n",
    "        \"defines numericalized special tokens \",\n",
    "        \"Thanks so much for cooking dinner. I really appreciate it\",\n",
    "        \"let's define the ground truth for prediction's start and en\",\n",
    "        \"pre-computation of embeddings for the second option is necessary because\",\n",
    "        \"to summarize attributions for each word token in the sequence.\",\n",
    "        \"Nice to meet you, Sergio. So, where are you from\"\n",
    "    ])\n",
    "    time.sleep(0.05)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    [ProcessingInput(\n",
    "        input_name='endpointdata',\n",
    "        source = \"s3://{}/{}/{}\".format(bucket, data_capture_prefix,endpoint_name),\n",
    "        #source=f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture',\n",
    "        destination = '/opt/ml/processing/input/endpoint',\n",
    "    )],\n",
    "    [ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/resultdata',\n",
    "        destination=destination,\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
