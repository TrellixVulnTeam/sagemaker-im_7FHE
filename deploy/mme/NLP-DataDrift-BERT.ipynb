{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe286427",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f1b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (22.2.2)\n"
     ]
    }
   ],
   "source": [
    "# need torch 1.3.1 for elastic inference\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install torch --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22613ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "model_prefix = \"sagemaker/nlp-data-drift-bert-model\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf715ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p nlp_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93f7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  249k  100  249k    0     0  7473k      0 --:--:-- --:--:-- --:--:-- 7555k\n",
      "Archive:  cola_public_1.1.zip\n",
      "   creating: cola_public/\n",
      "  inflating: cola_public/README      \n",
      "   creating: cola_public/tokenized/\n",
      "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
      "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
      "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
      "   creating: cola_public/raw/\n",
      "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
      "  inflating: cola_public/raw/in_domain_train.tsv  \n",
      "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./nlp_drift/cola_public_1.1.zip\"):\n",
    "    !curl -o ./nlp_drift/cola_public_1.1.zip https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
    "if not os.path.exists(\"./nlp_drift/cola_public/\"):\n",
    "    !cd nlp_drift && unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742bdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./nlp_drift/cola_public/raw/in_domain_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fcdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  Our friends won't buy this analysis, let alone...\n",
       "1      1  One more pseudo generalization and I'm giving up.\n",
       "2      1   One more pseudo generalization or I'm giving up.\n",
       "3      1     The more we study verbs, the crazier they get.\n",
       "4      1          Day by day the facts are getting murkier."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de57c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The professor talked us.' 'We yelled ourselves hoarse.'\n",
      " 'We yelled ourselves.' 'We yelled Harry hoarse.'\n",
      " 'Harry coughed himself into a fit.']\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[20:25])\n",
    "print(labels[20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117ffa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "train.to_csv(\"./nlp_drift/cola_public/train.csv\", index=False)\n",
    "test.to_csv(\"./nlp_drift/cola_public/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e00172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./nlp_drift/cola_public/train.csv\", bucket=bucket, key_prefix=model_prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"./nlp_drift/cola_public/test.csv\", bucket=bucket, key_prefix=model_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f24534",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p nlp_drift/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39f1107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nlp_drift/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile nlp_drift/code/requirements.txt\n",
    "tqdm\n",
    "requests==2.22.0\n",
    "regex\n",
    "sentencepiece\n",
    "sacremoses\n",
    "transformers==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e869e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nlp_drift/code/train_deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nlp_drift/code/train_deploy.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "MAX_LEN = 64  # this is the max length of the sentence\n",
    "\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def _get_train_data_loader(batch_size, training_dir, is_distributed):\n",
    "    logger.info(\"Get train data loader\")\n",
    "\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"train.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    if is_distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "    else:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def _get_test_data_loader(test_batch_size, training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, \"test.csv\"))\n",
    "    sentences = dataset.sentence.values\n",
    "    labels = dataset.label.values\n",
    "\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    # pad shorter sentences\n",
    "    input_ids_padded = []\n",
    "    for i in input_ids:\n",
    "        while len(i) < MAX_LEN:\n",
    "            i.append(0)\n",
    "        input_ids_padded.append(i)\n",
    "    input_ids = input_ids_padded\n",
    "\n",
    "    # mask; 0: added, 1: otherwise\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    # convert to PyTorch data types.\n",
    "    train_inputs = torch.tensor(input_ids)\n",
    "    train_labels = torch.tensor(labels)\n",
    "    train_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "    logger.debug(\"NLP_DRIFT:Distributed training - %s\", is_distributed)\n",
    "    use_cuda = args.num_gpus > 0\n",
    "    logger.debug(\"NLP_DRIFT:Number of gpus available - %d\", args.num_gpus)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(args.hosts)\n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        host_rank = args.hosts.index(args.current_host)\n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "        logger.info(\n",
    "            \"NLP_DRIFT:Initialized the distributed environment: '%s' backend on %d nodes. \"\n",
    "            \"NLP_DRIFT:Current host rank is %d. Number of gpus: %d\",\n",
    "            args.backend, dist.get_world_size(),\n",
    "            dist.get_rank(), args.num_gpus\n",
    "        )\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(args.seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed)\n",
    "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\n",
    "\n",
    "    logger.debug(\n",
    "        \"NLP_DRIFT:Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.debug(\n",
    "        \"NLP_DRIFT:Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Starting BertForSequenceClassification\\n\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels=args.num_labels,  # The number of output labels--2 for binary classification.\n",
    "        output_attentions=False,  # Whether the model returns attentions weights.\n",
    "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    if is_distributed and use_cuda:\n",
    "        # multi-machine multi-gpu case\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "        eps=1e-8,  # args.adam_epsilon - default is 1e-8.\n",
    "    )\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:End of defining BertForSequenceClassification\\n\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "            if step % args.log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"NLP_DRIFT:Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        step * len(batch[0]),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * step / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        logger.info(\"NLP_DRIFT:Average training loss: %f\\n\", total_loss / len(train_loader))\n",
    "\n",
    "        test(model, test_loader, device)\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Saving tuned model.\")\n",
    "    model_2_save = model.module if hasattr(model, \"module\") else model\n",
    "    model_2_save.save_pretrained(save_directory=args.model_dir)\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    _, eval_accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to(\"cpu\").numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    logger.info(\"NLP_DRIFT:Test set: Accuracy: %f\\n\", tmp_eval_accuracy)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"NLP_DRIFT:================ objects in model_dir ===================\")\n",
    "    print(os.listdir(model_dir))\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    print(\"NLP_DRIFT:================ model loaded ===========================\")\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        print(\"NLP_DRIFT:================ input sentences ===============\")\n",
    "        print(data)\n",
    "        \n",
    "        if isinstance(data, str):\n",
    "            data = [data]\n",
    "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Input type can be a string or an non-empty list. \\\n",
    "                             I got {}\".format(data))\n",
    "                       \n",
    "        #encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
    "        #encoded = tokenizer(data, add_special_tokens=True) \n",
    "        \n",
    "        # for backward compatibility use the following way to encode \n",
    "        # https://github.com/huggingface/transformers/issues/5580\n",
    "        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
    "        \n",
    "        print(\"NLP_DRIFT:================ encoded sentences ==============\")\n",
    "        print(input_ids)\n",
    "\n",
    "        # pad shorter sentence\n",
    "        padded =  torch.zeros(len(input_ids), MAX_LEN) \n",
    "        for i, p in enumerate(input_ids):\n",
    "            padded[i, :len(p)] = torch.tensor(p)\n",
    "     \n",
    "        # create mask\n",
    "        mask = (padded != 0)\n",
    "        \n",
    "        print(\"NLP_DRIFT:================= padded input and attention mask ================\")\n",
    "        print(padded, '\\n', mask)\n",
    "\n",
    "        return padded.long(), mask.long()\n",
    "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
    "    \n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    input_id, input_mask = input_data\n",
    "    input_id = input_id.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    print(\"NLP_DRIFT:============== encoded data =================\")\n",
    "    print(input_id, input_mask)\n",
    "    with torch.no_grad():\n",
    "        y = model(input_id, attention_mask=input_mask)[0]\n",
    "        print(\"NLP_DRIFT:=============== inference result =================\")\n",
    "        print(y)\n",
    "    return y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument(\n",
    "        \"--num_labels\", type=int, default=2, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", type=int, default=64, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\", type=int, default=1000, metavar=\"N\", help=\"input batch size for testing (default: 1000)\"\n",
    "    )\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2, metavar=\"N\", help=\"number of epochs to train (default: 10)\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
    "    parser.add_argument(\n",
    "        \"--log-interval\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        metavar=\"N\",\n",
    "        help=\"how many batches to wait before logging training status\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--backend\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
    "    )\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    train(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4787dbd8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-18 17:44:47 Starting - Starting the training job......\n",
      "2022-09-18 17:45:25 Starting - Preparing the instances for training......\n",
      "2022-09-18 17:46:42 Downloading - Downloading input data...\n",
      "2022-09-18 17:47:17 Training - Downloading the training image............\n",
      "2022-09-18 17:49:03 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:06,792 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:06,829 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:06,830 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:07,254 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:07,255 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:07,255 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:07,255 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmptksuqumb/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2022.9.13-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.11.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses, default-user-module-name\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895251 sha256=e6e674b2a04365000013269ec915758e3a501627db2c7e22b9bf5a73496555af\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=17068 sha256=8ba3c054bf90189d0b789a62487ec0f2cef02b4b53329b7a15f7706aac6d55c6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-at4gjg_7/wheels/3c/7a/19/8d4779d1c6a88da4f870022c145fc7a34915992a29a93fc33b\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sentencepiece, sacremoses, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 regex-2022.9.13 sacremoses-0.0.53 sentencepiece-0.1.97 transformers-2.3.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2022-09-18 17:49:15,041 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-09-18-17-44-47-123\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-622343165275/pytorch-training-2022-09-18-17-44-47-123/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-622343165275/pytorch-training-2022-09-18-17-44-47-123/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-09-18-17-44-47-123\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-622343165275/pytorch-training-2022-09-18-17-44-47-123/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_deploy.py --backend gloo --epochs 1 --num_labels 2\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34mINFO:__main__:NLP_DRIFT:Train Epoch: 1 [0/6413 (0%)] Loss: 0.643908\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Distributed training - False\u001b[0m\n",
      "\u001b[34mINFO:__main__:NLP_DRIFT:Train Epoch: 1 [3200/6413 (50%)] Loss: 0.443524\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Number of gpus available - 1\u001b[0m\n",
      "\n",
      "2022-09-18 17:50:18 Uploading - Uploading generated training model\u001b[34mINFO:__main__:NLP_DRIFT:Train Epoch: 1 [1300/6413 (99%)] Loss: 0.461298\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:NLP_DRIFT:Average training loss: 0.508384\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Processes 6413/6413 (100%) of train data\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Processes 2138/2138 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:NLP_DRIFT:Test set: Accuracy: 0.753623\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Starting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mINFO:__main__:NLP_DRIFT:Saving tuned model.\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:End of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mINFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mINFO:transformers.modeling_utils:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:38.725 algo-1:53 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:38.725 algo-1:53 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:38.725 algo-1:53 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:38.748 algo-1:53 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.553 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.553 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.553 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.554 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.564 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.564 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.564 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.567 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.567 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.567 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.568 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.570 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.570 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.570 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.573 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.573 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.573 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.574 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.576 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.576 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.576 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.579 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.579 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.579 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.580 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.582 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.582 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.582 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.585 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.585 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.585 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.586 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.588 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.588 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.588 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.591 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.591 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.591 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.592 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.594 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.594 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.594 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.597 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.597 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.597 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.598 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.600 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.600 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.600 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.603 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.603 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.604 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.604 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.606 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.607 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.607 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.609 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.610 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.610 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.610 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.612 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.613 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.613 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.616 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.616 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.616 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.617 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.619 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.619 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.619 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.622 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.622 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.622 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.623 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.625 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.625 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.625 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.628 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.628 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.628 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.629 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.631 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.631 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:49:39.631 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Train Epoch: 1 [0/6413 (0%)] Loss: 0.643908\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Train Epoch: 1 [3200/6413 (50%)] Loss: 0.443524\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Train Epoch: 1 [1300/6413 (99%)] Loss: 0.461298\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Average training loss: 0.508384\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Test set: Accuracy: 0.753623\u001b[0m\n",
      "\u001b[34mNLP_DRIFT:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[2022-09-18 17:50:09.433 algo-1:53 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2022-09-18 17:50:09,903 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-09-18 17:51:29 Completed - Training job completed\n",
      "Training seconds: 287\n",
      "Billable seconds: 287\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = f\"s3://{bucket}/{model_prefix}\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"nlp_drift/code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,  # this script only support distributed training for GPU instances.\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    },\n",
    "    disable_profiler=True, # disable debugger\n",
    ")\n",
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75c19534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/sagemaker/CustomModelMonitor/datacapture\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "#s3_capture_upload_path = f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture'\n",
    "prefix = \"sagemaker/CustomModelMonitor\"\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "\n",
    "print(s3_capture_upload_path)\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "endpoint_name='nlp-data-drift-bert-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0b43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cfa39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!nlp-data-drift-bert-endpoint\n",
      "<sagemaker.pytorch.model.PyTorchPredictor object at 0x7f223e93f910>\n"
     ]
    }
   ],
   "source": [
    "endpoint_name='nlp-data-drift-bert-endpoint'\n",
    "predictor = estimator.deploy(endpoint_name=endpoint_name,\n",
    "                             initial_instance_count=1, \n",
    "                             instance_type=\"ml.m4.xlarge\",\n",
    "                             data_capture_config=data_capture_config)\n",
    "print(endpoint_name)\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49152f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-data-drift-bert-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc87c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c5cac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint nlp-data-drift-bert-endpoint. \n",
      "Please wait...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# batch inference \n",
    "\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "result = predictor.predict([\n",
    "    \"CLI to download the zip file\", \n",
    "    \"Thanks so much for driving me home\",\n",
    "    \"construct the sub-embeddings and corresponding baselines\",\n",
    "    \"our Bert model and interpret what the model\",\n",
    "    \"Bert models using Captum library\",\n",
    "    \"case study we focus on a fine-tuned Question Answering model on SQUAD datase\",\n",
    "    \"we pretrain the model, we can load \",\n",
    "    \"need to define baselines / references, nu\",\n",
    "    \"defines numericalized special tokens \",\n",
    "    \"Thanks so much for cooking dinner. I really appreciate it\",\n",
    "    \"let's define the ground truth for prediction's start and en\",\n",
    "    \"pre-computation of embeddings for the second option is necessary because\",\n",
    "    \"to summarize attributions for each word token in the sequence.\",\n",
    "    \"Nice to meet you, Sergio. So, where are you from\"\n",
    "])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2665c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted class: \", np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a3624",
   "metadata": {},
   "source": [
    "#### View Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "809da965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "sagemaker/CustomModelMonitor/datacapture/nlp-data-drift-bert-endpoint/AllTraffic/2022/09/18/18/05-50-842-b30c8534-a9ba-4981-a29e-24b270114782.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Note: It takes a few minutes for the capture data to appear in S3\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dc617cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"application/json\",\"mode\":\"INPUT\",\"data\":\"[\\\"CLI to download the zip file\\\", \\\"Thanks so much for driving me home\\\", \\\"construct the sub-embeddings and corresponding baselines\\\", \\\"our Bert model and interpret what the model\\\", \\\"Bert models using Captum library\\\", \\\"case study we focus on a fine-tuned Question Answering model on SQUAD datase\\\", \\\"we pretrain the model, we can load \\\", \\\"need to define baselines / references, nu\\\", \\\"defines numericalized special tokens \\\", \\\"Thanks so much for cooking dinner. I really appreciate it\\\", \\\"let's define the ground truth for prediction's start and en\\\", \\\"pre-computation of embeddings for the second option is necessary because\\\", \\\"to summarize attributions for each word token in the sequence.\\\", \\\"Nice to meet you, Sergio. So, where are you from\\\"]\",\"encoding\":\"JSON\"},\"endpointOutput\":{\"observedContentType\":\"application/json\",\"mode\":\"OUTPUT\",\"data\":\"[[0.6717401742935181, -0.8138699531555176], [-0.7949126362800598, 1.705125093460083], [-0.7300865650177002, 1.3512063026428223], [0.5613102912902832, -0.5136985182762146], [0.4626815617084503, -0.08009722828865051], [0.4613603949546814, 0.052586495876312256], [-0.2172597199678421, 0.7841755747795105], [0.633583664894104, -0.37642529606819153], [-0.20070098340511322, 0.8482598066329956], [-0.7740269899368286, 1.6977266073226929], [0.5297243595123291, -0.33763793110847473], [-0.35228675603866577, 1.2145626544952393], [0.15961718559265137, 0.3900618851184845], [-0.8108333945274353, 1.4918384552001953]]\",\"encoding\":\"JSON\"}},\"eventMetadata\":{\"eventId\":\"fb0be594-616e-408f-9440-41820a65c132\",\"inferenceTime\":\"2022-09-18T18:05:50Z\"},\"eventVersion\":\"0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9b878470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "       \"One more pseudo generalization and I'm giving up.\",\n",
       "       \"One more pseudo generalization or I'm giving up.\", ...,\n",
       "       'It is easy to slay the Gorgon.',\n",
       "       'I had the strangest feeling that I knew you.',\n",
       "       'What all did you get for Christmas?'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0437cc",
   "metadata": {},
   "source": [
    "#### Create the base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e6805e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2300: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "sentence_embeddings = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])\n",
    "        hidden_states = outputs[2]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        sentence_embeddings.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45ce3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_list = []\n",
    "\n",
    "for i in sentence_embeddings:\n",
    "    sentence_embeddings_list.append(i.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3cd87",
   "metadata": {},
   "source": [
    "#### Save sentence as a npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "410c8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nlp_drift/embeddings.npy', sentence_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9490d7e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.41459668e-01, -9.18637812e-02,  5.19062281e-01,  4.00024682e-01,\n",
       "        8.66519213e-02, -3.25883031e-01, -1.24016702e-01,  7.44163468e-02,\n",
       "        7.17647374e-01, -2.21915811e-01,  1.27307832e-01,  2.71159589e-01,\n",
       "       -2.26745367e-01,  3.91900063e-01, -1.14975072e-01, -1.54685378e-01,\n",
       "        5.66386878e-01,  1.60334148e-02,  5.11665940e-02, -8.28866363e-02,\n",
       "       -2.42578909e-02, -5.17321490e-02, -4.64935780e-01, -4.92869020e-02,\n",
       "        3.78277272e-01, -2.42931932e-01,  1.66612789e-02,  4.27489057e-02,\n",
       "       -4.87603247e-01,  3.37470293e-01,  1.70969591e-01, -4.77717727e-01,\n",
       "       -3.13293129e-01, -5.75514138e-03, -1.34845823e-01,  2.92395443e-01,\n",
       "       -2.04022259e-01,  4.02555794e-01, -5.67179203e-01,  1.42273039e-01,\n",
       "       -3.88651490e-01, -3.48770648e-01,  1.54632971e-01, -4.36050072e-02,\n",
       "       -4.35701072e-01, -2.95430809e-01,  3.90959084e-01,  2.03553438e-01,\n",
       "       -1.27090558e-01, -5.96862435e-01, -1.63994581e-01,  2.14198038e-01,\n",
       "       -1.01401545e-02,  8.75939056e-03,  2.94220328e-01, -3.91344249e-01,\n",
       "        1.33358195e-01, -8.47702682e-01, -4.13923621e-01,  1.51814163e-01,\n",
       "       -1.58177823e-01, -4.21438552e-02,  1.75296694e-01, -2.07068056e-01,\n",
       "        5.35203338e-01, -3.15407276e-01,  1.93224147e-01,  4.17491287e-01,\n",
       "       -8.05273771e-01, -4.79027033e-01, -2.67773837e-01, -4.22327220e-03,\n",
       "       -2.52414227e-01, -2.35971838e-01,  4.43621337e-01, -6.26238883e-02,\n",
       "       -4.15945530e-01, -9.00312215e-02, -4.09825109e-02, -3.11061502e-01,\n",
       "        1.05364889e-01,  2.92114884e-01, -4.86183941e-01, -5.59930243e-02,\n",
       "       -2.66967684e-01,  1.34093404e-01, -2.43624020e-03,  5.54834127e-01,\n",
       "       -3.76183957e-01,  6.04659915e-01, -3.53414923e-01,  8.36773738e-02,\n",
       "        3.47531021e-01, -1.01595826e-01,  2.04312637e-01, -4.65236664e-01,\n",
       "        1.46546125e-01, -1.96502265e-02, -3.63288552e-01, -2.95561969e-01,\n",
       "        2.18222082e-01, -1.08973205e+00, -4.16822612e-01, -5.00446185e-02,\n",
       "       -1.49678998e-02, -7.36222267e-02, -1.15438774e-01, -6.44769907e-01,\n",
       "       -1.18193157e-01, -1.35655746e-01,  2.10321695e-01,  5.56678057e-01,\n",
       "        1.35682166e-01, -6.99613392e-01, -6.73526525e-02,  3.08775246e-01,\n",
       "       -8.05480063e-01, -6.15152895e-01,  1.32988170e-01, -2.37497836e-01,\n",
       "        3.42423379e-01,  3.65614519e-02,  1.29979670e-01, -1.85523219e-02,\n",
       "       -1.68662310e-01, -1.33240938e-01, -3.03534776e-01,  1.23396605e-01,\n",
       "       -1.94969118e-01, -7.06584930e-01,  2.91216612e-01,  2.02378437e-01,\n",
       "        1.81148097e-01,  1.21221967e-01, -2.90186375e-01,  3.78536820e-01,\n",
       "        4.64348376e-01, -1.48984432e-01,  2.85754025e-01, -2.71712765e-02,\n",
       "       -4.92614180e-01,  2.60838956e-01,  5.22860408e-01,  3.59065503e-01,\n",
       "        2.38558978e-01,  4.40682560e-01,  1.07566968e-01, -1.88060313e-01,\n",
       "       -3.73944402e-01, -5.12475610e-01, -5.69389582e-01, -1.58001095e-01,\n",
       "       -4.45800185e-01, -2.86927551e-01, -4.59157750e-02,  3.74816597e-01,\n",
       "       -2.13803262e-01,  3.65022391e-01, -2.84619629e-01,  5.60481429e-01,\n",
       "       -1.04762457e-01, -4.84636366e-01, -5.11665642e-03,  1.64515540e-01,\n",
       "        3.58667731e-01,  2.91539997e-01,  1.42651662e-01,  8.39676261e-01,\n",
       "        4.62216437e-01, -1.53822005e-01,  4.71657634e-01,  1.56860530e-01,\n",
       "        4.73473668e-01, -6.88133419e-01,  5.03093973e-02, -2.71539062e-01,\n",
       "       -7.08668530e-02,  2.45834291e-01,  2.11416297e-02,  4.40855443e-01,\n",
       "       -3.16936135e-01, -2.32549936e-01, -2.25859344e-01, -5.08087337e-01,\n",
       "        4.59328502e-01, -9.73774344e-02, -5.36044091e-02,  1.62570283e-01,\n",
       "       -4.76751663e-02, -1.37404427e-01, -4.49380815e-01, -3.76118459e-02,\n",
       "        3.75530720e-01,  1.47777185e-01, -4.03004050e-01, -5.08713484e-01,\n",
       "        1.55387044e-01, -4.56035227e-01,  3.57726626e-02,  9.62299854e-02,\n",
       "        3.39617729e-01,  1.89585924e-01, -4.53946590e-01,  6.01778775e-02,\n",
       "        9.53613967e-02, -1.99645281e-01,  5.07241637e-02,  8.70362669e-02,\n",
       "        9.54782888e-02,  1.64917767e-01,  9.97734815e-03,  4.36994612e-01,\n",
       "       -4.81018305e-01,  9.31439400e-02, -4.53660727e-01, -2.05459401e-01,\n",
       "        3.07867676e-01, -1.25650182e-01, -7.72068650e-02, -1.80942148e-01,\n",
       "       -1.45098463e-01, -7.04362154e-01,  1.37776434e-01,  1.43234327e-01,\n",
       "        1.47205591e-01,  4.45376515e-01,  2.03408390e-01,  1.55002773e-01,\n",
       "        1.88007504e-02, -3.67499948e-01, -1.40906274e-01, -3.18972379e-01,\n",
       "        3.70834358e-02, -9.21659321e-02, -1.83553800e-01,  3.15664768e-01,\n",
       "       -3.29358280e-01, -1.44338921e-01, -5.05699158e-01,  4.16784585e-02,\n",
       "       -2.93838382e-01, -8.55292194e-03,  2.70328149e-02,  1.73095122e-01,\n",
       "       -2.02616081e-01,  2.38237262e-01,  1.70597315e-01, -4.31003273e-01,\n",
       "       -3.91726568e-03,  6.02224022e-02, -5.55183649e-01,  4.68674302e-01,\n",
       "       -5.67555353e-02, -2.37068251e-01,  3.98420870e-01,  4.24064212e-02,\n",
       "        2.45269626e-01, -5.99017739e-03, -1.45641744e-01,  1.00542575e-01,\n",
       "        1.54929981e-01, -1.78232044e-01, -4.64597225e-01,  8.87812600e-02,\n",
       "       -5.05230911e-02, -8.24211001e-01,  1.58495903e-02, -5.87337986e-02,\n",
       "       -4.90056634e-01,  5.52592427e-03,  1.49204433e-01, -2.60247514e-02,\n",
       "       -1.89193487e-01,  3.77895743e-01, -2.51291711e-02, -2.95398943e-02,\n",
       "        3.42126608e-01,  1.09741524e-01, -4.75208014e-01,  2.36612007e-01,\n",
       "        9.01746005e-02,  1.83028445e-01, -3.80633056e-01,  2.89924264e-01,\n",
       "       -7.17655048e-02, -3.76662016e-02, -1.49636231e-02, -4.52584356e-01,\n",
       "       -1.86853975e-01, -6.41917288e-02, -1.54171988e-01, -3.38527076e-02,\n",
       "        1.62959993e-01, -5.68726897e-01,  2.49449596e-01, -3.24600190e-01,\n",
       "        3.26850623e-01, -3.22593182e-01, -7.47667998e-02, -2.56547719e-01,\n",
       "       -8.27657461e-01,  7.72511363e-01,  5.09863019e-01, -1.47994906e-01,\n",
       "        4.96593602e-02, -2.46181548e-01,  2.82966942e-01,  2.09531516e-01,\n",
       "       -1.83223057e+01,  1.27236068e-01,  1.60959274e-01,  6.64411783e-02,\n",
       "        6.79392368e-02, -8.92203301e-02, -4.05105203e-02,  6.42990619e-02,\n",
       "        6.17697053e-02, -1.26665339e-01,  5.26150107e-01, -4.63486701e-01,\n",
       "        1.48110613e-01,  1.11218572e-01, -3.11273336e-01, -4.29273367e-01,\n",
       "        4.33292314e-02, -5.74534535e-01,  4.04145539e-01,  2.84657657e-01,\n",
       "        4.94403392e-02,  2.15975463e-01,  2.89201200e-01, -1.48725972e-01,\n",
       "        1.12027481e-01,  2.02226520e-01,  5.21053791e-01,  7.82107115e-02,\n",
       "        1.93453223e-01, -2.25502670e-01, -7.18049258e-02, -5.43016732e-01,\n",
       "        2.71206081e-01, -2.77330764e-02, -6.55056596e-01, -3.15455437e-01,\n",
       "       -2.95983493e-01,  1.02396905e-01, -1.70327891e-02, -8.42284597e-03,\n",
       "        4.38402832e-01, -1.88357368e-01, -1.97277278e-01, -5.58063053e-02,\n",
       "        9.20269266e-03, -7.15111345e-02,  1.23803511e-01,  9.76760760e-02,\n",
       "        2.43274495e-01,  3.01553726e-01,  4.79007602e-01,  1.00474596e-01,\n",
       "        2.19060302e-01, -3.68124783e-01, -3.60360026e-01,  2.76099324e-01,\n",
       "       -1.87256366e-01,  3.01067770e-01, -2.16489375e-01,  2.54275918e-01,\n",
       "       -4.45765369e-02, -2.97906250e-02, -3.68101418e-01,  1.68570936e-01,\n",
       "        5.87695464e-03, -2.94410348e-01, -1.84353739e-01,  4.81992625e-02,\n",
       "        4.39827926e-02, -6.87811822e-02, -3.96898985e-01, -4.66925055e-02,\n",
       "        6.84773922e-02, -1.00212443e+00,  5.56167252e-02,  2.06656396e-01,\n",
       "       -3.60994413e-02, -3.36277723e-01, -1.24444216e-02,  4.21645880e-01,\n",
       "       -1.28106028e-01, -2.32714161e-01, -4.18632269e-01, -3.66083741e-01,\n",
       "        2.93666899e-01, -3.09351593e-01, -1.68093517e-02,  2.41886020e-01,\n",
       "       -3.08982193e-01, -6.12994023e-02,  4.34591979e-01,  6.00322559e-02,\n",
       "        3.73871773e-01, -1.87342018e-01,  6.53541446e-01,  5.02179742e-01,\n",
       "        5.07888615e-01, -3.24798256e-01, -6.25169873e-01,  2.89446056e-01,\n",
       "       -4.97279733e-01, -1.50103420e-01,  8.23401287e-03,  1.56924322e-01,\n",
       "       -3.09847444e-02, -1.61887169e-01, -7.89992094e-01,  5.03453612e-02,\n",
       "        3.61113250e-01,  3.04006003e-02,  5.51582694e-01,  4.99459729e-02,\n",
       "        3.61632079e-01, -9.14106295e-02, -3.13085198e-01, -5.82083941e-01,\n",
       "        2.08037257e-01, -1.16525069e-02,  8.54906440e-02, -5.59990048e-01,\n",
       "       -1.23767331e-02,  1.63226724e-01, -5.54145098e-01,  7.14043081e-02,\n",
       "       -3.72947693e-01, -9.34599712e-02,  9.44143236e-02,  9.49143767e-02,\n",
       "        4.24661040e-02, -2.71737695e-01, -2.11972311e-01,  6.34715319e-01,\n",
       "       -5.21348715e-01,  3.56082857e-01, -1.14658177e-01,  5.68999387e-02,\n",
       "       -1.00281142e-01,  1.91977978e-01, -1.77876711e-01, -2.34771669e-02,\n",
       "        2.64369577e-01,  3.66591722e-01, -5.39532781e-01,  1.48977280e-01,\n",
       "       -2.05352485e-01,  4.33815457e-02, -2.06977934e-01,  2.49136239e-01,\n",
       "       -3.60928506e-01, -7.26570189e-02, -3.61481532e-02,  2.60594189e-01,\n",
       "        2.39737302e-01,  6.09424472e-01, -2.15549305e-01,  3.88824314e-01,\n",
       "       -9.42243040e-02,  2.17226937e-01, -2.22868010e-01, -5.43758214e-01,\n",
       "       -1.05390981e-01,  1.74475223e-01, -1.18792117e-01, -4.53070939e-01,\n",
       "        4.36121076e-01,  1.04040898e-01, -1.65331081e-01, -1.81022555e-01,\n",
       "       -7.45826840e-01,  2.91996449e-01,  1.57350162e-03,  3.46628651e-02,\n",
       "        3.62403914e-02, -1.46031976e-01, -5.04407138e-02, -3.05974007e-01,\n",
       "       -2.78999537e-01, -1.41514987e-01,  1.25548899e-01,  1.19728297e-01,\n",
       "       -1.02237210e-01, -2.24221647e-01,  3.42747629e-01, -3.40948731e-01,\n",
       "       -4.45964903e-01,  3.57420444e-01, -4.55897301e-03, -1.09096423e-01,\n",
       "       -5.28869778e-02,  1.22201726e-01, -6.48327351e-01,  2.41650626e-01,\n",
       "       -1.33629501e-01, -2.71540284e-01, -3.70662101e-02, -3.86331975e-01,\n",
       "        3.94754350e-01, -2.20706046e-01,  6.32252991e-01,  6.13961935e-01,\n",
       "       -3.87137264e-01,  1.57361150e-01, -1.90214247e-01,  1.05481111e-01,\n",
       "        2.73766726e-01, -8.33077431e-02,  2.50379384e-01,  5.21545470e-01,\n",
       "       -2.29299873e-01, -3.76100779e-01, -2.69154549e-01, -2.28329003e-01,\n",
       "       -1.09771915e-01, -1.64909035e-01,  3.52965385e-01, -1.34360760e-01,\n",
       "        1.30462229e-01, -3.44904214e-02,  1.57757029e-02,  4.58718598e-01,\n",
       "       -3.16446930e-01, -5.52561522e-01,  2.03367636e-01, -6.35455176e-02,\n",
       "        1.43576801e-01,  8.29438418e-02, -4.27594900e-01,  3.06106567e-01,\n",
       "       -2.08105773e-01,  1.43690243e-01, -1.28682643e-01, -3.94145399e-01,\n",
       "       -6.39283061e-02,  1.13434792e-01,  4.49050754e-01,  7.29419515e-02,\n",
       "        3.10380310e-01,  5.07208347e-01,  1.70312077e-01,  1.26001090e-01,\n",
       "       -2.03778580e-01, -1.67066276e-01,  9.65508074e-03,  2.07445636e-01,\n",
       "       -6.99929669e-02, -3.77005816e-01,  2.92200208e-01,  2.12911338e-01,\n",
       "        6.39123619e-02,  1.17832214e-01,  4.57734108e-01, -4.86869514e-01,\n",
       "       -7.52595067e-01, -2.82134503e-01,  1.69198811e-01, -4.71999049e-01,\n",
       "        8.75132307e-02,  3.53990607e-02,  2.58210897e-01,  2.61638641e-01,\n",
       "       -6.68675721e-01,  2.26831853e-01, -4.93766874e-01, -1.86323330e-01,\n",
       "       -5.81000224e-02,  9.52357873e-02, -4.41171616e-01, -1.89023167e-01,\n",
       "        2.69099027e-01, -1.59057193e-02,  3.59505229e-02, -2.51529604e-01,\n",
       "       -2.62587443e-02, -5.13730533e-02, -2.94971406e-01,  2.37063140e-01,\n",
       "       -2.63030201e-01, -5.45410573e-01, -3.99926454e-01,  1.93914361e-02,\n",
       "       -3.20886932e-02,  4.66094911e-01, -1.88207105e-02, -2.69132733e-01,\n",
       "       -3.89401495e-01,  4.60176729e-02, -3.14089209e-01, -2.23906226e-02,\n",
       "       -7.03929543e-01, -8.16360340e-02, -9.91651267e-02,  1.72406048e-01,\n",
       "        1.63221657e-01,  1.50593087e-01,  1.67218700e-01,  5.94109535e-01,\n",
       "        3.90080810e-01,  7.75965005e-02, -3.73394251e-01,  1.46181017e-01,\n",
       "       -4.30101663e-01,  1.20996058e-01,  4.76959199e-02, -1.40692651e-01,\n",
       "       -8.65415782e-02,  2.50028998e-01, -1.22959092e-02,  6.14897847e-01,\n",
       "        1.30832627e-01,  5.93555689e-01, -1.10216483e-01, -1.19119808e-02,\n",
       "        8.51760507e-01,  2.99260706e-01, -2.66834080e-01,  4.39341307e-01,\n",
       "       -1.13857150e-01, -4.94625628e-01, -5.42312637e-02,  7.56926239e-02,\n",
       "        3.44280034e-01, -1.62795037e-01,  9.77776423e-02, -3.64625067e-01,\n",
       "        6.82471842e-02, -1.45815611e-01, -2.48905584e-01,  3.30841988e-01,\n",
       "        2.94760257e-01,  1.69660091e-01,  6.42565824e-03,  4.71728474e-01,\n",
       "        3.96615684e-01, -1.12529501e-01, -2.54358888e-01, -2.83672005e-01,\n",
       "        3.02062482e-01, -1.32212326e-01,  4.42414403e-01,  2.94468790e-01,\n",
       "        1.21766031e-01, -7.20783249e-02, -3.39876354e-01, -2.92524904e-01,\n",
       "       -1.89352900e-01,  2.73457587e-01, -2.41174191e-01, -7.23722503e-02,\n",
       "        6.91494197e-02,  3.09254587e-01, -1.91981152e-01, -3.61349940e-01,\n",
       "       -1.38134569e-01, -1.88890159e-01,  5.10626912e-01,  3.63482744e-01,\n",
       "        2.47130841e-02,  3.69699955e-01,  3.68476540e-01,  6.61514342e-01,\n",
       "        1.02441776e+00,  1.82997286e-01, -2.26496011e-01, -6.42516136e-01,\n",
       "       -1.04738459e-01,  1.85829356e-01,  5.92339873e-01,  4.76816222e-02,\n",
       "        1.73592776e-01, -6.32629022e-02,  1.14314109e-01,  2.81245530e-01,\n",
       "       -4.38797981e-01, -1.07995860e-01,  2.16354787e-01,  3.56218219e-01,\n",
       "       -2.98834164e-02,  3.83604586e-01, -2.02973604e-01,  2.52639681e-01,\n",
       "       -1.23557568e-01, -1.47867322e-01, -1.22693747e-01, -1.65841013e-01,\n",
       "        2.90254354e-01, -5.75084209e-01,  3.82012904e-01, -6.04162335e-01,\n",
       "       -1.38077080e-01, -1.79547310e-01, -3.01174253e-01, -3.59288454e-01,\n",
       "       -6.62706733e-01,  3.72732401e-01,  2.41458625e-01, -3.65143478e-01,\n",
       "        1.01024367e-01,  2.07394771e-02, -1.36021182e-01,  8.74111652e-02,\n",
       "        2.33260557e-01, -2.20200405e-01,  3.29984844e-01,  9.94610786e-01,\n",
       "       -3.81564289e-01,  4.55839425e-01, -3.81661765e-02, -3.14570367e-01,\n",
       "       -7.87771225e-01, -4.76504803e-01,  3.75115693e-01,  6.88303888e-01,\n",
       "        6.18016049e-02,  1.16711080e-01, -2.82953322e-01,  4.07918245e-02,\n",
       "        5.39169759e-02,  1.36292540e-02,  2.24170193e-01, -1.36858091e-01,\n",
       "       -7.19729587e-02, -3.07178348e-01, -3.18575025e-01,  2.57634163e-01,\n",
       "       -4.84975278e-02, -2.53151834e-01,  2.80238986e-01,  1.77789271e-01,\n",
       "        8.71491507e-02,  1.70427963e-01,  1.99158132e-01, -1.45030275e-01,\n",
       "        2.11684600e-01,  2.57575005e-01,  3.21730599e-02, -3.71196330e-01,\n",
       "       -1.56524569e-01, -4.27634954e-01, -2.00111762e-01,  2.00112909e-03,\n",
       "        1.11238718e-01,  6.49568737e-01, -3.65679145e-01, -3.08431387e-01,\n",
       "        5.58512330e-01, -7.91891515e-02,  1.02438591e-01, -9.24131721e-02,\n",
       "        1.55004915e-02,  5.37300706e-01, -4.97027665e-01, -4.86674756e-02,\n",
       "       -1.35427907e-01, -5.36279939e-02, -3.83026123e-01,  1.84012711e-01,\n",
       "        1.63800210e-01, -2.88008571e-01, -5.35461158e-02, -1.34804651e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f62a9fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: nlp_drift/embeddings.npy to s3://sagemaker-us-east-1-622343165275/sagemaker/nlp-data-drift-bert-model/embeddings/embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp nlp_drift/embeddings.npy s3://{bucket}/{model_prefix}/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506d3b9",
   "metadata": {},
   "source": [
    "#### Dockerfile create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c42c4257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-nlp/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-nlp/Dockerfile\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install sagemaker\n",
    "RUN pip3 install scipy\n",
    "RUN pip3 install transformers\n",
    "RUN pip3 install torch\n",
    "RUN pip3 install s3fs\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ADD evaluation.py /\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"/evaluation.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f619cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nlp_drift/code/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nlp_drift/code/evaluate.py\n",
    "\"\"\"Custom Model Monitoring script for Detecting Data Drift in NLP using SageMaker Model Monitor\n",
    "\"\"\"\n",
    "\n",
    "# Python Built-Ins:\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# External Dependencies:\n",
    "import numpy as np\n",
    "import boto3\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_environment():\n",
    "    \"\"\"Load configuration variables for SM Model Monitoring job\n",
    "\n",
    "    See https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-contract-inputs.html\n",
    "    \"\"\"\n",
    "    print(f\"nlp-drift::get_environment()::\")\n",
    "    try:\n",
    "        with open(\"/opt/ml/config/processingjobconfig.json\", \"r\") as conffile:\n",
    "            defaults = json.loads(conffile.read())[\"Environment\"]\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Unable to read environment vars from SM processing config file\")\n",
    "        defaults = {}\n",
    "\n",
    "    return SimpleNamespace(\n",
    "        dataset_format=os.environ.get(\"dataset_format\", defaults.get(\"dataset_format\")),\n",
    "        dataset_source=os.environ.get(\n",
    "            \"dataset_source\",\n",
    "            defaults.get(\"dataset_source\", \"/opt/ml/processing/input/endpoint\"),\n",
    "        ),\n",
    "        end_time=os.environ.get(\"end_time\", defaults.get(\"end_time\")),\n",
    "        output_path=os.environ.get(\n",
    "            \"output_path\",\n",
    "            defaults.get(\"output_path\", \"/opt/ml/processing/resultdata\"),\n",
    "        ),\n",
    "        publish_cloudwatch_metrics=os.environ.get(\n",
    "            \"publish_cloudwatch_metrics\",\n",
    "            defaults.get(\"publish_cloudwatch_metrics\", \"Enabled\"),\n",
    "        ),\n",
    "        sagemaker_endpoint_name=os.environ.get(\n",
    "            \"sagemaker_endpoint_name\",\n",
    "            defaults.get(\"sagemaker_endpoint_name\"),\n",
    "        ),\n",
    "        sagemaker_monitoring_schedule_name=os.environ.get(\n",
    "            \"sagemaker_monitoring_schedule_name\",\n",
    "            defaults.get(\"sagemaker_monitoring_schedule_name\"),\n",
    "        ),\n",
    "        start_time=os.environ.get(\n",
    "            \"start_time\", \n",
    "            defaults.get(\"start_time\")),\n",
    "        max_ratio_threshold=float(os.environ.get(\n",
    "            \"THRESHOLD\", \n",
    "             defaults.get(\"THRESHOLD\", \"nan\"))),\n",
    "        bucket=os.environ.get(\n",
    "            \"bucket\",\n",
    "            defaults.get(\"bucket\", \"None\")),\n",
    "    )\n",
    "\n",
    "\n",
    "def download_embeddings_file():\n",
    "    \n",
    "    env = get_environment()\n",
    "    print(f\"nlp-drift::Starting s3fs: download\")\n",
    "    \n",
    "    from s3fs.core import S3FileSystem\n",
    "    s3 = S3FileSystem()\n",
    "    \n",
    "    key = 'sagemaker/nlp-data-drift-bert-model/embeddings/embeddings.npy'\n",
    "    bucket = env.bucket\n",
    "    print(f\"nlp-drift::S3 bucket name is={bucket}\")\n",
    "\n",
    "    return np.load(s3.open('{}/{}'.format(bucket, key)))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    env = get_environment()\n",
    "    print(f\"nlp-drift::Starting evaluation with config\\n{env}\")\n",
    "\n",
    "    print(f\"nlp-drift::Downloading Embedding File\")\n",
    "    \n",
    "    #download BERT embedding file used for fine-tuning BertForSequenceClassification\n",
    "    embedding_list = download_embeddings_file()\n",
    "    \n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                      output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                      )\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    sent_cosine_dict = {}\n",
    "    violations = []\n",
    "    \n",
    "    total_record_count = 0  # Including error predictions that we can't read the response for\n",
    "    error_record_count = 0\n",
    "    counts = defaultdict(int)  # dict defaulting to 0 when unseen keys are requested\n",
    "    print(f\"nlp-drift::counts={counts}::\")\n",
    "    \n",
    "    for path, directories, filenames in os.walk(env.dataset_source):\n",
    "        for filename in filter(lambda f: f.lower().endswith(\".jsonl\"), filenames):\n",
    "            print(f\"nlp-drift::starting:DRIFT:Analysis:filename={filename}:\")\n",
    "            \n",
    "            with open(os.path.join(path, filename), \"r\") as file:\n",
    "                for entry in file:\n",
    "                    total_record_count += 1\n",
    "                    try:\n",
    "                        response = json.loads(json.loads(entry)[\"captureData\"][\"endpointInput\"][\"data\"])\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                    for record in response:\n",
    "                        encoded_dict = tokenizer.encode_plus(\n",
    "                            record, \n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 64,\n",
    "                            padding= True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                            truncation=True,\n",
    "                            )\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])\n",
    "                            hidden_states = outputs[2]\n",
    "                            token_vecs = hidden_states[-2][0]\n",
    "                            input_sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "                        \n",
    "                        cosine_score = 0\n",
    "                        \n",
    "                        for embed_item in embedding_list:\n",
    "                            cosine_score += (1 - cosine(input_sentence_embedding, embed_item))\n",
    "                            print(f\"nlp-drift::cos:={cosine_score}\")\n",
    "                            \n",
    "                        cosine_score_avg = cosine_score/(len(embedding_list))\n",
    "                        if cosine_score_avg < env.max_ratio_threshold:\n",
    "                            error_record_count += 1\n",
    "                            sent_cosine_dict[record] = cosine_score_avg\n",
    "                            violations.append({\n",
    "                                    \"sentence\": record,\n",
    "                                    \"avg_cosine_score\": cosine_score_avg,\n",
    "                                    \"feature_name\": \"sent_cosine_score\",\n",
    "                                    \"constraint_check_type\": \"baseline_drift_check\",\n",
    "                                    \"endpoint_name\" : env.sagemaker_endpoint_name,\n",
    "                                    \"monitoring_schedule_name\": env.sagemaker_monitoring_schedule_name\n",
    "                                })\n",
    "        \n",
    "    print(f\"nlp-drift::Checking for constraint violations...\")\n",
    "    print(f\"nlp-drift::Violations: {violations if len(violations) else 'None'}\")\n",
    "\n",
    "    print(f\"nlp-drift::Writing violations file...\")\n",
    "    with open(os.path.join(env.output_path, \"constraints_violations.json\"), \"w\") as outfile:\n",
    "        outfile.write(json.dumps(\n",
    "            { \"violations\": violations },\n",
    "            indent=4,\n",
    "        ))\n",
    "    \n",
    "    print(f\"nlp-drift::Writing overall status output...\")\n",
    "    with open(\"/opt/ml/output/message\", \"w\") as outfile:\n",
    "        if len(violations):\n",
    "            msg = ''\n",
    "            for v in violations:\n",
    "                msg += f\"CompletedWithViolations: {v['sentence']}\"\n",
    "                msg +=\"\\n\"\n",
    "        else:\n",
    "            msg = \"Completed: Job completed successfully with no violations.\"\n",
    "        outfile.write(msg)\n",
    "        print(msg)\n",
    "\n",
    "    if True:\n",
    "    #if env.publish_cloudwatch_metrics:\n",
    "        print(f\"nlp-drift::Writing CloudWatch metrics...\")\n",
    "        with open(\"/opt/ml/output/metrics/cloudwatch/cloudwatch_metrics.jsonl\", \"a+\") as outfile:\n",
    "            # One metric per line (JSONLines list of dictionaries)\n",
    "            # Remember these metrics are aggregated in graphs, so we report them as statistics on our dataset\n",
    "            outfile.write(json.dumps(\n",
    "            { \"violations\": violations },\n",
    "            indent=4,\n",
    "            ))\n",
    "    print(f\"nlp-drift::Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a2707",
   "metadata": {},
   "source": [
    "#### Build Custom Container for Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8afff0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'nlp-data-drift-bert-v1'\n",
    "tag = ':latest'\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c486fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp-data-drift-bert-v1'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecr_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "800bfcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-data-drift-bert-v1:latest\n",
      "622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-v1:latest\n"
     ]
    }
   ],
   "source": [
    "print(ecr_repository + tag)\n",
    "print(processing_repository_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b6a8aa6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository name is nlp-data-drift-bert-test\n",
      "account got =622343165275\n",
      "region got=us-east-1 \n",
      "Login Succeeded\n",
      "sha256:9af1451c3dcc51944a619729e081c6966a38ed7edb5f09c7206768b7f5ce0fe0\n",
      "Docker image created repo:name:or:algorithm_name=nlp-data-drift-bert-test fullName= 622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-test:latest\n",
      "Docker push full name image=622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-test:latest\n",
      "The push refers to repository [622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-test]\n",
      "d8e3719b5b21: Preparing\n",
      "6fd29d78e330: Preparing\n",
      "60d5590dfd0f: Preparing\n",
      "ea77d2057171: Preparing\n",
      "fea0a1339bc6: Preparing\n",
      "bfb7ec3efbd0: Preparing\n",
      "7aa05e0971fb: Preparing\n",
      "578ed91ab344: Preparing\n",
      "eaa28ace589d: Preparing\n",
      "2012c49cb260: Preparing\n",
      "926aa14921f2: Preparing\n",
      "e06e631d87d6: Preparing\n",
      "bfb7ec3efbd0: Waiting\n",
      "7aa05e0971fb: Waiting\n",
      "eaa28ace589d: Waiting\n",
      "926aa14921f2: Waiting\n",
      "e06e631d87d6: Waiting\n",
      "ea77d2057171: Retrying in 5 seconds\n",
      "60d5590dfd0f: Retrying in 5 seconds\n",
      "6fd29d78e330: Retrying in 5 seconds\n",
      "fea0a1339bc6: Retrying in 5 seconds\n",
      "d8e3719b5b21: Retrying in 5 seconds\n",
      "ea77d2057171: Retrying in 4 seconds\n",
      "60d5590dfd0f: Retrying in 4 seconds\n",
      "6fd29d78e330: Retrying in 4 seconds\n",
      "fea0a1339bc6: Retrying in 4 seconds\n",
      "d8e3719b5b21: Retrying in 4 seconds\n",
      "ea77d2057171: Retrying in 3 seconds\n",
      "6fd29d78e330: Retrying in 3 seconds\n",
      "60d5590dfd0f: Retrying in 3 seconds\n",
      "fea0a1339bc6: Retrying in 3 seconds\n",
      "d8e3719b5b21: Retrying in 3 seconds\n",
      "ea77d2057171: Retrying in 2 seconds\n",
      "6fd29d78e330: Retrying in 2 seconds\n",
      "60d5590dfd0f: Retrying in 2 seconds\n",
      "fea0a1339bc6: Retrying in 2 seconds\n",
      "d8e3719b5b21: Retrying in 2 seconds\n",
      "ea77d2057171: Retrying in 1 second\n",
      "60d5590dfd0f: Retrying in 1 second\n",
      "6fd29d78e330: Retrying in 1 second\n",
      "fea0a1339bc6: Retrying in 1 second\n",
      "d8e3719b5b21: Retrying in 1 second\n",
      "ea77d2057171: Retrying in 10 seconds\n",
      "60d5590dfd0f: Retrying in 10 seconds\n",
      "fea0a1339bc6: Retrying in 10 seconds\n",
      "d8e3719b5b21: Retrying in 10 seconds\n",
      "6fd29d78e330: Retrying in 10 seconds\n",
      "ea77d2057171: Retrying in 9 seconds\n",
      "60d5590dfd0f: Retrying in 9 seconds\n",
      "fea0a1339bc6: Retrying in 9 seconds\n",
      "6fd29d78e330: Retrying in 9 seconds\n",
      "d8e3719b5b21: Retrying in 9 seconds\n",
      "ea77d2057171: Retrying in 8 seconds\n",
      "60d5590dfd0f: Retrying in 8 seconds\n",
      "fea0a1339bc6: Retrying in 8 seconds\n",
      "d8e3719b5b21: Retrying in 8 seconds\n",
      "6fd29d78e330: Retrying in 8 seconds\n",
      "ea77d2057171: Retrying in 7 seconds\n",
      "60d5590dfd0f: Retrying in 7 seconds\n",
      "fea0a1339bc6: Retrying in 7 seconds\n",
      "6fd29d78e330: Retrying in 7 seconds\n",
      "d8e3719b5b21: Retrying in 7 seconds\n",
      "ea77d2057171: Retrying in 6 seconds\n",
      "60d5590dfd0f: Retrying in 6 seconds\n",
      "fea0a1339bc6: Retrying in 6 seconds\n",
      "6fd29d78e330: Retrying in 6 seconds\n",
      "d8e3719b5b21: Retrying in 6 seconds\n",
      "ea77d2057171: Retrying in 5 seconds\n",
      "60d5590dfd0f: Retrying in 5 seconds\n",
      "fea0a1339bc6: Retrying in 5 seconds\n",
      "d8e3719b5b21: Retrying in 5 seconds\n",
      "6fd29d78e330: Retrying in 5 seconds\n",
      "ea77d2057171: Retrying in 4 seconds\n",
      "60d5590dfd0f: Retrying in 4 seconds\n",
      "fea0a1339bc6: Retrying in 4 seconds\n",
      "6fd29d78e330: Retrying in 4 seconds\n",
      "d8e3719b5b21: Retrying in 4 seconds\n",
      "ea77d2057171: Retrying in 3 seconds\n",
      "60d5590dfd0f: Retrying in 3 seconds\n",
      "fea0a1339bc6: Retrying in 3 seconds\n",
      "6fd29d78e330: Retrying in 3 seconds\n",
      "d8e3719b5b21: Retrying in 3 seconds\n",
      "ea77d2057171: Retrying in 2 seconds\n",
      "60d5590dfd0f: Retrying in 2 seconds\n",
      "fea0a1339bc6: Retrying in 2 seconds\n",
      "d8e3719b5b21: Retrying in 2 seconds\n",
      "6fd29d78e330: Retrying in 2 seconds\n",
      "ea77d2057171: Retrying in 1 second\n",
      "60d5590dfd0f: Retrying in 1 second\n",
      "fea0a1339bc6: Retrying in 1 second\n",
      "6fd29d78e330: Retrying in 1 second\n",
      "d8e3719b5b21: Retrying in 1 second\n",
      "ea77d2057171: Retrying in 15 seconds\n",
      "6fd29d78e330: Retrying in 15 seconds\n",
      "d8e3719b5b21: Retrying in 15 seconds\n",
      "60d5590dfd0f: Retrying in 15 seconds\n",
      "fea0a1339bc6: Retrying in 15 seconds\n",
      "ea77d2057171: Retrying in 14 seconds\n",
      "6fd29d78e330: Retrying in 14 seconds\n",
      "d8e3719b5b21: Retrying in 14 seconds\n",
      "60d5590dfd0f: Retrying in 14 seconds\n",
      "fea0a1339bc6: Retrying in 14 seconds\n",
      "ea77d2057171: Retrying in 13 seconds\n",
      "6fd29d78e330: Retrying in 13 seconds\n",
      "d8e3719b5b21: Retrying in 13 seconds\n",
      "60d5590dfd0f: Retrying in 13 seconds\n",
      "fea0a1339bc6: Retrying in 13 seconds\n",
      "ea77d2057171: Retrying in 12 seconds\n",
      "6fd29d78e330: Retrying in 12 seconds\n",
      "60d5590dfd0f: Retrying in 12 seconds\n",
      "d8e3719b5b21: Retrying in 12 seconds\n",
      "fea0a1339bc6: Retrying in 12 seconds\n",
      "ea77d2057171: Retrying in 11 seconds\n",
      "6fd29d78e330: Retrying in 11 seconds\n",
      "d8e3719b5b21: Retrying in 11 seconds\n",
      "60d5590dfd0f: Retrying in 11 seconds\n",
      "fea0a1339bc6: Retrying in 11 seconds\n",
      "ea77d2057171: Retrying in 10 seconds\n",
      "6fd29d78e330: Retrying in 10 seconds\n",
      "d8e3719b5b21: Retrying in 10 seconds\n",
      "60d5590dfd0f: Retrying in 10 seconds\n",
      "fea0a1339bc6: Retrying in 10 seconds\n",
      "ea77d2057171: Retrying in 9 seconds\n",
      "6fd29d78e330: Retrying in 9 seconds\n",
      "d8e3719b5b21: Retrying in 9 seconds\n",
      "60d5590dfd0f: Retrying in 9 seconds\n",
      "fea0a1339bc6: Retrying in 9 seconds\n",
      "ea77d2057171: Retrying in 8 seconds\n",
      "6fd29d78e330: Retrying in 8 seconds\n",
      "d8e3719b5b21: Retrying in 8 seconds\n",
      "60d5590dfd0f: Retrying in 8 seconds\n",
      "fea0a1339bc6: Retrying in 8 seconds\n",
      "ea77d2057171: Retrying in 7 seconds\n",
      "6fd29d78e330: Retrying in 7 seconds\n",
      "60d5590dfd0f: Retrying in 7 seconds\n",
      "d8e3719b5b21: Retrying in 7 seconds\n",
      "fea0a1339bc6: Retrying in 7 seconds\n",
      "ea77d2057171: Retrying in 6 seconds\n",
      "6fd29d78e330: Retrying in 6 seconds\n",
      "d8e3719b5b21: Retrying in 6 seconds\n",
      "60d5590dfd0f: Retrying in 6 seconds\n",
      "fea0a1339bc6: Retrying in 6 seconds\n",
      "ea77d2057171: Retrying in 5 seconds\n",
      "6fd29d78e330: Retrying in 5 seconds\n",
      "d8e3719b5b21: Retrying in 5 seconds\n",
      "60d5590dfd0f: Retrying in 5 seconds\n",
      "fea0a1339bc6: Retrying in 5 seconds\n",
      "ea77d2057171: Retrying in 4 seconds\n",
      "6fd29d78e330: Retrying in 4 seconds\n",
      "60d5590dfd0f: Retrying in 4 seconds\n",
      "d8e3719b5b21: Retrying in 4 seconds\n",
      "fea0a1339bc6: Retrying in 4 seconds\n",
      "ea77d2057171: Retrying in 3 seconds\n",
      "6fd29d78e330: Retrying in 3 seconds\n",
      "d8e3719b5b21: Retrying in 3 seconds\n",
      "60d5590dfd0f: Retrying in 3 seconds\n",
      "fea0a1339bc6: Retrying in 3 seconds\n",
      "ea77d2057171: Retrying in 2 seconds\n",
      "6fd29d78e330: Retrying in 2 seconds\n",
      "60d5590dfd0f: Retrying in 2 seconds\n",
      "d8e3719b5b21: Retrying in 2 seconds\n",
      "fea0a1339bc6: Retrying in 2 seconds\n",
      "ea77d2057171: Retrying in 1 second\n",
      "6fd29d78e330: Retrying in 1 second\n",
      "60d5590dfd0f: Retrying in 1 second\n",
      "d8e3719b5b21: Retrying in 1 second\n",
      "fea0a1339bc6: Retrying in 1 second\n",
      "60d5590dfd0f: Retrying in 20 seconds\n",
      "fea0a1339bc6: Retrying in 20 seconds\n",
      "6fd29d78e330: Retrying in 20 seconds\n",
      "ea77d2057171: Retrying in 20 seconds\n",
      "d8e3719b5b21: Retrying in 20 seconds\n",
      "fea0a1339bc6: Retrying in 19 seconds\n",
      "60d5590dfd0f: Retrying in 19 seconds\n",
      "6fd29d78e330: Retrying in 19 seconds\n",
      "ea77d2057171: Retrying in 19 seconds\n",
      "d8e3719b5b21: Retrying in 19 seconds\n",
      "60d5590dfd0f: Retrying in 18 seconds\n",
      "fea0a1339bc6: Retrying in 18 seconds\n",
      "ea77d2057171: Retrying in 18 seconds\n",
      "6fd29d78e330: Retrying in 18 seconds\n",
      "d8e3719b5b21: Retrying in 18 seconds\n",
      "fea0a1339bc6: Retrying in 17 seconds\n",
      "60d5590dfd0f: Retrying in 17 seconds\n",
      "ea77d2057171: Retrying in 17 seconds\n",
      "6fd29d78e330: Retrying in 17 seconds\n",
      "d8e3719b5b21: Retrying in 17 seconds\n",
      "6fd29d78e330: Retrying in 16 seconds\n",
      "ea77d2057171: Retrying in 16 seconds\n",
      "60d5590dfd0f: Retrying in 16 seconds\n",
      "fea0a1339bc6: Retrying in 16 seconds\n",
      "d8e3719b5b21: Retrying in 16 seconds\n",
      "60d5590dfd0f: Retrying in 15 seconds\n",
      "fea0a1339bc6: Retrying in 15 seconds\n",
      "6fd29d78e330: Retrying in 15 seconds\n",
      "ea77d2057171: Retrying in 15 seconds\n",
      "d8e3719b5b21: Retrying in 15 seconds\n",
      "60d5590dfd0f: Retrying in 14 seconds\n",
      "fea0a1339bc6: Retrying in 14 seconds\n",
      "ea77d2057171: Retrying in 14 seconds\n",
      "6fd29d78e330: Retrying in 14 seconds\n",
      "d8e3719b5b21: Retrying in 14 seconds\n",
      "fea0a1339bc6: Retrying in 13 seconds\n",
      "60d5590dfd0f: Retrying in 13 seconds\n",
      "ea77d2057171: Retrying in 13 seconds\n",
      "6fd29d78e330: Retrying in 13 seconds\n",
      "d8e3719b5b21: Retrying in 13 seconds\n",
      "6fd29d78e330: Retrying in 12 seconds\n",
      "fea0a1339bc6: Retrying in 12 seconds\n",
      "ea77d2057171: Retrying in 12 seconds\n",
      "60d5590dfd0f: Retrying in 12 seconds\n",
      "d8e3719b5b21: Retrying in 12 seconds\n",
      "60d5590dfd0f: Retrying in 11 seconds\n",
      "fea0a1339bc6: Retrying in 11 seconds\n",
      "6fd29d78e330: Retrying in 11 seconds\n",
      "ea77d2057171: Retrying in 11 seconds\n",
      "d8e3719b5b21: Retrying in 11 seconds\n",
      "fea0a1339bc6: Retrying in 10 seconds\n",
      "60d5590dfd0f: Retrying in 10 seconds\n",
      "6fd29d78e330: Retrying in 10 seconds\n",
      "ea77d2057171: Retrying in 10 seconds\n",
      "d8e3719b5b21: Retrying in 10 seconds\n",
      "60d5590dfd0f: Retrying in 9 seconds\n",
      "fea0a1339bc6: Retrying in 9 seconds\n",
      "ea77d2057171: Retrying in 9 seconds\n",
      "6fd29d78e330: Retrying in 9 seconds\n",
      "d8e3719b5b21: Retrying in 9 seconds\n",
      "60d5590dfd0f: Retrying in 8 seconds\n",
      "fea0a1339bc6: Retrying in 8 seconds\n",
      "ea77d2057171: Retrying in 8 seconds\n",
      "6fd29d78e330: Retrying in 8 seconds\n",
      "d8e3719b5b21: Retrying in 8 seconds\n",
      "fea0a1339bc6: Retrying in 7 seconds\n",
      "60d5590dfd0f: Retrying in 7 seconds\n",
      "ea77d2057171: Retrying in 7 seconds\n",
      "6fd29d78e330: Retrying in 7 seconds\n",
      "d8e3719b5b21: Retrying in 7 seconds\n",
      "60d5590dfd0f: Retrying in 6 seconds\n",
      "fea0a1339bc6: Retrying in 6 seconds\n",
      "ea77d2057171: Retrying in 6 seconds\n",
      "6fd29d78e330: Retrying in 6 seconds\n",
      "d8e3719b5b21: Retrying in 6 seconds\n",
      "60d5590dfd0f: Retrying in 5 seconds\n",
      "fea0a1339bc6: Retrying in 5 seconds\n",
      "ea77d2057171: Retrying in 5 seconds\n",
      "6fd29d78e330: Retrying in 5 seconds\n",
      "d8e3719b5b21: Retrying in 5 seconds\n",
      "fea0a1339bc6: Retrying in 4 seconds\n",
      "60d5590dfd0f: Retrying in 4 seconds\n",
      "ea77d2057171: Retrying in 4 seconds\n",
      "6fd29d78e330: Retrying in 4 seconds\n",
      "d8e3719b5b21: Retrying in 4 seconds\n",
      "6fd29d78e330: Retrying in 3 seconds\n",
      "fea0a1339bc6: Retrying in 3 seconds\n",
      "ea77d2057171: Retrying in 3 seconds\n",
      "60d5590dfd0f: Retrying in 3 seconds\n",
      "d8e3719b5b21: Retrying in 3 seconds\n",
      "60d5590dfd0f: Retrying in 2 seconds\n",
      "6fd29d78e330: Retrying in 2 seconds\n",
      "fea0a1339bc6: Retrying in 2 seconds\n",
      "ea77d2057171: Retrying in 2 seconds\n",
      "d8e3719b5b21: Retrying in 2 seconds\n",
      "60d5590dfd0f: Retrying in 1 second\n",
      "fea0a1339bc6: Retrying in 1 second\n",
      "6fd29d78e330: Retrying in 1 second\n",
      "ea77d2057171: Retrying in 1 second\n",
      "d8e3719b5b21: Retrying in 1 second\n",
      "2012c49cb260: Waiting\n",
      "578ed91ab344: Waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "EOF\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# The name of our algorithm\\nalgorithm_name=nlp-data-drift-bert-test\\n\\ncd docker-nlp\\n\\necho \"Repository name is $algorithm_name\"\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\necho \"account got =$account\"\\n# Get the region defined in the current configuration (default to us-east-1 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-east-1}\\necho \"region got=$region \"\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\n$(aws ecr get-login --region ${region} --no-include-email)\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build -q -t ${algorithm_name} .\\n\\necho \"Docker image created repo:name:or:algorithm_name=$algorithm_name fullName= $fullname\"\\ndocker tag ${algorithm_name} ${fullname}\\n\\necho \"Docker push full name image=$fullname\"\\ndocker push ${fullname}\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47618/1707274872.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# The name of our algorithm\\nalgorithm_name=nlp-data-drift-bert-test\\n\\ncd docker-nlp\\n\\necho \"Repository name is $algorithm_name\"\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\necho \"account got =$account\"\\n# Get the region defined in the current configuration (default to us-east-1 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-east-1}\\necho \"region got=$region \"\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\n$(aws ecr get-login --region ${region} --no-include-email)\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build -q -t ${algorithm_name} .\\n\\necho \"Docker image created repo:name:or:algorithm_name=$algorithm_name fullName= $fullname\"\\ndocker tag ${algorithm_name} ${fullname}\\n\\necho \"Docker push full name image=$fullname\"\\ndocker push ${fullname}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2461\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2462\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# The name of our algorithm\\nalgorithm_name=nlp-data-drift-bert-test\\n\\ncd docker-nlp\\n\\necho \"Repository name is $algorithm_name\"\\n\\naccount=$(aws sts get-caller-identity --query Account --output text)\\necho \"account got =$account\"\\n# Get the region defined in the current configuration (default to us-east-1 if none defined)\\nregion=$(aws configure get region)\\nregion=${region:-us-east-1}\\necho \"region got=$region \"\\n\\nfullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\\n\\n# If the repository doesn\\'t exist in ECR, create it.\\naws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\\n\\nif [ $? -ne 0 ]\\nthen\\n    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\\nfi\\n\\n# Get the login command from ECR and execute it directly\\n$(aws ecr get-login --region ${region} --no-include-email)\\n\\n# Build the docker image locally with the image name and then push it to ECR\\n# with the full name.\\n\\ndocker build -q -t ${algorithm_name} .\\n\\necho \"Docker image created repo:name:or:algorithm_name=$algorithm_name fullName= $fullname\"\\ndocker tag ${algorithm_name} ${fullname}\\n\\necho \"Docker push full name image=$fullname\"\\ndocker push ${fullname}\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=nlp-data-drift-bert-test\n",
    "\n",
    "cd docker-nlp\n",
    "\n",
    "echo \"Repository name is $algorithm_name\"\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo \"account got =$account\"\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "echo \"region got=$region \"\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -q -t ${algorithm_name} .\n",
    "\n",
    "echo \"Docker image created repo:name:or:algorithm_name=$algorithm_name fullName= $fullname\"\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "echo \"Docker push full name image=$fullname\"\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde7ef5",
   "metadata": {},
   "source": [
    "#### Now create the monitoring schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29034d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import ModelMonitor\n",
    "\n",
    "monitor = ModelMonitor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5', 'bucket': bucket },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3253ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session.default_bucket()}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "\n",
    "processing_output = ProcessingOutput(\n",
    "    output_name='result',\n",
    "    source='/opt/ml/processing/resultdata',\n",
    "    destination=destination,\n",
    ")\n",
    "output = MonitoringOutput(source=processing_output.source, destination=processing_output.destination)\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='nlp-data-drift-bert-schedule',\n",
    "    output=output,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b32278",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = monitor.list_executions()\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(jobs) > 0:\n",
    "    last_execution_desc = monitor.list_executions()[-1].describe()\n",
    "    print(last_execution_desc)\n",
    "    print(f'\\nExit Message: {last_execution_desc.get(\"ExitMessage\", \"None\")}')\n",
    "else:\n",
    "    print(\"\"\"No processing job has been executed yet. \n",
    "    This means that one hour has not passed yet. \n",
    "    You can go to the next code cell and run the processing job manually\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bcb282b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-v1:latest'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c7d35b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-v1:latest'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image_uri_processing = sagemaker.image_uris.retrieve('processing_repository_uri', 'us-east-1')\n",
    "container = \"{}.dkr.ecr.{}.amazonaws.com/nlp-data-drift-bert-v1:latest\".format(\n",
    "    account_id, region\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1de92c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'622343165275.dkr.ecr.us-east-1.amazonaws.com/nlp-data-drift-bert-test:latest'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container = \"{}.dkr.ecr.{}.amazonaws.com/nlp-data-drift-bert-test:latest\".format(\n",
    "    account_id, region\n",
    ")\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc25c3b",
   "metadata": {},
   "source": [
    "### MANUALLY run the PROCESSING JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadf132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  nlp-data-drift-bert-v1-2022-09-19-06-36-20-256\n",
      "Inputs:  [{'InputName': 'endpointdata', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-622343165275/sagemaker/CustomModelMonitor/datacapture/nlp-data-drift-bert-endpoint', 'LocalPath': '/opt/ml/processing/input/endpoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-622343165275/sagemaker/CustomModelMonitor/nlp-data-drift-bert-endpoint/monitoring_schedule', 'LocalPath': '/opt/ml/processing/resultdata', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........."
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.model_monitor import CronExpressionGenerator, MonitoringOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "destination = f's3://{sagemaker_session.default_bucket()}/{prefix}/{endpoint_name}/monitoring_schedule'\n",
    "processor = Processor(\n",
    "    base_job_name='nlp-data-drift-bert-v1',\n",
    "    role=role,\n",
    "    image_uri=container, #processing_repository_uri,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    env={ 'THRESHOLD':'0.5','bucket': bucket },\n",
    ")\n",
    "    \n",
    "processor.run(\n",
    "    [ProcessingInput(\n",
    "        input_name='endpointdata',\n",
    "        source = \"s3://{}/{}/{}\".format(bucket, data_capture_prefix,endpoint_name),\n",
    "        #source=f's3://{sagemaker_session.default_bucket()}/{s3_prefix}/endpoint/data_capture',\n",
    "        destination = '/opt/ml/processing/input/endpoint',\n",
    "    )],\n",
    "    [ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/resultdata',\n",
    "        destination=destination,\n",
    "    )],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2febd",
   "metadata": {},
   "source": [
    "#### Clean up Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the monitoring schedule\n",
    "monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
