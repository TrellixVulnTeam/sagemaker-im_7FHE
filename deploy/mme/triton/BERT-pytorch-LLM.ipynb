{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7f0a6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb289a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "\n",
    "!pip install -qU pip awscli boto3 sagemaker transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81dbbd",
   "metadata": {},
   "source": [
    "### Test Create BERT model from HuggingFace\n",
    "**If you use from BERT it comes without HEAD o you need to add Head or alternately download from HuggingFace or using Auto so you get with Head**\n",
    "\n",
    "**Test how to create BERT torchscript model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "\n",
    "enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizing input text\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = enc.tokenize(text)\n",
    "print(f\"BERT:Tokenized:Text={tokenized_text}:::\")\n",
    "\n",
    "# Masking one of the input tokens\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"BERT:indexed_tokens:={indexed_tokens}::\")\n",
    "\n",
    "# -- segments id's\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Creating a dummy input\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "dummy_model_input = {'input_ids':tokens_tensor, 'attention_mask':segments_tensors }\n",
    "print(f\"BERT:Combining:DICT: all: creating dummy:input:Model:={dummy_model_input}::\")\n",
    "\n",
    "dummy_input = [tokens_tensor, segments_tensors]\n",
    "print(f\"BERT:Finally combining all: creating dummy:input={dummy_input}::\")\n",
    "\n",
    "# Initializing the model with the torchscript flag\n",
    "# Flag set to True even though it is not necessary as this model does not have an LM Head.\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, torchscript=True)\n",
    "\n",
    "# Instantiating the model\n",
    "model = BertModel(config)\n",
    "\n",
    "# The model needs to be in evaluation mode for torchscript \n",
    "model.eval()\n",
    "\n",
    "# If you are instantiating the model with `from_pretrained` you can also easily set the TorchScript flag\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
    "\n",
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n",
    "#torch.jit.save(traced_model, \"./bert-uc/traced_bert.pt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67d9d7da",
   "metadata": {},
   "source": [
    "for quantized export of the model to reduce size\n",
    "\n",
    "python convert_graph_to_onnx.py --framework <pt, tf> --model bert-base-cased --quantize bert-base-cased.onnx\n",
    "\n",
    "from transformers import convert_graph_to_onnx\n",
    "!python convert_graph_to_onnx.py --framework pt --model ./bert-uc/traced_bert.pt --quantize bert-base-uncased.onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dadaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using the BERT Tokensizer::\")\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(dummy_input)\n",
    "print(dummy_model_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda06a0",
   "metadata": {},
   "source": [
    "dummy_model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fd6c3",
   "metadata": {},
   "source": [
    "**Test Tokenizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef49f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dummy_model_bert_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"Using the BERT:AUTO:TOKENSIZER: Tokenizer::\")\n",
    "print(dummy_model_bert_input) # -- dict -- input id's and attention mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d57565",
   "metadata": {},
   "source": [
    "### Export as ONYX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c59857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# export\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple(dummy_model_input.values()), #tuple(dummy_model_input.values()),\n",
    "    f=\"./bert-uc/torch-model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=13, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ad47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokens_tensor, segments_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "# If you are instantiating the model with `from_pretrained` you can also easily set the TorchScript flag\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
    "\n",
    "# Change to eva lmodel\n",
    "model.eval()\n",
    "\n",
    "# run a dummy prediction of tokens by tensors\n",
    "output = model(tokens_tensor)\n",
    "print(len(output), type(output), type(output[0]))\n",
    "\n",
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n",
    "torch.jit.save(traced_model, \"./triton-serve/bert-uc/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2235417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:Tokenized:Text=['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'puppet', '##eer', '[SEP]']:::\n",
      "BERT:indexed_tokens:=[101, 2040, 2001, 3958, 27227, 1029, 102, 3958, 103, 2001, 1037, 13997, 11510, 102]::\n",
      "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
      "          1037, 13997, 11510,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "\n",
    "enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizing input text\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = enc.tokenize(text)\n",
    "print(f\"BERT:Tokenized:Text={tokenized_text}:::\")\n",
    "\n",
    "# Masking one of the input tokens\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"BERT:indexed_tokens:={indexed_tokens}::\")\n",
    "\n",
    "# -- segments id's -- CAN WE GENERATE THEM via model\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Creating a dummy input\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5a48c",
   "metadata": {},
   "source": [
    "## Start actual BERT with Head for Triton\n",
    "\n",
    "### This model from HuggingFace does not take the attension ID's so accepts only 1 input\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  \n",
    "  \n",
    "    {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "77293e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting triton-serve/bert-uc/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile triton-serve/bert-uc/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ee6c9",
   "metadata": {},
   "source": [
    "### Run for Triton server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa8c24",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bert-uc\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29b06400",
   "metadata": {},
   "source": [
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bert-uc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e70b77",
   "metadata": {},
   "source": [
    "**Have to use the same Tokenizer to generate the input to test as BERT uncased**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7e6cf",
   "metadata": {},
   "source": [
    "### Create the BERT Model in Torch Script mode -- .pt model\n",
    "use the ore trained and use torchscript flag here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b7fb2",
   "metadata": {},
   "source": [
    "### Create the LARGE CASE BERT Model in Torch Script using dummy inputs -- .pt model\n",
    "Create using the dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "703ab6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPTJModel\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b8e078eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using a model of type bert to instantiate a model of type gptj. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing GPTJForCausalLM: ['bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.13.output.dense.weight', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.16.output.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.22.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.18.attention.output.dense.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.12.output.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.23.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.23.output.dense.weight', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.14.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.22.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.self.key.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.19.output.dense.bias', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.15.attention.self.value.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.12.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.14.attention.output.dense.bias', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.15.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.23.output.dense.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.15.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.21.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.23.attention.self.key.weight', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.key.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.15.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTJForCausalLM were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['h.16.attn.q_proj.weight', 'h.21.ln_1.weight', 'h.4.mlp.fc_in.bias', 'h.8.attn.k_proj.weight', 'h.8.mlp.fc_in.weight', 'h.7.attn.k_proj.weight', 'h.23.mlp.fc_in.bias', 'h.4.attn.k_proj.weight', 'h.15.mlp.fc_in.bias', 'h.18.mlp.fc_out.bias', 'h.8.ln_1.weight', 'h.22.mlp.fc_out.bias', 'h.16.attn.v_proj.weight', 'h.0.attn.out_proj.weight', 'h.18.attn.q_proj.weight', 'h.5.mlp.fc_out.weight', 'h.2.mlp.fc_in.weight', 'h.23.attn.out_proj.weight', 'h.14.ln_1.weight', 'h.15.mlp.fc_in.weight', 'h.1.attn.out_proj.weight', 'h.4.ln_1.weight', 'h.4.attn.out_proj.weight', 'h.9.mlp.fc_in.bias', 'h.13.mlp.fc_out.bias', 'h.22.attn.k_proj.weight', 'h.7.attn.q_proj.weight', 'h.22.mlp.fc_out.weight', 'h.19.attn.q_proj.weight', 'h.19.attn.k_proj.weight', 'h.21.mlp.fc_in.weight', 'h.4.ln_1.bias', 'h.11.attn.q_proj.weight', 'h.21.attn.q_proj.weight', 'h.21.mlp.fc_out.bias', 'h.22.attn.q_proj.weight', 'h.14.ln_1.bias', 'h.7.attn.v_proj.weight', 'h.1.mlp.fc_in.bias', 'h.3.attn.out_proj.weight', 'h.11.mlp.fc_out.weight', 'h.11.attn.out_proj.weight', 'h.15.mlp.fc_out.weight', 'h.6.ln_1.bias', 'h.0.ln_1.weight', 'h.17.ln_1.weight', 'ln_f.bias', 'lm_head.weight', 'h.13.mlp.fc_in.weight', 'h.8.ln_1.bias', 'h.20.mlp.fc_in.bias', 'h.0.mlp.fc_in.weight', 'h.5.mlp.fc_in.weight', 'h.9.mlp.fc_out.weight', 'h.1.attn.q_proj.weight', 'h.13.attn.k_proj.weight', 'h.17.ln_1.bias', 'h.21.attn.k_proj.weight', 'h.9.mlp.fc_in.weight', 'h.20.ln_1.weight', 'h.3.mlp.fc_out.bias', 'h.17.mlp.fc_in.weight', 'h.18.attn.v_proj.weight', 'h.22.mlp.fc_in.weight', 'lm_head.bias', 'h.11.mlp.fc_in.bias', 'h.14.mlp.fc_in.weight', 'h.14.attn.k_proj.weight', 'h.18.ln_1.weight', 'wte.weight', 'h.3.ln_1.weight', 'h.9.ln_1.bias', 'h.19.attn.v_proj.weight', 'h.22.ln_1.bias', 'h.3.attn.k_proj.weight', 'h.11.attn.v_proj.weight', 'h.23.ln_1.weight', 'h.8.mlp.fc_out.bias', 'h.9.attn.k_proj.weight', 'h.10.mlp.fc_in.bias', 'h.13.mlp.fc_out.weight', 'h.15.attn.out_proj.weight', 'h.1.attn.v_proj.weight', 'h.1.mlp.fc_in.weight', 'h.4.mlp.fc_out.weight', 'h.10.attn.out_proj.weight', 'h.9.mlp.fc_out.bias', 'h.20.mlp.fc_out.weight', 'h.8.mlp.fc_in.bias', 'h.2.attn.v_proj.weight', 'h.10.ln_1.bias', 'h.5.ln_1.weight', 'h.8.attn.v_proj.weight', 'h.9.attn.out_proj.weight', 'h.15.attn.q_proj.weight', 'h.23.mlp.fc_out.weight', 'h.20.attn.q_proj.weight', 'h.23.mlp.fc_in.weight', 'h.7.ln_1.weight', 'h.4.mlp.fc_out.bias', 'h.9.attn.q_proj.weight', 'h.9.attn.v_proj.weight', 'h.6.mlp.fc_out.bias', 'h.3.ln_1.bias', 'h.18.attn.k_proj.weight', 'h.1.attn.k_proj.weight', 'h.13.mlp.fc_in.bias', 'h.5.mlp.fc_in.bias', 'h.1.mlp.fc_out.weight', 'h.7.mlp.fc_in.bias', 'h.10.mlp.fc_out.weight', 'h.16.mlp.fc_out.weight', 'h.2.mlp.fc_out.weight', 'h.8.mlp.fc_out.weight', 'h.17.attn.k_proj.weight', 'h.3.mlp.fc_in.weight', 'h.14.mlp.fc_out.bias', 'h.19.mlp.fc_in.bias', 'h.12.mlp.fc_out.weight', 'h.14.attn.out_proj.weight', 'h.3.mlp.fc_in.bias', 'h.7.mlp.fc_out.bias', 'h.20.attn.k_proj.weight', 'h.13.attn.q_proj.weight', 'h.6.attn.out_proj.weight', 'h.15.ln_1.weight', 'h.18.mlp.fc_out.weight', 'h.16.mlp.fc_in.bias', 'h.12.ln_1.bias', 'h.12.attn.q_proj.weight', 'h.3.mlp.fc_out.weight', 'h.0.ln_1.bias', 'h.20.mlp.fc_in.weight', 'h.22.mlp.fc_in.bias', 'h.12.attn.out_proj.weight', 'h.0.attn.k_proj.weight', 'h.19.mlp.fc_out.weight', 'h.16.mlp.fc_out.bias', 'h.1.ln_1.weight', 'h.3.attn.v_proj.weight', 'h.19.attn.out_proj.weight', 'h.0.mlp.fc_out.bias', 'h.6.mlp.fc_out.weight', 'h.16.mlp.fc_in.weight', 'h.10.ln_1.weight', 'h.12.attn.v_proj.weight', 'h.18.ln_1.bias', 'h.14.attn.q_proj.weight', 'h.13.attn.out_proj.weight', 'h.5.attn.k_proj.weight', 'h.12.mlp.fc_out.bias', 'h.17.mlp.fc_out.weight', 'h.2.mlp.fc_out.bias', 'h.14.mlp.fc_out.weight', 'h.23.attn.k_proj.weight', 'h.10.attn.k_proj.weight', 'h.12.ln_1.weight', 'h.13.ln_1.bias', 'h.17.mlp.fc_out.bias', 'h.11.mlp.fc_in.weight', 'h.23.mlp.fc_out.bias', 'h.1.ln_1.bias', 'h.8.attn.q_proj.weight', 'h.10.attn.q_proj.weight', 'h.21.ln_1.bias', 'h.11.ln_1.weight', 'h.11.attn.k_proj.weight', 'h.11.ln_1.bias', 'h.2.ln_1.bias', 'h.6.mlp.fc_in.bias', 'h.16.ln_1.bias', 'h.12.mlp.fc_in.weight', 'h.20.ln_1.bias', 'ln_f.weight', 'h.19.ln_1.bias', 'h.6.ln_1.weight', 'h.10.mlp.fc_out.bias', 'h.20.attn.v_proj.weight', 'h.17.attn.out_proj.weight', 'h.21.attn.v_proj.weight', 'h.5.mlp.fc_out.bias', 'h.11.mlp.fc_out.bias', 'h.12.mlp.fc_in.bias', 'h.16.attn.k_proj.weight', 'h.16.attn.out_proj.weight', 'h.16.ln_1.weight', 'h.7.mlp.fc_in.weight', 'h.17.mlp.fc_in.bias', 'h.20.mlp.fc_out.bias', 'h.7.mlp.fc_out.weight', 'h.21.mlp.fc_out.weight', 'h.10.mlp.fc_in.weight', 'h.4.attn.q_proj.weight', 'h.6.attn.k_proj.weight', 'h.15.ln_1.bias', 'h.19.mlp.fc_in.weight', 'h.19.ln_1.weight', 'h.9.ln_1.weight', 'h.21.attn.out_proj.weight', 'h.20.attn.out_proj.weight', 'h.3.attn.q_proj.weight', 'h.22.attn.out_proj.weight', 'h.14.mlp.fc_in.bias', 'h.6.attn.q_proj.weight', 'h.2.attn.out_proj.weight', 'h.5.attn.q_proj.weight', 'h.18.mlp.fc_in.weight', 'h.7.ln_1.bias', 'h.12.attn.k_proj.weight', 'h.13.attn.v_proj.weight', 'h.15.attn.k_proj.weight', 'h.17.attn.v_proj.weight', 'h.2.attn.k_proj.weight', 'h.21.mlp.fc_in.bias', 'h.10.attn.v_proj.weight', 'h.5.ln_1.bias', 'h.6.mlp.fc_in.weight', 'h.0.attn.v_proj.weight', 'h.1.mlp.fc_out.bias', 'h.5.attn.out_proj.weight', 'h.22.attn.v_proj.weight', 'h.23.ln_1.bias', 'h.4.mlp.fc_in.weight', 'h.0.mlp.fc_out.weight', 'h.2.ln_1.weight', 'h.22.ln_1.weight', 'h.17.attn.q_proj.weight', 'h.15.attn.v_proj.weight', 'h.4.attn.v_proj.weight', 'h.7.attn.out_proj.weight', 'h.18.mlp.fc_in.bias', 'h.6.attn.v_proj.weight', 'h.23.attn.v_proj.weight', 'h.0.attn.q_proj.weight', 'h.0.mlp.fc_in.bias', 'h.2.attn.q_proj.weight', 'h.15.mlp.fc_out.bias', 'h.8.attn.out_proj.weight', 'h.13.ln_1.weight', 'h.5.attn.v_proj.weight', 'h.2.mlp.fc_in.bias', 'h.19.mlp.fc_out.bias', 'h.14.attn.v_proj.weight', 'h.18.attn.out_proj.weight', 'h.23.attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(30522, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# -- IF you use from bert it comes without HEAD \n",
    "# model = BertModel.from_pretrained(\"bert-large-uncased\", torchscript=True)\n",
    "# enc = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Hence use From Huggingface directly so you can also shrink size\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(\"bert-large-uncased\", torchscript=True) #, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model = model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# traced_model = torch.jit.trace(model, dummy_inputs)\n",
    "# torch.jit.save(traced_model, \"./triton-serve/bert-uc/1/model.pt\")\n",
    "\n",
    "# print(\"Saved {}\".format(traced_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dcaa550",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(30522, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbd31e",
   "metadata": {},
   "source": [
    "#### Test encoders various methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "35264852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 13012, 2669, 28937, 8241, 3640, 1037, 6112, 1998, 3341, 1999, 7512, 2368, 6129, 5576, 23569, 27605, 5422, 2005, 2119, 17368, 2015, 1998, 14246, 2271, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\", \n",
    "    padding=\"max_length\", \n",
    "    max_length=64\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "71819ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode_plus(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\",\n",
    "    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "    max_length = 512,           \n",
    "    pad_to_max_length = True, # Pad & truncate all sentences\n",
    ")\n",
    "#encoded_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ceede",
   "metadata": {},
   "source": [
    "**Predict test using the traced model Needs Tokens and Attention mask both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cdfd9e95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101, 13012,  2669, 28937,  8241,  3640,  1037,  6112,  1998,  3341,\n",
       "         1999,  7512,  2368,  6129,  5576, 23569, 27605,  5422,  2005,  2119,\n",
       "        17368,  2015,  1998, 14246,  2271,  1012,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "84906a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'triton inference server provides a cloud and edge inferencing solution optimized for both cpus and gpus.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    encoded_input['input_ids'][0],\n",
    "    skip_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#     max_length = 512,           \n",
    "#     pad_to_max_length = True, # Pad & truncate all sentences\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "776b54d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] triton inference server provides a cloud and edge inferencing solution optimized for both cpus and gpus. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] bureau transitioned show daggers rusticgirl sunrise overseas drank synonym engaged scenery explorers 1808 ρ [unused17] [unused369] circular vowel bolsheviks liturgicalco lease turbine portrayal butt anxiously com'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\", \n",
    "    return_tensors='pt',\n",
    "    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "    max_length = 100, # -- this model has max length set to 100 -- not to 512,           \n",
    "    pad_to_max_length = True, # Pad & truncate all sentences\n",
    "\n",
    ")\n",
    "unscripted_output = model.generate( # -- both work the same way \n",
    "    #**encoded_input, \n",
    "    inputs= encoded_input['input_ids'],\n",
    "    return_dict=True, \n",
    "    output_attentions=False, \n",
    "    output_hidden_states=False,\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    temperature=0.9,\n",
    "\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "tokenizer.decode(unscripted_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9baf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.batch_decode(unscripted_output)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8754f108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 13012,  2669, 28937,  8241,  3640,  1037,  6112,  1998,  3341,\n",
       "          1999,  7512,  2368,  6129,  5576, 23569, 27605,  5422,  2005,  2119,\n",
       "         17368,  2015,  1998, 14246,  2271,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          4879, 23946,  2265, 24210, 27471, 15239, 13932,  6931, 10749, 10675,\n",
       "          5117, 17363, 19264, 13040,  1171,    18,   374,  8206, 12710, 28755,\n",
       "         19246,  3597, 10084, 14027, 13954, 10007, 23403,  4012]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscripted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "08b955a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./triton-serve/bert-uc/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c2c0f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_save_directory = \"./triton-serve/bert-uc/1/tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c06fb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "seq_len = 512\n",
    "\n",
    "traced_model = torch.jit.trace(model, encoded_input['input_ids'])\n",
    "torch.jit.save(traced_model, \"./triton-serve/bert-uc/1/model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0d3ee",
   "metadata": {},
   "source": [
    "### UPLOAD of the Model.tar after it has been created correctly by \n",
    "\n",
    "Because we share the same model tar with bloom and with bert-uc\n",
    "rm model.tar.gz in the triton-serve directory\n",
    "\n",
    "\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" --exclude \"*.tar\" -zcvf model.tar.gz bert-uc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967461e",
   "metadata": {},
   "source": [
    "**Upload the model.tar.gz to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ca977b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2b081aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc/model.tar.gz\n",
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/\n"
     ]
    }
   ],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5944baa",
   "metadata": {},
   "source": [
    "#### Start Single Model Triton for starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ffd76",
   "metadata": {},
   "source": [
    "**Triton Image download and sagemaker variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "793b94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f727e",
   "metadata": {},
   "source": [
    "**Model creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a6a78150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5-bert-uc--2022-09-14-16-13-44-370\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:622343165275:model/p5-bert-uc--2022-09-14-16-13-44-370', 'ResponseMetadata': {'RequestId': '09f5e088-4fbb-4697-a2dd-e065e0db4b53', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '09f5e088-4fbb-4697-a2dd-e065e0db4b53', 'content-type': 'application/x-amz-json-1.1', 'content-length': '97', 'date': 'Wed, 14 Sep 2022 16:13:43 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-bert-uc-\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bert-uc',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913539a",
   "metadata": {},
   "source": [
    "**Endpoint config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d1aa9c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:622343165275:endpoint-config/p5-bert-uc--2022-09-14-16-13-44-370\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc7e19",
   "metadata": {},
   "source": [
    "**Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ff1539ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:622343165275:endpoint/p5-bert-uc--2022-09-14-16-13-44-370\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a077bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGLE:Model:endpoint:Triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n",
      "Single:model:triton:Status: Creating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56676/3870043288.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Creating\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EndpointStatus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa7e64",
   "metadata": {},
   "source": [
    "**Now Invoke The endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a9bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_text(text, enc, max_length=512):\n",
    "    #enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(f\"Tokenize:text:why??::max_length={max_length}::Tokenizer={enc}\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=max_length)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "def _get_sample_tokenized_text_binary(text, input_names, output_names, enc, max_length=512):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_names[0], [1, max_length], \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], [1, max_length], \"INT32\"))\n",
    "    indexed_tokens, attention_mask = tokenize_text(text,enc)\n",
    "\n",
    "    indexed_tokens = np.array(indexed_tokens, dtype=np.int32)\n",
    "    indexed_tokens = np.expand_dims(indexed_tokens, axis=0)\n",
    "    inputs[0].set_data_from_numpy(indexed_tokens, binary_data=True)\n",
    "\n",
    "    attention_mask = np.array(attention_mask, dtype=np.int32)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    inputs[1].set_data_from_numpy(attention_mask, binary_data=True)\n",
    "\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[1], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_pt(text, enc, max_length=512):\n",
    "    return _get_sample_tokenized_text_binary(\n",
    "        text, [\"INPUT__0\", \"INPUT__1\"], [\"OUTPUT__0\", \"1634__1\"], enc, max_length\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_trt(text, enc):\n",
    "    return _get_sample_tokenized_text_binary(text, [\"token_ids\", \"attn_mask\"], [\"output\", \"1634\"], enc, max_length)\n",
    "\n",
    "def get_decoded_text(tensors_tokens, enc):\n",
    "    return_text=tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "max_seq_length=100 #512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(text_triton, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24494f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    torch.tensor(output['outputs'][0]['data']), #tokenizer.decode(unscripted_output[0])\n",
    "    skip_special_tokens=True,\n",
    "    clean_up=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acf99e",
   "metadata": {},
   "source": [
    "#### Use the Binary Headers for Triton - faster but same results - BUT ERRORS out as NO RESPONSE is recieved \n",
    "\n",
    "We can also use binary+json as the payload format to get better performance for the inference call. The specification of this format is provided here.\n",
    "\n",
    "Note: With the binary+json format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header application/vnd.sagemaker-triton.binary+json;json-header-size={}.\n",
    "\n",
    "Please not, this is different from using Inference-Header-Content-Length header on a stand-alone Triton server since custom headers are not allowed in SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93779871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage the Tokenizer=PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})::max_seq_length=512:: create above when creating the model \n",
      "Tokenize:text:why??::max_length=512::Tokenizer=PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "Error in parsing respinse -- probably the body is empty\n",
      "CPU times: user 3.46 ms, sys: 9.35 ms, total: 12.8 ms\n",
      "Wall time: 32.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "max_seq_length=512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "\n",
    "#input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "request_body, header_length = get_sample_tokenized_text_binary_pt(text_triton, enc) # this returns \n",
    "\n",
    "\n",
    "response_binary = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, \n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(header_length), \n",
    "    Body=request_body\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "try:\n",
    "    # Read response body\n",
    "    result = httpclient.InferenceServerClient.parse_response_body(\n",
    "        response_binary[\"Body\"].read(), header_length=int(header_length_str)\n",
    "    )\n",
    "    output0_data = result.as_numpy(\"OUTPUT__0\")\n",
    "    output1_data = result.as_numpy(\"1634__1\")\n",
    "    print(output0_data)\n",
    "    print(output1_data)\n",
    "except:\n",
    "    print(\"Error in parsing respinse -- probably the body is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7edce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p5-bert-uc--2022-09-08-03-02-53-774'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ab05e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage the Tokenizer=PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})::max_seq_length=512:: create above when creating the model \n",
      "Tokenize:text:why??::max_length=512::Tokenizer=PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "CPU times: user 14 s, sys: 1.07 s, total: 15.1 s\n",
      "Wall time: 28.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_name', 'model_version', 'outputs'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "max_seq_length=512\n",
    "text_triton = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids}, # -- enc.tokenize(text)}, #\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "max_run = 100\n",
    "for ii in range(0, max_run):\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name_p5, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "    # -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "    output_dict.keys()\n",
    "\n",
    "    #enc.decode(output_dict['outputs'][0]['data'], skip_special_tokens=True)\n",
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "934dbe0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'OUTPUT__0',\n",
       " 'datatype': 'FP32',\n",
       " 'shape': [1, 512, 768],\n",
       " 'data': [-0.15243282914161682,\n",
       "  -0.8572331666946411,\n",
       "  0.06608188152313232,\n",
       "  -0.20899571478366852,\n",
       "  0.35779935121536255,\n",
       "  -0.4324319064617157,\n",
       "  0.21307486295700073,\n",
       "  0.7328435778617859,\n",
       "  0.2850395441055298,\n",
       "  -0.8913273811340332,\n",
       "  0.2898162603378296,\n",
       "  -0.2516830265522003,\n",
       "  0.17877909541130066,\n",
       "  0.22467152774333954,\n",
       "  -0.16646161675453186,\n",
       "  0.21520552039146423,\n",
       "  0.4105544686317444,\n",
       "  0.49988511204719543,\n",
       "  0.15959863364696503,\n",
       "  0.11675862967967987,\n",
       "  0.012811945751309395,\n",
       "  -0.6604794859886169,\n",
       "  0.46912506222724915,\n",
       "  0.11688332259654999,\n",
       "  0.15712383389472961,\n",
       "  -0.03815995156764984,\n",
       "  -0.13969361782073975,\n",
       "  0.015787124633789062,\n",
       "  0.11993402242660522,\n",
       "  0.3402771055698395,\n",
       "  -0.6780798435211182,\n",
       "  0.1458730399608612,\n",
       "  -0.25922656059265137,\n",
       "  -0.7832548022270203,\n",
       "  0.28116920590400696,\n",
       "  -0.12131515890359879,\n",
       "  -0.28475871682167053,\n",
       "  -0.2089376002550125,\n",
       "  -0.11570572853088379,\n",
       "  0.35242488980293274,\n",
       "  -0.9599204659461975,\n",
       "  0.19939231872558594,\n",
       "  0.09761316329240799,\n",
       "  -0.25746431946754456,\n",
       "  0.2714739441871643,\n",
       "  -0.4958275854587555,\n",
       "  -4.28070592880249,\n",
       "  0.0795796662569046,\n",
       "  0.06400274485349655,\n",
       "  -0.8140904307365417,\n",
       "  0.21850654482841492,\n",
       "  0.014872162602841854,\n",
       "  -0.38559287786483765,\n",
       "  0.49034759402275085,\n",
       "  0.21157188713550568,\n",
       "  0.28455108404159546,\n",
       "  -0.22176732122898102,\n",
       "  0.22864080965518951,\n",
       "  -0.060361556708812714,\n",
       "  -0.20122043788433075,\n",
       "  -0.005756697151809931,\n",
       "  -0.06988131254911423,\n",
       "  -0.15562108159065247,\n",
       "  -0.004894986283034086,\n",
       "  -0.7175312638282776,\n",
       "  0.5078800320625305,\n",
       "  -0.017568115144968033,\n",
       "  0.20313388109207153,\n",
       "  -0.27161484956741333,\n",
       "  0.59983891248703,\n",
       "  -0.24598833918571472,\n",
       "  0.18726643919944763,\n",
       "  0.17724545300006866,\n",
       "  -0.356765478849411,\n",
       "  0.3893219530582428,\n",
       "  0.04006847366690636,\n",
       "  -0.38726016879081726,\n",
       "  0.016088290140032768,\n",
       "  -0.6229373216629028,\n",
       "  0.013894503936171532,\n",
       "  -0.44776278734207153,\n",
       "  0.755653977394104,\n",
       "  0.46263158321380615,\n",
       "  0.047012973576784134,\n",
       "  0.4246334731578827,\n",
       "  0.15268069505691528,\n",
       "  -0.789059579372406,\n",
       "  0.0030534849502146244,\n",
       "  0.30108505487442017,\n",
       "  0.3011099696159363,\n",
       "  -0.6517355442047119,\n",
       "  -0.3106408715248108,\n",
       "  -0.8016434907913208,\n",
       "  0.3920685350894928,\n",
       "  0.21406258642673492,\n",
       "  0.052744146436452866,\n",
       "  -0.09847257286310196,\n",
       "  0.012269517406821251,\n",
       "  0.3281860649585724,\n",
       "  0.9568153023719788,\n",
       "  0.054322369396686554,\n",
       "  -0.5601522326469421,\n",
       "  0.14691179990768433,\n",
       "  -0.6472994089126587,\n",
       "  -0.06023532897233963,\n",
       "  0.30683863162994385,\n",
       "  0.23232325911521912,\n",
       "  -0.04246249049901962,\n",
       "  -0.2844262719154358,\n",
       "  -1.803192138671875,\n",
       "  -0.045831985771656036,\n",
       "  -0.003260977566242218,\n",
       "  0.21598969399929047,\n",
       "  -0.5739086270332336,\n",
       "  0.16243724524974823,\n",
       "  -0.22761230170726776,\n",
       "  0.6652820110321045,\n",
       "  -0.298556923866272,\n",
       "  0.06696085631847382,\n",
       "  -0.23198053240776062,\n",
       "  -0.18249568343162537,\n",
       "  -0.4174838364124298,\n",
       "  -0.09105788171291351,\n",
       "  -0.5179993510246277,\n",
       "  0.4876752197742462,\n",
       "  0.4783645570278168,\n",
       "  0.1439400464296341,\n",
       "  0.3334556818008423,\n",
       "  0.25536584854125977,\n",
       "  -0.21909332275390625,\n",
       "  0.6439536213874817,\n",
       "  0.20453940331935883,\n",
       "  -0.37209081649780273,\n",
       "  -0.17049318552017212,\n",
       "  -0.3931882977485657,\n",
       "  1.097961664199829,\n",
       "  -0.16415078938007355,\n",
       "  -0.18966242671012878,\n",
       "  -0.38879573345184326,\n",
       "  0.17658215761184692,\n",
       "  0.06191803142428398,\n",
       "  -0.11129635572433472,\n",
       "  -2.5897436141967773,\n",
       "  0.8232522010803223,\n",
       "  0.9554612040519714,\n",
       "  0.2858097553253174,\n",
       "  0.004961827769875526,\n",
       "  -0.2980957329273224,\n",
       "  -0.6197496652603149,\n",
       "  0.4078480899333954,\n",
       "  0.43433651328086853,\n",
       "  0.44900840520858765,\n",
       "  -0.298801451921463,\n",
       "  0.2750507593154907,\n",
       "  -0.6649825572967529,\n",
       "  0.4650048613548279,\n",
       "  -0.37900495529174805,\n",
       "  0.23983296751976013,\n",
       "  0.1963275820016861,\n",
       "  0.18936653435230255,\n",
       "  0.08404063433408737,\n",
       "  0.27934569120407104,\n",
       "  -0.1847696453332901,\n",
       "  0.2775258421897888,\n",
       "  -0.03290119022130966,\n",
       "  0.08018647134304047,\n",
       "  0.4324783980846405,\n",
       "  0.34535008668899536,\n",
       "  -0.4332107901573181,\n",
       "  -0.216921865940094,\n",
       "  -0.15295131504535675,\n",
       "  0.2409810721874237,\n",
       "  1.0469696521759033,\n",
       "  0.36875292658805847,\n",
       "  0.08053147792816162,\n",
       "  -0.1223502978682518,\n",
       "  -0.2162165343761444,\n",
       "  -0.007359195034950972,\n",
       "  0.5893856287002563,\n",
       "  -0.18758779764175415,\n",
       "  -0.8738098740577698,\n",
       "  0.30574169754981995,\n",
       "  -0.13024497032165527,\n",
       "  -0.09229726344347,\n",
       "  0.07358723878860474,\n",
       "  -0.3273221254348755,\n",
       "  0.3707204759120941,\n",
       "  -0.09124206751585007,\n",
       "  -0.07653260976076126,\n",
       "  -0.38909420371055603,\n",
       "  -0.3068927526473999,\n",
       "  -0.5193188190460205,\n",
       "  0.29507753252983093,\n",
       "  0.10124936699867249,\n",
       "  0.4992099404335022,\n",
       "  -0.5116150975227356,\n",
       "  0.0808495581150055,\n",
       "  -0.3332512378692627,\n",
       "  -0.3916238248348236,\n",
       "  -0.29075372219085693,\n",
       "  0.08020149916410446,\n",
       "  -0.33299875259399414,\n",
       "  0.5210301280021667,\n",
       "  0.22531406581401825,\n",
       "  -0.21531414985656738,\n",
       "  3.9804553985595703,\n",
       "  -0.5335700511932373,\n",
       "  -0.6236492395401001,\n",
       "  0.3646713197231293,\n",
       "  0.3299594521522522,\n",
       "  -0.7218930721282959,\n",
       "  0.11543698608875275,\n",
       "  -0.007148232776671648,\n",
       "  0.5000238418579102,\n",
       "  0.11779721081256866,\n",
       "  0.23821154236793518,\n",
       "  0.3014300465583801,\n",
       "  0.10538209229707718,\n",
       "  0.37916961312294006,\n",
       "  0.18390344083309174,\n",
       "  -0.5240735411643982,\n",
       "  0.3078731894493103,\n",
       "  -0.8637978434562683,\n",
       "  -0.07996610552072525,\n",
       "  -0.9679317474365234,\n",
       "  0.19479244947433472,\n",
       "  -0.27941086888313293,\n",
       "  0.3579581081867218,\n",
       "  0.30736058950424194,\n",
       "  -1.7784301042556763,\n",
       "  -0.35053661465644836,\n",
       "  -0.06332805752754211,\n",
       "  0.20748697221279144,\n",
       "  0.14578208327293396,\n",
       "  -0.3111460506916046,\n",
       "  0.07201452553272247,\n",
       "  -0.6581940054893494,\n",
       "  -0.6154274940490723,\n",
       "  -0.09685604274272919,\n",
       "  -0.17917901277542114,\n",
       "  0.18489520251750946,\n",
       "  0.566371738910675,\n",
       "  0.33499693870544434,\n",
       "  0.6630157232284546,\n",
       "  -0.14674519002437592,\n",
       "  0.11833595484495163,\n",
       "  0.3510623872280121,\n",
       "  0.13926023244857788,\n",
       "  0.5620833039283752,\n",
       "  -0.4351637363433838,\n",
       "  0.567243218421936,\n",
       "  0.13841907680034637,\n",
       "  0.7434300184249878,\n",
       "  0.11254368722438812,\n",
       "  0.32779985666275024,\n",
       "  0.41057053208351135,\n",
       "  -0.3029719591140747,\n",
       "  -0.2382582277059555,\n",
       "  -0.11079960316419601,\n",
       "  0.015436003915965557,\n",
       "  -0.13279466331005096,\n",
       "  0.12257592380046844,\n",
       "  -0.3194899559020996,\n",
       "  0.43891462683677673,\n",
       "  -0.6943170428276062,\n",
       "  -0.05838284641504288,\n",
       "  0.3810212016105652,\n",
       "  -0.09472791850566864,\n",
       "  0.3196665942668915,\n",
       "  0.11023896187543869,\n",
       "  -0.20110729336738586,\n",
       "  -0.19630540907382965,\n",
       "  -0.733989417552948,\n",
       "  -2.283050060272217,\n",
       "  -0.1596091091632843,\n",
       "  -0.3896175026893616,\n",
       "  0.25501734018325806,\n",
       "  0.5485239624977112,\n",
       "  0.7279982566833496,\n",
       "  -0.2977728843688965,\n",
       "  0.27647411823272705,\n",
       "  0.7247763276100159,\n",
       "  -0.625706136226654,\n",
       "  0.6716095805168152,\n",
       "  0.7512229681015015,\n",
       "  -0.6151925921440125,\n",
       "  0.5348860025405884,\n",
       "  0.26652613282203674,\n",
       "  -0.2668118476867676,\n",
       "  -0.059999823570251465,\n",
       "  -0.916610836982727,\n",
       "  0.05203899368643761,\n",
       "  -0.5905131101608276,\n",
       "  -0.5954939126968384,\n",
       "  -0.02916477434337139,\n",
       "  -0.05308451130986214,\n",
       "  0.7269306182861328,\n",
       "  0.64029461145401,\n",
       "  0.4839937090873718,\n",
       "  -0.0747174471616745,\n",
       "  -0.12876608967781067,\n",
       "  -0.3492620587348938,\n",
       "  0.42400145530700684,\n",
       "  0.31635794043540955,\n",
       "  0.001425330643542111,\n",
       "  0.37645307183265686,\n",
       "  -0.08180449157953262,\n",
       "  -0.682857096195221,\n",
       "  -2.667266368865967,\n",
       "  0.16403107345104218,\n",
       "  0.10986682027578354,\n",
       "  -0.3569868206977844,\n",
       "  -0.027537843212485313,\n",
       "  0.2773939073085785,\n",
       "  0.38499656319618225,\n",
       "  -0.008390780538320541,\n",
       "  -0.4786089062690735,\n",
       "  -0.09760541468858719,\n",
       "  0.24313293397426605,\n",
       "  -0.45595282316207886,\n",
       "  0.3337545096874237,\n",
       "  -0.010193522088229656,\n",
       "  0.6302138566970825,\n",
       "  0.01677335426211357,\n",
       "  0.34828659892082214,\n",
       "  -0.5686216950416565,\n",
       "  0.010367066599428654,\n",
       "  -0.13441316783428192,\n",
       "  -0.3208213150501251,\n",
       "  0.19600138068199158,\n",
       "  0.3707866668701172,\n",
       "  -0.22076064348220825,\n",
       "  0.5413828492164612,\n",
       "  0.6409174799919128,\n",
       "  -0.800566554069519,\n",
       "  -0.21035903692245483,\n",
       "  0.12587518990039825,\n",
       "  -0.13108915090560913,\n",
       "  0.405699223279953,\n",
       "  -0.4599815905094147,\n",
       "  -0.4158557951450348,\n",
       "  0.1859418898820877,\n",
       "  -0.48929986357688904,\n",
       "  -0.014476576820015907,\n",
       "  0.5688996911048889,\n",
       "  -0.1462414562702179,\n",
       "  0.6768046617507935,\n",
       "  0.3780023455619812,\n",
       "  -0.16517241299152374,\n",
       "  0.5860856771469116,\n",
       "  0.04229560121893883,\n",
       "  -0.14841297268867493,\n",
       "  0.5961641669273376,\n",
       "  0.050503432750701904,\n",
       "  0.7350917458534241,\n",
       "  -0.3551936745643616,\n",
       "  0.2134319543838501,\n",
       "  0.7498502135276794,\n",
       "  -0.06756069511175156,\n",
       "  0.2406913787126541,\n",
       "  1.6315383911132812,\n",
       "  0.03517923504114151,\n",
       "  0.42837265133857727,\n",
       "  0.13128745555877686,\n",
       "  0.004562667105346918,\n",
       "  -0.0020802682265639305,\n",
       "  -0.5440433621406555,\n",
       "  0.45915353298187256,\n",
       "  0.6549606323242188,\n",
       "  -0.8992667198181152,\n",
       "  -0.06954380124807358,\n",
       "  -0.3308899998664856,\n",
       "  0.7643892168998718,\n",
       "  -0.8476575613021851,\n",
       "  0.15809689462184906,\n",
       "  -0.411735862493515,\n",
       "  0.010811352171003819,\n",
       "  -0.1143619641661644,\n",
       "  0.33857402205467224,\n",
       "  0.22135883569717407,\n",
       "  -0.4099784195423126,\n",
       "  -1.6249817609786987,\n",
       "  0.3434310555458069,\n",
       "  -0.02272789552807808,\n",
       "  -0.3741368353366852,\n",
       "  0.16478930413722992,\n",
       "  -0.24276229739189148,\n",
       "  -0.3600810468196869,\n",
       "  0.1560140699148178,\n",
       "  -0.3518632650375366,\n",
       "  -0.15955300629138947,\n",
       "  0.3054552972316742,\n",
       "  -0.6356754302978516,\n",
       "  -0.4160648584365845,\n",
       "  0.10241243988275528,\n",
       "  0.1789943128824234,\n",
       "  -0.6628385186195374,\n",
       "  -0.43797510862350464,\n",
       "  0.09305015951395035,\n",
       "  0.21994896233081818,\n",
       "  0.2351069301366806,\n",
       "  0.2470349371433258,\n",
       "  -0.7779158353805542,\n",
       "  -0.07482749968767166,\n",
       "  0.042539793998003006,\n",
       "  -1.3215032815933228,\n",
       "  0.29316550493240356,\n",
       "  -0.11797415465116501,\n",
       "  0.5045604109764099,\n",
       "  0.36977142095565796,\n",
       "  0.12047497183084488,\n",
       "  -0.2272309958934784,\n",
       "  -0.4634229838848114,\n",
       "  0.03910891339182854,\n",
       "  -0.42650309205055237,\n",
       "  0.12236924469470978,\n",
       "  0.0734814777970314,\n",
       "  0.23430900275707245,\n",
       "  -0.4466637670993805,\n",
       "  -0.26380473375320435,\n",
       "  -0.4400947093963623,\n",
       "  -0.00739049119874835,\n",
       "  1.060275912284851,\n",
       "  -0.47253018617630005,\n",
       "  0.04528389126062393,\n",
       "  0.8045607209205627,\n",
       "  0.07821834087371826,\n",
       "  -0.37794145941734314,\n",
       "  0.49531713128089905,\n",
       "  0.009510708041489124,\n",
       "  0.17018736898899078,\n",
       "  -0.4015081226825714,\n",
       "  0.3611564338207245,\n",
       "  -0.15230809152126312,\n",
       "  0.14244310557842255,\n",
       "  -0.23516149818897247,\n",
       "  0.19474199414253235,\n",
       "  0.013688655570149422,\n",
       "  0.5158392786979675,\n",
       "  -0.20714668929576874,\n",
       "  -0.4269808232784271,\n",
       "  -0.1989830732345581,\n",
       "  0.019452707841992378,\n",
       "  0.003826805157586932,\n",
       "  -0.89957195520401,\n",
       "  -0.4286869168281555,\n",
       "  -0.0382392518222332,\n",
       "  0.38160526752471924,\n",
       "  0.5184934735298157,\n",
       "  0.03202255070209503,\n",
       "  -0.6420595049858093,\n",
       "  -0.1360992044210434,\n",
       "  0.050308577716350555,\n",
       "  0.3413587510585785,\n",
       "  -0.40859586000442505,\n",
       "  0.12311729788780212,\n",
       "  0.19384804368019104,\n",
       "  0.28185132145881653,\n",
       "  0.13951259851455688,\n",
       "  0.5814095139503479,\n",
       "  -0.3154950737953186,\n",
       "  -0.1854190230369568,\n",
       "  0.06399369239807129,\n",
       "  0.25282222032546997,\n",
       "  -0.05173686891794205,\n",
       "  -0.21610401570796967,\n",
       "  -0.03656230866909027,\n",
       "  -0.22397226095199585,\n",
       "  -0.007589278277009726,\n",
       "  0.1626456081867218,\n",
       "  -1.9365695714950562,\n",
       "  -0.10576067119836807,\n",
       "  0.590091347694397,\n",
       "  -0.07427757233381271,\n",
       "  0.17707780003547668,\n",
       "  -0.2830815315246582,\n",
       "  0.16561488807201385,\n",
       "  0.3820725083351135,\n",
       "  -0.10643642395734787,\n",
       "  -0.09023184329271317,\n",
       "  -0.15113085508346558,\n",
       "  0.6557567119598389,\n",
       "  0.11127658188343048,\n",
       "  0.1654430776834488,\n",
       "  -0.09123630821704865,\n",
       "  -0.6255208253860474,\n",
       "  -0.4871487617492676,\n",
       "  -0.34839850664138794,\n",
       "  0.10916300117969513,\n",
       "  -0.9921050071716309,\n",
       "  -0.2253103256225586,\n",
       "  0.6314945220947266,\n",
       "  0.7647891044616699,\n",
       "  0.1289629340171814,\n",
       "  0.03854874148964882,\n",
       "  -0.32970431447029114,\n",
       "  -0.002516691107302904,\n",
       "  0.3706390857696533,\n",
       "  0.05061403289437294,\n",
       "  0.24953855574131012,\n",
       "  0.35177135467529297,\n",
       "  0.12793877720832825,\n",
       "  -0.8429774641990662,\n",
       "  0.06458117067813873,\n",
       "  0.39192691445350647,\n",
       "  0.40299156308174133,\n",
       "  0.41197773814201355,\n",
       "  -0.23538699746131897,\n",
       "  0.43065574765205383,\n",
       "  0.43894556164741516,\n",
       "  -0.6606647968292236,\n",
       "  0.35109591484069824,\n",
       "  0.5190305113792419,\n",
       "  0.6520794034004211,\n",
       "  0.8571969866752625,\n",
       "  -0.05022159591317177,\n",
       "  -0.10029426962137222,\n",
       "  0.2952761650085449,\n",
       "  -0.32723426818847656,\n",
       "  -0.017702119424939156,\n",
       "  -0.7253603339195251,\n",
       "  -0.7763994336128235,\n",
       "  -0.0747443363070488,\n",
       "  -0.35449352860450745,\n",
       "  -0.198677659034729,\n",
       "  -0.49206504225730896,\n",
       "  0.38733184337615967,\n",
       "  0.1457296758890152,\n",
       "  -0.9076860547065735,\n",
       "  -0.33420440554618835,\n",
       "  0.5267676115036011,\n",
       "  -0.07851012051105499,\n",
       "  -0.43514230847358704,\n",
       "  -0.43984705209732056,\n",
       "  -0.08559336513280869,\n",
       "  -1.0786504745483398,\n",
       "  -0.28133469820022583,\n",
       "  -0.07433544844388962,\n",
       "  0.015066095627844334,\n",
       "  0.20733742415905,\n",
       "  0.11163754761219025,\n",
       "  -0.02526504173874855,\n",
       "  -0.7287487983703613,\n",
       "  0.19242285192012787,\n",
       "  -0.4180862605571747,\n",
       "  -0.29124394059181213,\n",
       "  0.132492333650589,\n",
       "  -0.1641431599855423,\n",
       "  0.5331347584724426,\n",
       "  -0.6004188060760498,\n",
       "  0.3266528248786926,\n",
       "  0.19985897839069366,\n",
       "  0.2508074641227722,\n",
       "  0.5183570981025696,\n",
       "  -0.6566883325576782,\n",
       "  0.011749832890927792,\n",
       "  -0.4450083076953888,\n",
       "  0.22919724881649017,\n",
       "  0.30244335532188416,\n",
       "  0.12190672755241394,\n",
       "  -0.48163577914237976,\n",
       "  -0.6678341031074524,\n",
       "  -0.05269142985343933,\n",
       "  0.14905701577663422,\n",
       "  -0.26032546162605286,\n",
       "  -0.28518247604370117,\n",
       "  0.04718537628650665,\n",
       "  0.5703840851783752,\n",
       "  -0.32296475768089294,\n",
       "  -0.767652690410614,\n",
       "  -0.028027929365634918,\n",
       "  0.4003502130508423,\n",
       "  0.7045083045959473,\n",
       "  -0.0024386586155742407,\n",
       "  0.33113357424736023,\n",
       "  0.47623196244239807,\n",
       "  0.6452591419219971,\n",
       "  -0.6414557099342346,\n",
       "  -0.4135698676109314,\n",
       "  -0.2912571430206299,\n",
       "  0.2803230583667755,\n",
       "  -0.25887420773506165,\n",
       "  -0.2568710446357727,\n",
       "  0.25726157426834106,\n",
       "  -0.11538080871105194,\n",
       "  -0.8551483750343323,\n",
       "  -0.24193046987056732,\n",
       "  -0.31402918696403503,\n",
       "  1.4807443618774414,\n",
       "  0.3956575393676758,\n",
       "  -0.33006277680397034,\n",
       "  -0.28906020522117615,\n",
       "  0.39670252799987793,\n",
       "  0.5810874700546265,\n",
       "  -0.08746033906936646,\n",
       "  0.7076026797294617,\n",
       "  -0.49196168780326843,\n",
       "  0.9828351140022278,\n",
       "  -0.11483540385961533,\n",
       "  0.5720252394676208,\n",
       "  -0.5338782668113708,\n",
       "  -0.3397670388221741,\n",
       "  1.0628236532211304,\n",
       "  0.22742335498332977,\n",
       "  0.245201975107193,\n",
       "  -0.055027008056640625,\n",
       "  -0.4301733672618866,\n",
       "  -0.11962784081697464,\n",
       "  -0.5559842586517334,\n",
       "  0.9395849108695984,\n",
       "  0.8768144845962524,\n",
       "  0.3721594214439392,\n",
       "  -0.2621835172176361,\n",
       "  0.6894754767417908,\n",
       "  0.031490884721279144,\n",
       "  -0.45206230878829956,\n",
       "  0.017423762008547783,\n",
       "  1.2395076751708984,\n",
       "  -0.6819962859153748,\n",
       "  -0.21774542331695557,\n",
       "  0.2407231628894806,\n",
       "  0.48617616295814514,\n",
       "  -0.7289584279060364,\n",
       "  -0.18832236528396606,\n",
       "  -0.868733823299408,\n",
       "  -0.07730326801538467,\n",
       "  -0.17490480840206146,\n",
       "  -0.49257999658584595,\n",
       "  0.1646927446126938,\n",
       "  -0.012394888326525688,\n",
       "  0.97123783826828,\n",
       "  -0.06959221512079239,\n",
       "  -1.0203454494476318,\n",
       "  0.5534039735794067,\n",
       "  -0.6465563774108887,\n",
       "  -0.32650575041770935,\n",
       "  0.1482006311416626,\n",
       "  0.10388380289077759,\n",
       "  0.2709469199180603,\n",
       "  -0.18422652781009674,\n",
       "  -0.26728710532188416,\n",
       "  0.1764126867055893,\n",
       "  0.383524626493454,\n",
       "  0.12405954301357269,\n",
       "  0.6090409755706787,\n",
       "  0.06067296117544174,\n",
       "  -0.26954978704452515,\n",
       "  0.08975710719823837,\n",
       "  -0.42179277539253235,\n",
       "  0.24224570393562317,\n",
       "  0.41087329387664795,\n",
       "  0.23719225823879242,\n",
       "  0.6771267056465149,\n",
       "  -0.16299887001514435,\n",
       "  0.19770996272563934,\n",
       "  0.04112868010997772,\n",
       "  0.28650322556495667,\n",
       "  -0.4513266384601593,\n",
       "  -0.20538204908370972,\n",
       "  0.7205323576927185,\n",
       "  -0.019271565601229668,\n",
       "  0.8237189054489136,\n",
       "  0.3385160267353058,\n",
       "  0.7132264375686646,\n",
       "  0.7630184888839722,\n",
       "  -0.13469535112380981,\n",
       "  -0.6349261403083801,\n",
       "  -1.4645342826843262,\n",
       "  0.3074992299079895,\n",
       "  0.23793555796146393,\n",
       "  0.6368815898895264,\n",
       "  -0.3870506286621094,\n",
       "  0.2774500548839569,\n",
       "  0.9369627237319946,\n",
       "  -0.15526948869228363,\n",
       "  -0.14886446297168732,\n",
       "  -0.368557333946228,\n",
       "  0.4437929093837738,\n",
       "  0.1247909739613533,\n",
       "  0.7323305606842041,\n",
       "  -1.0264643430709839,\n",
       "  0.049332935363054276,\n",
       "  -0.07287430763244629,\n",
       "  1.1246798038482666,\n",
       "  -0.25645190477371216,\n",
       "  0.79436856508255,\n",
       "  -0.26167434453964233,\n",
       "  0.21237047016620636,\n",
       "  -0.1532057672739029,\n",
       "  -0.02862439677119255,\n",
       "  -0.4626915156841278,\n",
       "  -0.8138589859008789,\n",
       "  -0.31271106004714966,\n",
       "  0.08624204248189926,\n",
       "  -0.10287543386220932,\n",
       "  -0.29928845167160034,\n",
       "  0.15514004230499268,\n",
       "  -0.2512807548046112,\n",
       "  0.45485618710517883,\n",
       "  -0.7099696397781372,\n",
       "  0.2852531969547272,\n",
       "  -0.3635960519313812,\n",
       "  0.4106740951538086,\n",
       "  -0.03722316026687622,\n",
       "  0.4611254632472992,\n",
       "  0.26617658138275146,\n",
       "  -0.12129662930965424,\n",
       "  0.018484685570001602,\n",
       "  0.7792474627494812,\n",
       "  -0.7508952021598816,\n",
       "  0.3767417371273041,\n",
       "  0.3458622992038727,\n",
       "  -0.017128678038716316,\n",
       "  1.0647449493408203,\n",
       "  -0.12255655974149704,\n",
       "  0.701700747013092,\n",
       "  -0.17210090160369873,\n",
       "  -0.21266409754753113,\n",
       "  0.3220290541648865,\n",
       "  1.042075514793396,\n",
       "  0.44450265169143677,\n",
       "  0.2265007495880127,\n",
       "  -0.18980370461940765,\n",
       "  0.18499352037906647,\n",
       "  -0.15596164762973785,\n",
       "  -0.0973033607006073,\n",
       "  -0.4193395674228668,\n",
       "  -0.5341764688491821,\n",
       "  0.3734762370586395,\n",
       "  -0.46815618872642517,\n",
       "  -0.15334327518939972,\n",
       "  1.063309907913208,\n",
       "  -0.30913424491882324,\n",
       "  0.266042560338974,\n",
       "  0.44362929463386536,\n",
       "  -0.10186593979597092,\n",
       "  -0.6078144311904907,\n",
       "  -0.5296705365180969,\n",
       "  -0.1086951345205307,\n",
       "  0.029407847672700882,\n",
       "  0.13378363847732544,\n",
       "  0.576744556427002,\n",
       "  -0.3269336521625519,\n",
       "  0.34273573756217957,\n",
       "  0.6779901385307312,\n",
       "  0.2496640533208847,\n",
       "  -0.1499091535806656,\n",
       "  -0.08570697158575058,\n",
       "  0.3204309940338135,\n",
       "  0.08414646238088608,\n",
       "  -0.646040678024292,\n",
       "  0.3570067882537842,\n",
       "  -5.690296649932861,\n",
       "  0.41276815533638,\n",
       "  -0.23042500019073486,\n",
       "  0.13111449778079987,\n",
       "  -0.4327654540538788,\n",
       "  -0.989452600479126,\n",
       "  -0.2599603831768036,\n",
       "  -0.15610752999782562,\n",
       "  -0.38662955164909363,\n",
       "  -0.4319131672382355,\n",
       "  -0.10625971853733063,\n",
       "  0.46662232279777527,\n",
       "  0.17167523503303528,\n",
       "  -0.26219871640205383,\n",
       "  -0.01638292521238327,\n",
       "  0.8044725060462952,\n",
       "  -0.7595658898353577,\n",
       "  -0.5881166458129883,\n",
       "  -0.025703895837068558,\n",
       "  -0.2029799222946167,\n",
       "  0.3818010985851288,\n",
       "  -0.1393527388572693,\n",
       "  -0.11042293906211853,\n",
       "  1.4981578588485718,\n",
       "  -0.0036011619959026575,\n",
       "  -0.08999548852443695,\n",
       "  0.0007514312164857984,\n",
       "  -0.3181125223636627,\n",
       "  0.2870157063007355,\n",
       "  -0.11600325256586075,\n",
       "  0.09765161573886871,\n",
       "  0.4179441034793854,\n",
       "  0.28066346049308777,\n",
       "  0.39467859268188477,\n",
       "  0.13252809643745422,\n",
       "  0.18732275068759918,\n",
       "  0.5330065488815308,\n",
       "  -0.005108908750116825,\n",
       "  0.14801445603370667,\n",
       "  0.8875568509101868,\n",
       "  0.8857049942016602,\n",
       "  0.0008163273450918496,\n",
       "  0.22233706712722778,\n",
       "  0.14635850489139557,\n",
       "  -0.3280401825904846,\n",
       "  -1.1395660638809204,\n",
       "  -0.019569920375943184,\n",
       "  0.05176907405257225,\n",
       "  0.20769082009792328,\n",
       "  -0.738433837890625,\n",
       "  0.0810212641954422,\n",
       "  -0.57448810338974,\n",
       "  0.01890065334737301,\n",
       "  -0.6642919778823853,\n",
       "  -0.23686735332012177,\n",
       "  0.4988414943218231,\n",
       "  -0.4114721119403839,\n",
       "  0.1259208470582962,\n",
       "  0.20167165994644165,\n",
       "  -0.038704901933670044,\n",
       "  0.9287362694740295,\n",
       "  -0.6229997873306274,\n",
       "  -0.25441837310791016,\n",
       "  -0.8440266251564026,\n",
       "  0.5294130444526672,\n",
       "  0.16173341870307922,\n",
       "  -0.7345454096794128,\n",
       "  0.36443933844566345,\n",
       "  0.5440770983695984,\n",
       "  -0.24436499178409576,\n",
       "  -0.22731997072696686,\n",
       "  0.36946284770965576,\n",
       "  -0.3556789457798004,\n",
       "  -0.10366591066122055,\n",
       "  -0.7346314787864685,\n",
       "  0.19080977141857147,\n",
       "  0.3967592418193817,\n",
       "  -0.04712070897221565,\n",
       "  -0.21215929090976715,\n",
       "  -0.08550339192152023,\n",
       "  0.19921128451824188,\n",
       "  -0.03884485736489296,\n",
       "  0.05804602429270744,\n",
       "  1.0955346822738647,\n",
       "  -1.2348569631576538,\n",
       "  -0.49811726808547974,\n",
       "  -0.2274211347103119,\n",
       "  -0.7072733640670776,\n",
       "  0.46943145990371704,\n",
       "  -0.28947630524635315,\n",
       "  0.5181368589401245,\n",
       "  0.2877310514450073,\n",
       "  -0.14419975876808167,\n",
       "  -0.09450389444828033,\n",
       "  -0.17006058990955353,\n",
       "  -0.4197314977645874,\n",
       "  -0.24286271631717682,\n",
       "  1.2148160934448242,\n",
       "  -0.03178662061691284,\n",
       "  0.042943235486745834,\n",
       "  -0.36007633805274963,\n",
       "  0.5239415168762207,\n",
       "  -0.8245298266410828,\n",
       "  0.015807142481207848,\n",
       "  0.26179471611976624,\n",
       "  0.556049108505249,\n",
       "  -0.3179343640804291,\n",
       "  0.12528179585933685,\n",
       "  -0.5196822285652161,\n",
       "  0.03170362114906311,\n",
       "  0.41035670042037964,\n",
       "  -0.9063966870307922,\n",
       "  0.07459063827991486,\n",
       "  0.039600737392902374,\n",
       "  -0.04662210866808891,\n",
       "  0.008652030490338802,\n",
       "  -0.3990488648414612,\n",
       "  -1.518363118171692,\n",
       "  -0.2523789405822754,\n",
       "  0.16996394097805023,\n",
       "  0.5184571146965027,\n",
       "  0.07396390289068222,\n",
       "  0.26741066575050354,\n",
       "  -0.016938261687755585,\n",
       "  -0.1327054798603058,\n",
       "  -0.11027899384498596,\n",
       "  -0.1153007447719574,\n",
       "  0.1325850486755371,\n",
       "  -0.19647115468978882,\n",
       "  -0.9545804262161255,\n",
       "  -0.17417313158512115,\n",
       "  0.7408426403999329,\n",
       "  0.2696540355682373,\n",
       "  0.3485738933086395,\n",
       "  0.022285165265202522,\n",
       "  0.16813504695892334,\n",
       "  -0.10243787616491318,\n",
       "  0.01871873438358307,\n",
       "  0.4852330982685089,\n",
       "  0.4862951338291168,\n",
       "  -0.7041104435920715,\n",
       "  0.33142268657684326,\n",
       "  -0.006252328399568796,\n",
       "  0.2609426975250244,\n",
       "  0.738551139831543,\n",
       "  -0.7768146395683289,\n",
       "  0.6602467894554138,\n",
       "  0.5471489429473877,\n",
       "  0.40082600712776184,\n",
       "  -0.46358439326286316,\n",
       "  -0.3200080096721649,\n",
       "  -0.13953787088394165,\n",
       "  0.8278697729110718,\n",
       "  -0.4627791941165924,\n",
       "  -0.32709482312202454,\n",
       "  0.6943374872207642,\n",
       "  0.7043707966804504,\n",
       "  -0.06588182598352432,\n",
       "  0.9223544597625732,\n",
       "  -0.25970831513404846,\n",
       "  -0.09014131873846054,\n",
       "  -0.12743474543094635,\n",
       "  -0.5048604607582092,\n",
       "  -0.471625953912735,\n",
       "  0.19957235455513,\n",
       "  0.3579193949699402,\n",
       "  -0.08009441941976547,\n",
       "  0.010041814297437668,\n",
       "  -0.48221009969711304,\n",
       "  0.19295629858970642,\n",
       "  -0.16072118282318115,\n",
       "  0.48191940784454346,\n",
       "  0.22397290170192719,\n",
       "  0.23785580694675446,\n",
       "  0.5573644638061523,\n",
       "  0.013030987232923508,\n",
       "  0.21993489563465118,\n",
       "  0.17214475572109222,\n",
       "  0.8985391855239868,\n",
       "  1.1059409379959106,\n",
       "  -0.6165094375610352,\n",
       "  -0.15247495472431183,\n",
       "  -0.3756446838378906,\n",
       "  -0.23433122038841248,\n",
       "  0.06411436945199966,\n",
       "  -0.1295158565044403,\n",
       "  0.22380417585372925,\n",
       "  -0.6371030807495117,\n",
       "  1.1724536418914795,\n",
       "  -0.04513296112418175,\n",
       "  -0.02387055568397045,\n",
       "  -0.41024717688560486,\n",
       "  0.3714029788970947,\n",
       "  0.07922203093767166,\n",
       "  0.2749096751213074,\n",
       "  -0.02760433219373226,\n",
       "  -0.6063851714134216,\n",
       "  0.3749072551727295,\n",
       "  -0.4484843611717224,\n",
       "  -0.2681480050086975,\n",
       "  0.49209827184677124,\n",
       "  -0.7064786553382874,\n",
       "  0.3814597427845001,\n",
       "  -0.8379508256912231,\n",
       "  -0.10589183866977692,\n",
       "  0.8880450129508972,\n",
       "  -0.4523322582244873,\n",
       "  0.4013178050518036,\n",
       "  0.7524261474609375,\n",
       "  -0.3678078055381775,\n",
       "  1.2483851909637451,\n",
       "  -0.4370359778404236,\n",
       "  -0.21779949963092804,\n",
       "  0.0536472424864769,\n",
       "  0.4341031014919281,\n",
       "  0.31509914994239807,\n",
       "  0.15433916449546814,\n",
       "  0.30003172159194946,\n",
       "  0.031102627515792847,\n",
       "  -0.22313790023326874,\n",
       "  -0.7915427088737488,\n",
       "  0.2622305750846863,\n",
       "  -0.41356760263442993,\n",
       "  -0.08469942957162857,\n",
       "  0.5810692310333252,\n",
       "  -0.4819425940513611,\n",
       "  -0.37441498041152954,\n",
       "  0.7897458672523499,\n",
       "  -0.11677210032939911,\n",
       "  -0.19369973242282867,\n",
       "  0.17002998292446136,\n",
       "  0.3060799837112427,\n",
       "  0.25039854645729065,\n",
       "  0.5973700881004333,\n",
       "  0.09002286940813065,\n",
       "  0.0933876633644104,\n",
       "  0.20862933993339539,\n",
       "  -0.20386505126953125,\n",
       "  -0.00020156119717285037,\n",
       "  0.733384370803833,\n",
       "  -0.3118767738342285,\n",
       "  0.5473107695579529,\n",
       "  -0.06287690252065659,\n",
       "  0.450851708650589,\n",
       "  0.3495155870914459,\n",
       "  0.30077821016311646,\n",
       "  0.1542934775352478,\n",
       "  -0.5043416023254395,\n",
       "  ...]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['outputs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids\n",
    "attention_mask \n",
    "\n",
    "# open file in write mode\n",
    "with open(r'./temp-bloom/input_ids.txt', 'w') as fp:\n",
    "    for item in input_ids:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done input_ids')\n",
    "    \n",
    "# open file in write mode\n",
    "with open(r'./temp-bloom/attention_mask.txt', 'w') as fp:\n",
    "    for item in attention_mask:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done attention_mask')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76e7b7",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ad731aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6b0ce106-09a3-4e2c-a085-d2ae8b466ca8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6b0ce106-09a3-4e2c-a085-d2ae8b466ca8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 14 Sep 2022 16:35:37 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018458b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# triton\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# custom CloudWatch\n",
    "#from cloudwatch import get_endpoint_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it  -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:21.08-py3 /bin/bash generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e7e1",
   "metadata": {},
   "source": [
    "## START MME for triton "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb154d",
   "metadata": {},
   "source": [
    "**Upload first**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20911d36",
   "metadata": {},
   "source": [
    "### Upload multiple copies for MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee73801",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1,100):\n",
    "    s3_model_path_triton_mme = sagemaker.s3.S3Uploader().upload(\n",
    "        local_path=\"./triton-serve/model.tar.gz\",\n",
    "        desired_s3_uri=f\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc/model-{ii}\",\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "s3_model_path_mme='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc'\n",
    "print(\"MULTIPLE:Uplodas:\")\n",
    "print(s3_model_path_triton_mme)\n",
    "print(s3_model_path_mme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd65248",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065499b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_mme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f57189",
   "metadata": {},
   "source": [
    "**Create the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5_mme = name_from_base(f\"p5-bert-uc-mme\")\n",
    "print(endpoint_name_p5_mme)\n",
    "\n",
    "container_p5_mme = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_mme,\n",
    "    'Mode':'MultiModel',\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'model-1',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response_mme = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5_mme, ExecutionRoleArn=role, PrimaryContainer=container_p5_mme\n",
    ")\n",
    "print(create_model_response_mme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55113980",
   "metadata": {},
   "source": [
    "**Create the Endpoint config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01871b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response_mme = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5_mme,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\", #\"ml.g4dn.xlarge\",ml.g5.8xlarge\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5_mme,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response_mme[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eea317",
   "metadata": {},
   "source": [
    "**Create the endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response_mme = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, EndpointConfigName=endpoint_name_p5_mme\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response_mme[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57358295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"MME:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"MME:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"MME:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923a7e3",
   "metadata": {},
   "source": [
    "**Test the end point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "max_seq_length=512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, ContentType=\"application/octet-stream\", Body=json.dumps(payload), TargetModel  = \"/model-9/model.tar.gz\"\n",
    ")\n",
    "\n",
    "output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "# -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "output_dict.keys()\n",
    "\n",
    "enc.decode(output_dict['outputs'][0]['data'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5_mme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, ContentType=\"text/json\", Body=json.dumps(payload), TargetModel  = \"/model-9/model.tar.gz\"\n",
    ")\n",
    "output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "# -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47523b7",
   "metadata": {},
   "source": [
    "**set up in S3 payload to be used for inference load testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab918ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=512\n",
    "text_triton = \"\"\"\n",
    "                Create payload JSON and upload it on S3. \n",
    "                This will be used by Inference Recommender to run the load test.\n",
    "              \"\"\"\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Sample payload to be used with Inference Recommender\")\n",
    "print(payload)\n",
    "\n",
    "payload_location = \"./sample-payload/\"\n",
    "!mkdir -p $payload_location\n",
    "\n",
    "payload_archive_name = \"payload.tar.gz\"\n",
    "\n",
    "with open(payload_location + \"request.json\", \"w\") as f:\n",
    "    json.dump(payload, f)\n",
    "\n",
    "\n",
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *\n",
    "\n",
    "print(f\"payload.tar.gz created at {payload_location}/{payload_archive_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ff4e0",
   "metadata": {},
   "source": [
    "**Upload sample payload to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sample_data_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=f\"{payload_archive_name}\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_test_data\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_sample_data_path_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75392fe5",
   "metadata": {},
   "source": [
    "## Inference Load test set up\n",
    "### DOES NOT WORK FOR MME -- SO SKIP this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_domain = \"NATURAL_LANGUAGE_PROCESSING\"\n",
    "ml_task = \"FILL_MASK\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.6.0\"\n",
    "model_tested = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = \"pt-triton-benchmark-model-\" + ts\n",
    "model_package_group_name = \"pt-triton-benchmark-model-group-\" + ts\n",
    "advanced_job = \"pt-triton-benchmark-advanced-job-\" + ts\n",
    "\n",
    "print(f\"SageMaker Model Name: {sm_model_name}\")\n",
    "print(f\"SageMaker Mode Package Name: {model_package_group_name}\")\n",
    "print(f\"SageMaker Advanced Job Name: {advanced_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2693ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_mme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31528a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_infrec_mme = {\n",
    "    'Image': triton_image_uri,\n",
    "    \"NearestModelName\": model_tested, #'model-1',\n",
    "    \"Framework\": ml_framework,\n",
    "    'ModelDataUrl': s3_model_path_mme,\n",
    "    #'Mode':'MultiModel',\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'model-1',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageGroupDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    ")\n",
    "print(f\"Model Registry package group: {model_pacakge_group_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_version_response = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    "    Domain=ml_domain,\n",
    "    Task=ml_task,\n",
    "    SamplePayloadUrl=s3_sample_data_path_triton,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [container_infrec_mme],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\n",
    "            \"ml.g4dn.4xlarge\",\n",
    "            \"ml.g4dn.4xlarge\",\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\"application/octet-stream\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"],\n",
    "    },\n",
    ")\n",
    "model_package_version_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d888600",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_response = sm_client.create_inference_recommendations_job(\n",
    "    JobName=advanced_job,\n",
    "    JobDescription=\"nlp triton Inference Advanced Recommender Job\",\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_version_response[\"ModelPackageArn\"],\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            #{\"InstanceType\": \"ml.p3.8xlarge\"},\n",
    "            #{\"InstanceType\": \"ml.p3.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.p2.16xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.8xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.4xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.12xlarge\"},\n",
    "        ],\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [\n",
    "                {\n",
    "                    \"InitialNumberOfUsers\": 2,\n",
    "                    \"SpawnRate\": 3,\n",
    "                    \"DurationInSeconds\": 900,\n",
    "                },  # simulating 50 users, 2 initial and 3 new users every minute for 16 minutes\n",
    "            ],  # second phase, we will strt with 50 users, steady traffic for 5 minutes\n",
    "        },\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 10, \"MaxParallelOfTests\": 5},\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 30000,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 500}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(advanced_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ecfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ended = False\n",
    "while not ended:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(advanced_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        print(f\"Inference recommender job status: {inference_recommender_job['Status']} \")\n",
    "        ended = True\n",
    "    else:\n",
    "        print(\"Inference recommender job in progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e605df2",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67cb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e27518",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5_mme)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5_mme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be8d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
