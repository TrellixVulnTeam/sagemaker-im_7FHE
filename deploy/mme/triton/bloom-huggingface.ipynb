{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip awscli boto3 sagemaker transformers==4.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/bigscience/bloom-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc26e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p bloom-3b/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566449fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fd3389c",
   "metadata": {},
   "source": [
    "We follow 2 paths -- Standard Hugging Face deploy for BLOOM and then TRITON Based Deployments \n",
    "Triton we follow 2 paths -- Single Model and MME\n",
    "\n",
    "## Triton-serve\n",
    "| -- model.tar.gz  -- this is Triton format -- see below\n",
    "| -- model_bloom.tar.gz  -- this is normal HuggingFace with Pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b12f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bloom-3b/code/inference.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(f\"Bloom:LLM:model_fn()::called dir={model_dir}::\")\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model_8bit, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    print(f\"Bloom:LLM:predict_fn()::called dir={model_and_tokenizer}::\")\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output_sequences = model.generate(input_ids=encoded_input['input_ids'].cuda(), **data)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def torch_predict_fn(input_data, model):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    input_data = data.to(device)\n",
    "    model.eval()\n",
    "    with torch.jit.optimized_execution(True, {\"target_device\": \"eia:0\"}):\n",
    "        output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bloom-3b/code/requirements.txt\n",
    "bitsandbytes\n",
    "accelerate\n",
    "git+https://github.com/huggingface/transformers.git@main#egg=transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72424b6c",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "model.tar.gz/\n",
    "|- pytorch_model.bin\n",
    "|- ....\n",
    "|- code/\n",
    "  |- inference.py\n",
    "  |- requirements.txt \n",
    "  \n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78caebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz triton-serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session=sagemaker.Session()\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./bloom-3b/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/models\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "print(s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path='s3://sagemaker-us-east-1-622343165275/bloom/models/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72627b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "model_id='bloom-3b'\n",
    "endpoint_name = name_from_base(f\"{model_id}-bnb\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version='py38',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b19d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c09d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15d308",
   "metadata": {},
   "source": [
    "## Deploy on p5 instances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14306d6b",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "\n",
    "it expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "\n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge\n",
    "ml.g4dn.xlarge\n",
    "\n",
    "SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\n",
    "SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE. \n",
    "\n",
    "TRITON INFERENCE SERVE -- https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c52068",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve/bloom-3b/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c72451c",
   "metadata": {},
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz -C triton-serve/bloom-3b\n",
    "\n",
    "expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "cd triton-serve\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz bloom-3b\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bloom-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0f653",
   "metadata": {},
   "source": [
    "#### Load the model to convert to a .pt state for TRITON server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771833aa",
   "metadata": {},
   "source": [
    "#### This saves as PyTorch model we need torchscript model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH)  # --    collections.OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c646f",
   "metadata": {},
   "source": [
    "#### This saves as a PyTorch SCRIPT mode based model which is what we need for the model to load in TritonServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660342b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41702412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH) # collections.OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be17774",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['h.0.input_layernorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel, BloomConfig\n",
    "\n",
    "# Initializing a Bloom configuration\n",
    "configuration = BloomConfig(**model_bin)\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = BloomModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(type(configuration))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_tensor)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16056210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82562546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BloomModel, BloomConfig\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "text =  \"Transformers with bnb-Int8 work best on\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bloom-3b\") # - class type is BloomTokenizerFast\n",
    "encoded_input = tokenizer(text, return_tensors='pt').convert_to_tensors()\n",
    "print(type(encoded_input), encoded_input)\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "JSON_PATH=\"./bloom-3b/pytorch_model_json.json\"\n",
    "# write the model to json file\n",
    "with open(JSON_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_bin))\n",
    "    \n",
    "# - load the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(JSON_PATH, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model_8bit = model_8bit.eval()\n",
    "model_8bit.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(model_8bit, dummy_inputs)\n",
    "# Save the TorchScript model\n",
    "traced_model.save(\"./triton-serve/bloom-3b/1/model.pt\")\n",
    "\n",
    "print(\":PyTorch:TorchScript:Model:Saved {}\".format(traced_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef907d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module = torch.jit.trace(\n",
    "    model, \n",
    "    [tokens_tensor, segments_tensors] ,\n",
    "    strict=False)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"./triton-serve/bloom-3b/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf273c65",
   "metadata": {},
   "source": [
    "## Standard Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4536853",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55853168",
   "metadata": {},
   "source": [
    "**Single Model from EXACT s3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8eaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216000\", #\"16777216\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168af62",
   "metadata": {},
   "source": [
    "**SingleModel end point config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95d5ec",
   "metadata": {},
   "source": [
    "**Finally create the end point -- SINGLE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92602981",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ca3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33414044",
   "metadata": {},
   "source": [
    "**Now Invoke the Single Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc86022",
   "metadata": {},
   "source": [
    "**SINGLE Model Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ea12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699421c",
   "metadata": {},
   "source": [
    "## START TRITON MME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8142c",
   "metadata": {},
   "source": [
    "#### MME container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f29f93",
   "metadata": {},
   "source": [
    "**1. Create The MME container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_mme_model_path,\n",
    "    \"Mode\" : \"MultiModel\",\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538f4ce4",
   "metadata": {},
   "source": [
    "**2. Create The MME End Point CONFIG P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd589b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae02aa",
   "metadata": {},
   "source": [
    "**3. Create The MME ENDPoint P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede51dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63d5e7",
   "metadata": {},
   "source": [
    "#### Now invoke the MME end point"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a021e09",
   "metadata": {},
   "source": [
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b/model.tar.gz\n",
    "    \n",
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f379989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"bloom-3b/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"bloom-3b/model.tar.gz\",\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71bd47",
   "metadata": {},
   "source": [
    "## Clean up MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb0301",
   "metadata": {},
   "source": [
    "### Now predict on P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_p5.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2dd56",
   "metadata": {},
   "source": [
    "## BOTO 3 way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2196f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
