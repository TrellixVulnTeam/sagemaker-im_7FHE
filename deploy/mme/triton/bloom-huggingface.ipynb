{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip awscli boto3 sagemaker transformers==4.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd43c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/bigscience/bloom-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p bloom-3b/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14400ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59b6d888",
   "metadata": {},
   "source": [
    "We follow 2 paths -- Standard Hugging Face deploy for BLOOM and then TRITON Based Deployments \n",
    "Triton we follow 2 paths -- Single Model and MME\n",
    "\n",
    "## Triton-serve\n",
    "| -- model.tar.gz  -- this is Triton format -- see below\n",
    "| -- model_bloom.tar.gz  -- this is normal HuggingFace with Pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae7ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom:LLM:starting df commands\n",
      "[['Filesystem', 'Size', 'Used', 'Avail', 'Use%', 'Mounted', 'on'], ['devtmpfs', '185G', '0', '185G', '0%', '/dev'], ['tmpfs', '185G', '152K', '185G', '1%', '/dev/shm'], ['tmpfs', '185G', '996K', '185G', '1%', '/run'], ['tmpfs', '185G', '0', '185G', '0%', '/sys/fs/cgroup'], ['/dev/nvme0n1p1', '140G', '91G', '50G', '65%', '/'], ['/dev/nvme5n1', '197G', '51G', '136G', '28%', '/home/ec2-user/SageMaker'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1000'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1002'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1001']]\n",
      "Bloom:LLM:ENDING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(\"Bloom:LLM:starting df commands\")\n",
    "    df_output_lines = [s.split() for s in os.popen(\"df -Ph\").read().splitlines()]\n",
    "    print(df_output_lines)\n",
    "    print(\"Bloom:LLM:ENDING\")\n",
    "except:\n",
    "    print(\"error in running df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09f9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bloom-3b/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-3b/code/inference.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "try:\n",
    "    print(\"Bloom:LLM:starting df commands\")\n",
    "    df_output_lines = [s.split() for s in os.popen(\"df -Ph\").read().splitlines()]\n",
    "    print(df_output_lines)\n",
    "    print(\"Bloom:LLM:ENDING\")\n",
    "except:\n",
    "    print(\"error in running df\")\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(f\"Bloom:LLM:model_fn()::called dir={model_dir}::\")\n",
    "    model_8bit = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model_8bit, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    print(f\"Bloom:LLM:predict_fn()::called dir={model_and_tokenizer}::\")\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output_sequences = model.generate(input_ids=encoded_input['input_ids'].cuda(), **data)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def torch_predict_fn(input_data, model):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    input_data = data.to(device)\n",
    "    model.eval()\n",
    "    with torch.jit.optimized_execution(True, {\"target_device\": \"eia:0\"}):\n",
    "        output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b8a7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bloom-3b/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-3b/code/requirements.txt\n",
    "bitsandbytes\n",
    "accelerate\n",
    "git+https://github.com/huggingface/transformers.git@main#egg=transformers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47734dfa",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "model.tar.gz/\n",
    "|- pytorch_model.bin\n",
    "|- ....\n",
    "|- code/\n",
    "  |- inference.py\n",
    "  |- requirements.txt \n",
    "  \n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fc7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz triton-serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d714bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/.ipynb_checkpoints/\n",
      "code/.ipynb_checkpoints/inference-checkpoint.py\n",
      "code/requirements.txt\n",
      "code/inference.py\n",
      "config.json\n",
      "LICENSE\n",
      "pytorch_model.bin\n",
      "pytorch_model_json.json\n",
      "README.md\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-3b && rm model.tar.gz && tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a6c6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session=sagemaker.Session()\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e485da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom/models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_model_path = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./bloom-3b/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/models\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "print(s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beee27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_model_path='s3://sagemaker-us-east-1-622343165275/bloom/models/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09df166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "model_id='bloom-3b'\n",
    "endpoint_name = name_from_base(f\"{model_id}-bnb\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version='py38',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48ea8999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51908092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bloom-3b-bnb-2022-09-20-00-10-56-525'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "427eef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers with bnb-Int8 work best on a single threaded CPU. If you have\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c67f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a129a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd90305",
   "metadata": {},
   "source": [
    "## Deploy on p5 instances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed51e6d6",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "\n",
    "it expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "\n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge\n",
    "ml.g4dn.xlarge\n",
    "\n",
    "SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\n",
    "SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE. \n",
    "\n",
    "TRITON INFERENCE SERVE -- https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe987da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve/bloom-3b/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aca0ae3b",
   "metadata": {},
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz -C triton-serve/bloom-3b\n",
    "\n",
    "expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "cd triton-serve\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz bloom-3b\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bloom-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fe2ca",
   "metadata": {},
   "source": [
    "#### Load the model to convert to a .pt state for TRITON server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2678ea",
   "metadata": {},
   "source": [
    "#### This saves as PyTorch model we need torchscript model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH)  # --    collections.OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84499e3f",
   "metadata": {},
   "source": [
    "#### This saves as a PyTorch SCRIPT mode based model which is what we need for the model to load in TritonServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH) # collections.OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['h.0.input_layernorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel, BloomConfig\n",
    "\n",
    "# Initializing a Bloom configuration\n",
    "configuration = BloomConfig(**model_bin)\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = BloomModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(type(configuration))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_tensor)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff70137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BloomModel, BloomConfig\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "text =  \"Transformers with bnb-Int8 work best on\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bloom-3b\") # - class type is BloomTokenizerFast\n",
    "encoded_input = tokenizer(text, return_tensors='pt').convert_to_tensors()\n",
    "print(type(encoded_input), encoded_input)\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "JSON_PATH=\"./bloom-3b/pytorch_model_json.json\"\n",
    "# write the model to json file\n",
    "with open(JSON_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_bin))\n",
    "    \n",
    "# - load the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(JSON_PATH, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model_8bit = model_8bit.eval()\n",
    "model_8bit.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(model_8bit, dummy_inputs)\n",
    "# Save the TorchScript model\n",
    "traced_model.save(\"./triton-serve/bloom-3b/1/model.pt\")\n",
    "\n",
    "print(\":PyTorch:TorchScript:Model:Saved {}\".format(traced_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module = torch.jit.trace(\n",
    "    model, \n",
    "    [tokens_tensor, segments_tensors] ,\n",
    "    strict=False)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"./triton-serve/bloom-3b/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac515b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299825cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bd11c",
   "metadata": {},
   "source": [
    "## Standard Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb774b17",
   "metadata": {},
   "source": [
    "**Single Model from EXACT s3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631bf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216000\", #\"16777216\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72de350",
   "metadata": {},
   "source": [
    "**SingleModel end point config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a115b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae253196",
   "metadata": {},
   "source": [
    "**Finally create the end point -- SINGLE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726543c",
   "metadata": {},
   "source": [
    "**Now Invoke the Single Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea177fd5",
   "metadata": {},
   "source": [
    "**SINGLE Model Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e771da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4ed02",
   "metadata": {},
   "source": [
    "## START TRITON MME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeba47b",
   "metadata": {},
   "source": [
    "#### MME container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54547e",
   "metadata": {},
   "source": [
    "**1. Create The MME container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd59bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_mme_model_path,\n",
    "    \"Mode\" : \"MultiModel\",\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d194600",
   "metadata": {},
   "source": [
    "**2. Create The MME End Point CONFIG P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019eb8d2",
   "metadata": {},
   "source": [
    "**3. Create The MME ENDPoint P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ccb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c959d3e",
   "metadata": {},
   "source": [
    "#### Now invoke the MME end point"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75761c45",
   "metadata": {},
   "source": [
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b/model.tar.gz\n",
    "    \n",
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac661",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"bloom-3b/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"bloom-3b/model.tar.gz\",\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b59f3e",
   "metadata": {},
   "source": [
    "## Clean up MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ba3b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86396/885946240.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm_client' is not defined"
     ]
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956979dc",
   "metadata": {},
   "source": [
    "### Now predict on P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d442abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5401b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da73cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_p5.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f0d39",
   "metadata": {},
   "source": [
    "## BOTO 3 way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30485f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
