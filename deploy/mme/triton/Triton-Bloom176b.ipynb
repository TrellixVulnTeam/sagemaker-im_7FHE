{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4db7ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/microsoft/bloom-deepspeed-inference-int8/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61696869",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05201ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c6ff9",
   "metadata": {},
   "source": [
    "#### Start triton offering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d44bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        7.7G     0  7.7G   0% /dev\n",
      "tmpfs           7.7G  4.0K  7.7G   1% /dev/shm\n",
      "tmpfs           7.7G  608K  7.7G   1% /run\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  140G   89G   52G  64% /\n",
      "/dev/nvme2n1    985G  357G  578G  39% /home/ec2-user/SageMaker\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1000\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1001\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1002\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bee23a",
   "metadata": {},
   "source": [
    "### Start Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbde580",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bloom-176b\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -exclude \"*.tar\" -zcvf model.tar.gz bert-gptj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "afb9e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./bloom-176b\n",
    "!mkdir -p ./bloom-176b/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9c28d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.pt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.pt\n",
    "print(\"hello model pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4cfbc",
   "metadata": {},
   "source": [
    "#### Following this configuration \n",
    "\n",
    "https://github.com/triton-inference-server/python_backend\n",
    "\n",
    "Number of Inputs and # of Outputs have to be the same in triton \n",
    "\n",
    "for Testing or running Triton locally or to look at the configurations -- use -- https://chroniclesofai.com/mlops-chapter-8-model-server-with-nvidia-triton-local-part-1-b/\n",
    "\n",
    "    \n",
    "**Below is the RAW file with 2 inputs and outputs**    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "52007762",
   "metadata": {},
   "source": [
    "\n",
    "%%writefile ./bloom-176b/config.pbtxt\n",
    "\n",
    "name: \"bloom-176b\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6231915",
   "metadata": {},
   "source": [
    "**But we wil test with just 1 input and output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2eb7af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/config.pbtxt\n",
    "\n",
    "name: \"bloom-176b\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 512 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7c9dafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "reqs = subprocess.check_output([sys.executable, '-m', 'pip','freeze'])\n",
    "#[r.decode().split('==')[0] for r in reqs.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5e060af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.py\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install nvidia-pyindex\n",
    "# !pip install tritonclient[http]\n",
    "# !pip install torch\n",
    "# implement pip as a subprocess:\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install','torch'])\n",
    "\n",
    "# process output with an API in the subprocess module:\n",
    "reqs = subprocess.check_output([sys.executable, '-m', 'pip','freeze'])\n",
    "installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n",
    "\n",
    "print(installed_packages)\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "class TritonPythonModel:\n",
    "    # Every Python model must have \"TritonPythonModel\" as the class name!\n",
    "    def initialize(self, args):\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize called:args={args}\", flush=True)\n",
    "        self.output_dtype = pb_utils.triton_string_to_numpy(\n",
    "            pb_utils.get_output_config_by_name(\n",
    "                json.loads(args['model_config']), \"OUTPUT__0\" #\"output\"\n",
    "            )['data_type']\n",
    "        )\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize::output_type={self.output_dtype}\", flush=True)\n",
    "        \n",
    "        #from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "        self.model = None #T5ForConditionalGeneration.from_pretrained(\"t5-small\").cuda()\n",
    "        \n",
    "        try:\n",
    "            from os import listdir\n",
    "            from os.path import isfile, join\n",
    "            onlyfiles = [f for f in listdir(\".\") if isfile(join('.', f))]\n",
    "            print(f\"TritonPythonModel:Bloom-176b:onlyfiles::{onlyfiles}\", flush=True)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialized\", flush=True)\n",
    "\n",
    "    def execute(self, requests):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: execute called:requests={requests}:\", flush=True)\n",
    "        \n",
    "        responses = []\n",
    "        try:\n",
    "            for request in requests:\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute :EACH:Request:requests={request}:\", flush=True)\n",
    "                \n",
    "                input_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT__0\")\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute :EACH:Request:input_0={input_0}::\")\n",
    "                try:\n",
    "                    input_ids = input_0.as_numpy()\n",
    "                    \n",
    "                     # Convert to numpy array on cpu:\n",
    "                    input_ids = torch.as_tensor(input_ids).long().cuda()\n",
    "                    summary = self.model.generate(input_ids, num_beams=1)\n",
    "                    np_summary = summary.cpu().int().detach().numpy()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                summary = [1]*512  \n",
    "\n",
    "               \n",
    "                \n",
    "                np_summary = np.array(summary)\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[\n",
    "                        pb_utils.Tensor(\n",
    "                            \"OUTPUT__0\",\n",
    "                            np_summary.astype(self.output_dtype)\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "                responses.append(inference_response)\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute():responses={responses}::{len(responses)}::\")\n",
    "        except :\n",
    "            print(\"TritonPythonModel:Bloom-176b: execute():Exception Thrown\", flush=True)\n",
    "            print (f\"TritonPythonModel:Bloom-176b: execute(): traceback:eror:predict={traceback.format_exc()}\", flush=True)\n",
    "            \n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: finalize()::TritonPythonModel finalized: called\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "623bc5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘model.tar.gz’: No such file or directory\n",
      "bloom-176b/\n",
      "bloom-176b/1/\n",
      "bloom-176b/1/model.py\n",
      "bloom-176b/1/.ipynb_checkpoints/\n",
      "bloom-176b/1/.ipynb_checkpoints/model-checkpoint.py\n",
      "bloom-176b/1/.ipynb_checkpoints/model-checkpoint.pt\n",
      "bloom-176b/1/model.pt\n",
      "bloom-176b/.ipynb_checkpoints/\n",
      "bloom-176b/.ipynb_checkpoints/config-checkpoint.pbtxt\n",
      "bloom-176b/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-11 04:22 bloom-176b/\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-10 20:09 bloom-176b/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 3697 2022-10-13 00:11 bloom-176b/1/model.py\n",
      "drwxrwxr-x ec2-user/ec2-user    0 2022-10-10 20:09 bloom-176b/1/.ipynb_checkpoints/\n",
      "-rw-rw-r-- ec2-user/ec2-user 2112 2022-10-10 16:49 bloom-176b/1/.ipynb_checkpoints/model-checkpoint.py\n",
      "-rw-rw-r-- ec2-user/ec2-user   24 2022-10-10 16:49 bloom-176b/1/.ipynb_checkpoints/model-checkpoint.pt\n",
      "-rw-rw-r-- ec2-user/ec2-user   24 2022-10-12 22:58 bloom-176b/1/model.pt\n",
      "drwxrwxr-x ec2-user/ec2-user    0 2022-10-10 17:11 bloom-176b/.ipynb_checkpoints/\n",
      "-rw-rw-r-- ec2-user/ec2-user  268 2022-10-11 04:22 bloom-176b/.ipynb_checkpoints/config-checkpoint.pbtxt\n",
      "-rw-rw-r-- ec2-user/ec2-user  381 2022-10-12 23:15 bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-176b && rm model.tar.gz\n",
    "!rm model.tar.gz\n",
    "!tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\".bin\" --exclude \".tar\" -zcvf model.tar.gz bloom-176b  \n",
    "!tar -tvf model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f3450820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c1cb2f9",
   "metadata": {},
   "source": [
    "%%sh\n",
    "# Cell 11\n",
    "\n",
    "echo \"Starting Docker Build\"\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=bloom-176b-sagemaker\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "echo \"fullname:image=${fullname}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "echo \"Starting the Docker Build with ${algorithm_name}\"\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "echo \"Pushing Docker image ${fullname} to ECR \"\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b622d42",
   "metadata": {},
   "source": [
    "#### Start creating the end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c03322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3_client = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c438ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b/model.tar.gz\n",
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "929c15ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)\n",
    "\n",
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e81a9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ff847688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom-176b--2022-10-13-00-11-16-799\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:622343165275:model/bloom-176b--2022-10-13-00-11-16-799', 'ResponseMetadata': {'RequestId': '10839bec-e854-405e-8a4c-48293d86f6e5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '10839bec-e854-405e-8a4c-48293d86f6e5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '97', 'date': 'Thu, 13 Oct 2022 00:11:16 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"bloom-176b-\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-176b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "760f647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint-config/bloom-176b--2022-10-13-00-11-16-799',\n",
       " 'ResponseMetadata': {'RequestId': 'a21b169d-0e7f-4e18-97ca-f7de1cdd221a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a21b169d-0e7f-4e18-97ca-f7de1cdd221a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '116',\n",
       "   'date': 'Thu, 13 Oct 2022 00:11:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_name_p5,\n",
    "    ProductionVariants = [{\n",
    "        'InstanceType'        : 'ml.g4dn.xlarge',\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName'           : endpoint_name_p5,\n",
    "        'VariantName'         : 'AllTraffic'}])\n",
    "\n",
    "create_endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d23ec548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-13-00-11-16-799',\n",
       " 'ResponseMetadata': {'RequestId': '269dcb94-2345-4746-a35b-ddf609e11827',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '269dcb94-2345-4746-a35b-ddf609e11827',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '103',\n",
       "   'date': 'Thu, 13 Oct 2022 00:11:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the end point\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName         = endpoint_name_p5,\n",
    "    EndpointConfigName   = endpoint_name_p5)\n",
    "\n",
    "create_endpoint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "853bded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-13-00-11-16-799\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4df07a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bloom-176b', 'model_version': '1', 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'INT32', 'shape': [512], 'data': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}]}\n",
      "CPU times: user 8.28 ms, sys: 3.6 ms, total: 11.9 ms\n",
      "Wall time: 2.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "text='this is a sample 1 line '\n",
    "\n",
    "text = [1]*512\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "            'datatype':\"INT32\",\n",
    "            'shape':[1, 512],\n",
    "\n",
    "        }\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d079adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3d63723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 115)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# print(tokenizer)\n",
    "input_ids = tokenizer(\"summarize: SageMaker enables customers to deploy a model using custom code with NVIDIA Triton Inference Server. This functionality is available through the development of Triton Inference Server Containers. These containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. For a list of all available Deep Learning Containers images, see Available Deep Learning Containers Images. Deep Learning Containers images are maintained and regularly updated with security patches.\", return_tensors='pt').input_ids\n",
    "print(input_ids.numpy().astype(np.int32).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70748d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set for http client\n",
    "\n",
    "input_data = input_ids.numpy().astype(np.int32)\n",
    "\n",
    "input_name = 'input'\n",
    "output_name = \"output\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "inputs.append(httpclient.InferInput(input_name, input_data.shape, \"INT32\"))\n",
    "inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "outputs.append(\n",
    "    httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(inputs, outputs=outputs)\n",
    "\n",
    "request_body_input, header_length = httpclient.InferenceServerClient.generate_request_body(inputs)\n",
    "\n",
    "print(header_length)\n",
    "\n",
    "request_body_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name_p5,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel='model.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "output_data = result.as_numpy(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16067fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.decode(\n",
    "            output_data[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dc25e",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2c06c6ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-13-00-01-27-641\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16142/574915204.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-13-00-01-27-641\"."
     ]
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aa789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
