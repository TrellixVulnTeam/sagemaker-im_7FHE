{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f106c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc321e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087370fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/microsoft/bloom-deepspeed-inference-int8/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2852be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722eee5c",
   "metadata": {},
   "source": [
    "#### Start triton offering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbdc6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs        7.7G     0  7.7G   0% /dev\n",
      "tmpfs           7.7G  4.0K  7.7G   1% /dev/shm\n",
      "tmpfs           7.7G  608K  7.7G   1% /run\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  140G   89G   52G  64% /\n",
      "/dev/nvme2n1    985G  357G  578G  39% /home/ec2-user/SageMaker\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1000\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1001\n",
      "tmpfs           1.6G     0  1.6G   0% /run/user/1002\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1bd87",
   "metadata": {},
   "source": [
    "### Start Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f973129",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bert-gptj\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -exclude \"*.tar\" -zcvf model.tar.gz bert-gptj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af2f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./bloom-176b\n",
    "!mkdir -p ./bloom-176b/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd166018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.pt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.pt\n",
    "print(\"hello model pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4485d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/config.pbtxt\n",
    "\n",
    "name: \"t5\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 8\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "    \n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group {\n",
    "  count: 1\n",
    "  kind: KIND_GPU\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68fef4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.py\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton_python_backend_utils as pb_utils\n",
    "\n",
    "class TritonPythonModel:\n",
    "    # Every Python model must have \"TritonPythonModel\" as the class name!\n",
    "    def initialize(self, args):\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize called:args={args}\")\n",
    "        self.output_dtype = pb_utils.triton_string_to_numpy(\n",
    "            pb_utils.get_output_config_by_name(\n",
    "                json.loads(args['model_config']), \"output\"\n",
    "            )['data_type']\n",
    "        )\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize::output_type={self.output_dtype}\")\n",
    "        \n",
    "        #from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "        self.model = None #T5ForConditionalGeneration.from_pretrained(\"t5-small\").cuda()\n",
    "        print(\"TritonPythonModel initialized\")\n",
    "\n",
    "    def execute(self, requests):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: execute called:requests={requests}:\")\n",
    "        \n",
    "        responses = []\n",
    "        try:\n",
    "            for request in requests:\n",
    "                input = pb_utils.get_input_tensor_by_name(request, \"input\")\n",
    "                input_ids = input.as_numpy()\n",
    "\n",
    "                input_ids = torch.as_tensor(input_ids).long().cuda()\n",
    "                summary = [1,2,3] # self.model.generate(input_ids, num_beams=1)\n",
    "\n",
    "                # Convert to numpy array on cpu:\n",
    "                np_summary = summary.cpu().int().detach().numpy()\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[\n",
    "                        pb_utils.Tensor(\n",
    "                            \"output\",\n",
    "                            np_summary.astype(self.output_dtype)\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                responses.append(inference_response)\n",
    "        except Exception as e:\n",
    "            print(f\"TritonPythonModel:Bloom-176b: execute:Exception Thrown\")\n",
    "            print (e.message, e.args)\n",
    "            return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: finalize()::TritonPythonModel finalized: called\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4eaab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘model.tar.gz’: No such file or directory\n",
      "bloom-176b/\n",
      "bloom-176b/1/\n",
      "bloom-176b/1/model.py\n",
      "bloom-176b/1/model.pt\n",
      "bloom-176b/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-10 05:34 bloom-176b/\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-10 05:26 bloom-176b/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 2112 2022-10-10 16:22 bloom-176b/1/model.py\n",
      "-rw-rw-r-- ec2-user/ec2-user   24 2022-10-10 16:22 bloom-176b/1/model.pt\n",
      "-rw-rw-r-- ec2-user/ec2-user  260 2022-10-10 16:22 bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-176b && rm model.tar.gz\n",
    "!rm model.tar.gz\n",
    "!tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\".bin\" --exclude \".tar\" -zcvf model.tar.gz bloom-176b  \n",
    "!tar -tvf model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c93ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa4c3de2",
   "metadata": {},
   "source": [
    "%%sh\n",
    "# Cell 11\n",
    "\n",
    "echo \"Starting Docker Build\"\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=bloom-176b-sagemaker\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "echo \"fullname:image=${fullname}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "echo \"Starting the Docker Build with ${algorithm_name}\"\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "echo \"Pushing Docker image ${fullname} to ECR \"\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096826e7",
   "metadata": {},
   "source": [
    "#### Start creating the end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e7369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3_client = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b173a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b/model.tar.gz\n",
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65d3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)\n",
    "\n",
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f31be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f088744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5-bert-uc--2022-10-10-16-22-59-716\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:622343165275:model/p5-bert-uc--2022-10-10-16-22-59-716', 'ResponseMetadata': {'RequestId': 'b17387d3-74ce-4187-bb47-d2a1c727f932', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b17387d3-74ce-4187-bb47-d2a1c727f932', 'content-type': 'application/x-amz-json-1.1', 'content-length': '97', 'date': 'Mon, 10 Oct 2022 16:22:59 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-bert-uc-\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-176b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8459c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint-config/p5-bert-uc--2022-10-10-16-22-59-716',\n",
       " 'ResponseMetadata': {'RequestId': 'b705cdd1-f83b-4a41-89d7-a916d56a6b8f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b705cdd1-f83b-4a41-89d7-a916d56a6b8f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '116',\n",
       "   'date': 'Mon, 10 Oct 2022 16:22:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_name_p5,\n",
    "    ProductionVariants = [{\n",
    "        'InstanceType'        : 'ml.g4dn.xlarge',\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName'           : endpoint_name_p5,\n",
    "        'VariantName'         : 'AllTraffic'}])\n",
    "\n",
    "create_endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29cb40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the end point\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName         = endpoint_name_p5,\n",
    "    EndpointConfigName   = endpoint_name_p5)\n",
    "\n",
    "create_endpoint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b8b07e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint/p5-bert-uc--2022-10-10-16-22-59-716',\n",
       " 'ResponseMetadata': {'RequestId': '5d5e32e0-60f2-4476-8626-29c5028d8b07',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5d5e32e0-60f2-4476-8626-29c5028d8b07',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '103',\n",
       "   'date': 'Mon, 10 Oct 2022 16:23:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_endpoint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed49aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d5816fff5e4fa28ac994cda3c7d6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 115)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# print(tokenizer)\n",
    "input_ids = tokenizer(\"summarize: SageMaker enables customers to deploy a model using custom code with NVIDIA Triton Inference Server. This functionality is available through the development of Triton Inference Server Containers. These containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. For a list of all available Deep Learning Containers images, see Available Deep Learning Containers Images. Deep Learning Containers images are maintained and regularly updated with security patches.\", return_tensors='pt').input_ids\n",
    "print(input_ids.numpy().astype(np.int32).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17676be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set for http client\n",
    "\n",
    "input_data = input_ids.numpy().astype(np.int32)\n",
    "\n",
    "input_name = 'input'\n",
    "output_name = \"output\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "inputs.append(httpclient.InferInput(input_name, input_data.shape, \"INT32\"))\n",
    "inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "outputs.append(\n",
    "    httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "    inputs, outputs=outputs)\n",
    "\n",
    "print(header_length)\n",
    "\n",
    "request_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel='model.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "output_data = result.as_numpy(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55315f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.decode(\n",
    "            output_data[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84f5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
