{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a06542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30600518",
   "metadata": {},
   "source": [
    "### Pull microsoft model quanized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ad83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/microsoft/bloom-deepspeed-inference-int8/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b974737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d945e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-deepspeed-inference-int8 && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d190d",
   "metadata": {},
   "source": [
    "#### Start triton offering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17946d2",
   "metadata": {},
   "source": [
    "### Microsoft quantized model uploaded too\n",
    "\n",
    "aws s3 sync ./bloom-deepspeed-inference-int8/ s3://sagemaker-us-east-1-622343165275/bloom-176B/raw_model_microsoft/\n",
    "\n",
    "s3://sagemaker-us-east-1-622343165275/bloom-176B/raw_model_microsoft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae0903",
   "metadata": {},
   "source": [
    "### Start Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb47fa",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bloom-176b\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -exclude \"*.tar\" -zcvf model.tar.gz bert-gptj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5cd614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./bloom-176b\n",
    "!mkdir -p ./bloom-176b/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da451074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.pt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.pt\n",
    "print(\"hello model pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6547633",
   "metadata": {},
   "source": [
    "#### Following this configuration \n",
    "\n",
    "https://github.com/triton-inference-server/python_backend\n",
    "\n",
    "Number of Inputs and # of Outputs have to be the same in triton \n",
    "\n",
    "for Testing or running Triton locally or to look at the configurations -- use -- https://chroniclesofai.com/mlops-chapter-8-model-server-with-nvidia-triton-local-part-1-b/\n",
    "\n",
    "    \n",
    "**Below is the RAW file with 2 inputs and outputs**    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e8866f9",
   "metadata": {},
   "source": [
    "\n",
    "%%writefile ./bloom-176b/config.pbtxt\n",
    "\n",
    "name: \"bloom-176b\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530005c4",
   "metadata": {},
   "source": [
    "**But we wil test with just 1 input and output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24560ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/config.pbtxt\n",
    "\n",
    "name: \"bloom-176b\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ 512 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3ee22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "reqs = subprocess.check_output([sys.executable, '-m', 'pip','freeze'])\n",
    "#[r.decode().split('==')[0] for r in reqs.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dff4b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bloom-176b/1/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bloom-176b/1/model.py\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install nvidia-pyindex\n",
    "# !pip install tritonclient[http]\n",
    "# !pip install torch\n",
    "# implement pip as a subprocess:\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install','torch'])\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install','boto3'])\n",
    "\n",
    "# process output with an API in the subprocess module:\n",
    "reqs = subprocess.check_output([sys.executable, '-m', 'pip','freeze'])\n",
    "installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n",
    "\n",
    "print(f\"TritonPythonModel:Bloom-176b:installed_packages={installed_packages}:\")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton_python_backend_utils as pb_utils\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "print(f\"TritonPythonModel:Bloom-176b:s3_client={s3_client}::\")\n",
    "\n",
    "\n",
    "class TritonPythonModel:\n",
    "    # Every Python model must have \"TritonPythonModel\" as the class name!\n",
    "    def initialize(self, args):\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize():: called:args={args}\", flush=True)\n",
    "        self.output_dtype = pb_utils.triton_string_to_numpy(\n",
    "            pb_utils.get_output_config_by_name(\n",
    "                json.loads(args['model_config']), \"OUTPUT__0\" #\"output\"\n",
    "            )['data_type']\n",
    "        )\n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize()::output_type={self.output_dtype}\", flush=True)\n",
    "        \n",
    "        #from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "        self.model = None #T5ForConditionalGeneration.from_pretrained(\"t5-small\").cuda()\n",
    "        \n",
    "        try:\n",
    "            from os import listdir\n",
    "            from os.path import isfile, join\n",
    "            onlyfiles = [f for f in listdir(\".\") if isfile(join('.', f))]\n",
    "            print(f\"TritonPythonModel:Bloom-176b:initialize()::current:working:dir={os.getcwd()}::\", flush=True)\n",
    "            print(f\"TritonPythonModel:Bloom-176b:initialize()::onlyfiles::{onlyfiles}\", flush=True)\n",
    "            \n",
    "            print(f\"TritonPythonModel:Bloom-176b:initialize()::starting:download::\", flush=True)\n",
    "            self.start_download()\n",
    "        except:\n",
    "            print(f\"TritonPythonModel:Bloom-176b:initialize():ERROR IN DOWNLOAD={traceback.format_exc()}::\", flush=True)\n",
    "            \n",
    "        print(f\"TritonPythonModel:Bloom-176b:initialize()::Completed::\", flush=True)\n",
    "\n",
    "    def download_files_executor(self, s3_files, model_dir='/tmp/model'):\n",
    "        \n",
    "        print(f\"TritonPythonModel:Bloom-176b:download_files_executor():files={s3_files}::model_dir={model_dir}::Started::\", flush=True)\n",
    "        \n",
    "        def _download_file(s3_1_file_path, model_dir):\n",
    "\n",
    "            global s3_client\n",
    "\n",
    "            local_file_path = os.path.join(model_dir, s3_1_file_path.split(\"/\")[-1])\n",
    "\n",
    "            bucket, *key = s3_1_file_path.split(\"/\")\n",
    "            key = \"/\".join(key)\n",
    "            print(f\"TritonPythonModel:Bloom-176b:download_files_executor():EXECUTOR::STARTED::local_file_path={local_file_path}::bucket={bucket}::key={key}::\", flush=True)  \n",
    "            try:\n",
    "                s3_client.download_file(bucket, key, local_file_path)\n",
    "                print(f\"TritonPythonModel:Bloom-176b:download_files_executor():EXECUTOR::ENDED::local_file_path={local_file_path}::bucket={bucket}::key={key}::\", flush=True)\n",
    "            except:\n",
    "                #time.sleep(0.5)\n",
    "                print(f\"TritonPythonModel:Bloom-176b:download_files_executor():EXECUTOR:ERROR IN DOWNLOAD={traceback.format_exc()}::bucket={bucket}:: key={key}::\", flush=True)\n",
    "\n",
    "            return local_file_path\n",
    "\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = [executor.submit(_download_file, s3_file, model_dir) for s3_file in s3_files]\n",
    "            for future in as_completed(futures):\n",
    "                print (f\"TritonPythonModel:Bloom-176b:download_files_executor()::downloaded: {future.result()}\", flush=True)\n",
    "                \n",
    "        print(f\"TritonPythonModel:Bloom-176b:download_files_executor():All:Executors:Started!!\")\n",
    "\n",
    "    def start_download(self):\n",
    "        print(f\"TritonPythonModel:Bloom-176b:start_download()::called::\", flush=True) \n",
    "        # - s3://sagemaker-us-east-1-622343165275/bloom-176B/raw_model_microsoft/config.json  -- QUANTIZED model\n",
    "        # - s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b/model.tar.gz'\n",
    "        s3_objects = s3_client.list_objects(Bucket='sagemaker-us-east-1-622343165275', Prefix='bloom-176B/raw_model_microsoft/')[\"Contents\"]\n",
    "        s3_paths = [os.path.join('sagemaker-us-east-1-622343165275', obj[\"Key\"]) for obj in s3_objects]\n",
    "        print(f\"TritonPythonModel:Bloom-176b::start_download()::downloading files::s3_paths:list::{s3_paths}::\", flush=True)\n",
    "\n",
    "        model_dir = \"/tmp/model\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        print(f\"TritonPythonModel:Bloom-176b::start_download()::temp:directory:created:::Now to start Executors\", flush=True)\n",
    "        \n",
    "        self.download_files_executor(s3_paths, model_dir)               \n",
    "\n",
    "                       \n",
    "    def execute(self, requests):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: execute() called:requests={requests}:\", flush=True)\n",
    "        \n",
    "        responses = []\n",
    "        try:\n",
    "            for request in requests:\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute :EACH:Request:requests={request}:\", flush=True)\n",
    "                \n",
    "                input_0 = pb_utils.get_input_tensor_by_name(request, \"INPUT__0\")\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute :EACH:Request:input_0={input_0}::\", flush=True)\n",
    "                try:\n",
    "                    input_ids = input_0.as_numpy()\n",
    "                    \n",
    "                     # Convert to numpy array on cpu:\n",
    "                    input_ids = torch.as_tensor(input_ids).long().cuda()\n",
    "                    summary = self.model.generate(input_ids, num_beams=1)\n",
    "                    np_summary = summary.cpu().int().detach().numpy()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                summary = [1]*512  \n",
    "\n",
    "               \n",
    "                \n",
    "                np_summary = np.array(summary)\n",
    "                inference_response = pb_utils.InferenceResponse(\n",
    "                    output_tensors=[\n",
    "                        pb_utils.Tensor(\n",
    "                            \"OUTPUT__0\",\n",
    "                            np_summary.astype(self.output_dtype)\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "                responses.append(inference_response)\n",
    "                print(f\"TritonPythonModel:Bloom-176b: execute():responses={responses}::{len(responses)}::\", flush=True)\n",
    "        except :\n",
    "            print(\"TritonPythonModel:Bloom-176b: execute():Exception Thrown\", flush=True)\n",
    "            print (f\"TritonPythonModel:Bloom-176b: execute(): traceback:eror:predict={traceback.format_exc()}\", flush=True)\n",
    "            \n",
    "        return responses\n",
    "\n",
    "    def finalize(self):\n",
    "        print(f\"TritonPythonModel:Bloom-176b: finalize()::TritonPythonModel finalized: called\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bec3e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘model.tar.gz’: No such file or directory\n",
      "bloom-176b/\n",
      "bloom-176b/1/\n",
      "bloom-176b/1/model.py\n",
      "bloom-176b/1/.ipynb_checkpoints/\n",
      "bloom-176b/1/.ipynb_checkpoints/model-checkpoint.py\n",
      "bloom-176b/1/.ipynb_checkpoints/model-checkpoint.pt\n",
      "bloom-176b/1/model.pt\n",
      "bloom-176b/.ipynb_checkpoints/\n",
      "bloom-176b/.ipynb_checkpoints/config-checkpoint.pbtxt\n",
      "bloom-176b/config.pbtxt\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-11 04:22 bloom-176b/\n",
      "drwxrwxr-x ec2-user/ec2-user 0 2022-10-10 20:09 bloom-176b/1/\n",
      "-rw-rw-r-- ec2-user/ec2-user 7092 2022-10-17 19:57 bloom-176b/1/model.py\n",
      "drwxrwxr-x ec2-user/ec2-user    0 2022-10-10 20:09 bloom-176b/1/.ipynb_checkpoints/\n",
      "-rw-rw-r-- ec2-user/ec2-user 2112 2022-10-10 16:49 bloom-176b/1/.ipynb_checkpoints/model-checkpoint.py\n",
      "-rw-rw-r-- ec2-user/ec2-user   24 2022-10-10 16:49 bloom-176b/1/.ipynb_checkpoints/model-checkpoint.pt\n",
      "-rw-rw-r-- ec2-user/ec2-user   24 2022-10-17 19:27 bloom-176b/1/model.pt\n",
      "drwxrwxr-x ec2-user/ec2-user    0 2022-10-10 17:11 bloom-176b/.ipynb_checkpoints/\n",
      "-rw-rw-r-- ec2-user/ec2-user  268 2022-10-11 04:22 bloom-176b/.ipynb_checkpoints/config-checkpoint.pbtxt\n",
      "-rw-rw-r-- ec2-user/ec2-user  313 2022-10-17 19:27 bloom-176b/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-176b && rm model.tar.gz\n",
    "!rm model.tar.gz\n",
    "!tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\".bin\" --exclude \".tar\" -zcvf model.tar.gz bloom-176b  \n",
    "!tar -tvf model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f91e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef671bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "98838239",
   "metadata": {},
   "source": [
    "%%sh\n",
    "# Cell 11\n",
    "\n",
    "echo \"Starting Docker Build\"\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=bloom-176b-sagemaker\n",
    "\n",
    "cd container\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-east-1 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "echo \"fullname:image=${fullname}\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "echo \"Starting the Docker Build with ${algorithm_name}\"\n",
    "docker build -q -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "echo \"Pushing Docker image ${fullname} to ECR \"\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c473e",
   "metadata": {},
   "source": [
    "#### Start creating the end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de528454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3_client = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31adff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b/model.tar.gz\n",
      "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\n"
     ]
    }
   ],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-176b'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "159b16f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785573368785.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:22.07-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)\n",
    "\n",
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a1e28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41f36094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "733f4a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom-176b--2022-10-17-19-59-17-677\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:622343165275:model/bloom-176b--2022-10-17-19-59-17-677', 'ResponseMetadata': {'RequestId': '9ec53759-fedb-43dd-b837-a6abdbfe09d8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9ec53759-fedb-43dd-b837-a6abdbfe09d8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '97', 'date': 'Mon, 17 Oct 2022 19:59:17 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"bloom-176b-\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-176b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"1677721600\", #\"16777216000\", \"16777216\"\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cd19ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint-config/bloom-176b--2022-10-17-19-59-17-677',\n",
       " 'ResponseMetadata': {'RequestId': '1335a240-b857-4786-bfef-19b54096ed8a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1335a240-b857-4786-bfef-19b54096ed8a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '116',\n",
       "   'date': 'Mon, 17 Oct 2022 19:59:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_name_p5,\n",
    "    ProductionVariants = [{\n",
    "        'InstanceType'                               : 'ml.g5.48xlarge', #'ml.p3.2xlarge', #'ml.g4dn.xlarge', 'ml.g5.48xlarge'\n",
    "        'InitialVariantWeight'                       : 1,\n",
    "        'InitialInstanceCount'                       : 1,\n",
    "        'ModelName'                                  : endpoint_name_p5,\n",
    "        'VariantName'                                : 'AllTraffic',\n",
    "        #'VolumeSizeInGB'                             : 400,\n",
    "        'ModelDataDownloadTimeoutInSeconds'          : 3600, #123,\n",
    "        'ContainerStartupHealthCheckTimeoutInSeconds': 3600, #123,\n",
    "        \n",
    "    }])\n",
    "\n",
    "create_endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71ffcd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-17-19-59-17-677',\n",
       " 'ResponseMetadata': {'RequestId': '0cc469fc-c5d6-4edf-ae0a-897f70be51fb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0cc469fc-c5d6-4edf-ae0a-897f70be51fb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '103',\n",
       "   'date': 'Mon, 17 Oct 2022 19:59:24 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the end point\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName         = endpoint_name_p5,\n",
    "    EndpointConfigName   = endpoint_name_p5)\n",
    "\n",
    "create_endpoint_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acbb356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Failed\n",
      "Arn: arn:aws:sagemaker:us-east-1:622343165275:endpoint/bloom-176b--2022-10-17-19-59-17-677\n",
      "Status: Failed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91070a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "text='this is a sample 1 line '\n",
    "\n",
    "text = [1]*512\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "            'datatype':\"INT32\",\n",
    "            'shape':[1, 512],\n",
    "\n",
    "        }\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# print(tokenizer)\n",
    "input_ids = tokenizer(\"summarize: SageMaker enables customers to deploy a model using custom code with NVIDIA Triton Inference Server. This functionality is available through the development of Triton Inference Server Containers. These containers include NVIDIA Triton Inference Server, support for common ML frameworks, and useful environment variables that let you optimize performance on SageMaker. For a list of all available Deep Learning Containers images, see Available Deep Learning Containers Images. Deep Learning Containers images are maintained and regularly updated with security patches.\", return_tensors='pt').input_ids\n",
    "print(input_ids.numpy().astype(np.int32).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75477035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set for http client\n",
    "\n",
    "input_data = input_ids.numpy().astype(np.int32)\n",
    "\n",
    "input_name = 'input'\n",
    "output_name = \"output\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "inputs.append(httpclient.InferInput(input_name, input_data.shape, \"INT32\"))\n",
    "inputs[0].set_data_from_numpy(input_data, binary_data=True)\n",
    "outputs.append(\n",
    "    httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(inputs, outputs=outputs)\n",
    "\n",
    "request_body_input, header_length = httpclient.InferenceServerClient.generate_request_body(inputs)\n",
    "\n",
    "print(header_length)\n",
    "\n",
    "request_body_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14560bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(EndpointName=endpoint_name_p5,\n",
    "                                  ContentType='application/vnd.sagemaker-triton.binary+json;json-header-size={}'.format(header_length),\n",
    "                                  Body=request_body,\n",
    "                                 TargetModel='model.tar.gz')\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response['ContentType'][len(header_length_prefix):]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response['Body'].read(), header_length=int(header_length_str))\n",
    "output_data = result.as_numpy(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b66c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_output = tokenizer.decode(\n",
    "            output_data[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False,\n",
    "        )\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7efd7f",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320e9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
