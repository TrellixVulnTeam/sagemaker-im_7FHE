{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip awscli boto3 sagemaker transformers==4.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8909179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d613839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/bigscience/bloom-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p bloom-176b/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18718e77",
   "metadata": {},
   "source": [
    "#### Structure follows - https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "910d3c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-176b/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-176b/code/inference.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def download_files(files, model_dir):\n",
    "    \n",
    "    def _download_file(s3_path, model_dir):\n",
    "        \n",
    "        global s3_client\n",
    "        \n",
    "        local_file_path = os.path.join(model_dir, s3_path.split(\"/\")[-1])\n",
    "        \n",
    "        bucket, *key = s3_path.split(\"/\")\n",
    "        key = \"/\".join(key)\n",
    "        \n",
    "        try:\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "\n",
    "        return local_file_path\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(_download_file, file, input_path) for file in files]\n",
    "        for future in as_completed(futures):\n",
    "            print (f\"Bloom:176:LLM:downloaded: {future.result()}\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n",
    "    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n",
    "    print(f\"Bloom:176:LLM:bucket={bucket}::key={key_prefix}\")\n",
    "    model_dir = \"/tmp/model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    s3_objects = s3_client.list_objects(Bucket=bucket, Prefix=key_prefix)[\"Contents\"]\n",
    "    s3_paths = [os.path.join(bucket, obj[\"Key\"]) for obj in s3_objects]\n",
    "    print(\"Bloom:176:LLM:downloading files\")\n",
    "    download_files(s3_paths, model_dir)\n",
    "    print(\"Bloom:176:LLM:downloading finished\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    print(\"Bloom:176:LLM:Tokenizer is initialized\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True, \n",
    "        max_memory={\n",
    "            0: \"0GIB\",\n",
    "            1: \"26GIB\",\n",
    "            2: \"26GIB\",\n",
    "            3: \"26GIB\",\n",
    "            4: \"26GIB\",\n",
    "            5: \"26GIB\",\n",
    "            6: \"26GIB\",\n",
    "            7: \"26GIB\",\n",
    "        } \n",
    "    )\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    print(\"Bloom:176:LLM:Loaded the model\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    print(\"Bloom:176:LLM:predict_fn request received\")\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    print(\"Bloom:176:LLM:Input text is \"+ text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    result_length = 50 \n",
    "    #output_sequences = model.generate(input_ids=encoded_input['input_ids'], **data)\n",
    "    output_sequences = model.generate( input_ids=encoded_input[\"input_ids\"], max_length=result_length)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "#Override out of the box input function\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"Bloom:176:LLM:Received the input \" + input_data)\n",
    "    print(\"Bloom:176:LLM:Content type \" + content_type)\n",
    "    if content_type == \"application/json\":\n",
    "        return json.loads(input_data)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "#Override out of the box output function\n",
    "def output_fn(prediction, accept):\n",
    "    print(\"Bloom:176:LLM:Returning the output \" + prediction)\n",
    "    print(\"Bloom:176:LLM:accept type \" + accept)\n",
    "    output = {\"outputs\": prediction}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7609472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bloom-176b/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-176b/code/requirements.txt\n",
    "transformers==4.22.0\n",
    "accelerate\n",
    "bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebab6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-176b && rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffd8e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/requirements.txt\n",
      "code/inference.py\n",
      "notebook.ipynb\n",
      "old_inf.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd bloom-176b  && tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\".ipynb_checkpoints\" -zcvf model.tar.gz *   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3871b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-622343165275/bloom-llm-test/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-176b && aws s3 cp model.tar.gz s3://sagemaker-us-east-1-622343165275/bloom-llm-test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0df6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8959276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom:LLM:starting df commands\n",
      "[['Filesystem', 'Size', 'Used', 'Avail', 'Use%', 'Mounted', 'on'], ['devtmpfs', '185G', '0', '185G', '0%', '/dev'], ['tmpfs', '185G', '152K', '185G', '1%', '/dev/shm'], ['tmpfs', '185G', '996K', '185G', '1%', '/run'], ['tmpfs', '185G', '0', '185G', '0%', '/sys/fs/cgroup'], ['/dev/nvme0n1p1', '140G', '93G', '48G', '67%', '/'], ['/dev/nvme5n1', '197G', '51G', '136G', '28%', '/home/ec2-user/SageMaker'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1000'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1002'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1001']]\n",
      "Bloom:LLM:ENDING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(\"Bloom:LLM:starting df commands\")\n",
    "    df_output_lines = [s.split() for s in os.popen(\"df -Ph\").read().splitlines()]\n",
    "    print(df_output_lines)\n",
    "    print(\"Bloom:LLM:ENDING\")\n",
    "except:\n",
    "    print(\"error in running df\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee413a11",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "model.tar.gz/\n",
    "|- pytorch_model.bin\n",
    "|- ....\n",
    "|- code/\n",
    "  |- inference.py\n",
    "  |- requirements.txt \n",
    "  \n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cafd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session=sagemaker.Session()\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6c11b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom-llm-test/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_model_path = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./bloom-176b/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom-llm-test\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "print(s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3676f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_model_path='s3://sagemaker-us-east-1-622343165275/bloom/models/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "986e120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "model_id='bloom-176b'\n",
    "endpoint_name = name_from_base(f\"{model_id}-bnb\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version='py38',\n",
    "    entry_point='inference.py',\n",
    "    #source_dir='code',\n",
    "    model_server_workers=1,\n",
    "    env={'MMS_LOG_LEVEL':'1', 'MMS_USE_FRAMEWORK':'PT', \n",
    "         'MODEL_S3_BUCKET':'sagemaker-us-east-1-622343165275', \n",
    "         'MODEL_S3_PREFIX' :'bloom-176B/model.tar.gz'\n",
    "        }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "38406086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloom-176b-bnb-2022-09-22-00-54-02-747'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0dc083b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ddd3c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bloom-176b-bnb-2022-09-21-21-20-29-024'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f2d1351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7f7a7e4c1f10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "predictor_bl = HuggingFacePredictor(\n",
    "    endpoint_name = 'bloom-176b-p4-patched-boto3-1-worker-2022-09-21-17-09-06',\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "predictor_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bae8b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 'Transformers with bnb-Int8 work best on CPUs with AVX-512 support. The bnb-Int8 format is a fixed-width integer format that is optimized for the Intel AVX-512 instruction set. It is a signed'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "predictor_bl.predict(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8dc7da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 'Transformers with bnb-Int8 work best on CPUs with AVX-512 support. The bnb-Int8 format is a fixed-width integer format that is optimized for the Intel AVX-512 instruction set. It is a signed'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "predictor_bl.predict(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44493992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac07b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a66d5",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69ebb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "#predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2aba27",
   "metadata": {},
   "source": [
    "## Deploy on p5 instances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faeb19a0",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "\n",
    "it expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "\n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge\n",
    "ml.g4dn.xlarge\n",
    "\n",
    "SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\n",
    "SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE. \n",
    "\n",
    "TRITON INFERENCE SERVE -- https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve/bloom-3b/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41b5fda6",
   "metadata": {},
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz -C triton-serve/bloom-3b\n",
    "\n",
    "expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    └── config.pbtxt    \n",
    "    ├── 1/\n",
    "    │   └── pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "cd triton-serve\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz bloom-3b\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bloom-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe509e",
   "metadata": {},
   "source": [
    "#### Load the model to convert to a .pt state for TRITON server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e52f5",
   "metadata": {},
   "source": [
    "#### This saves as PyTorch model we need torchscript model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH)  # --    collections.OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820c69e",
   "metadata": {},
   "source": [
    "#### This saves as a PyTorch SCRIPT mode based model which is what we need for the model to load in TritonServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5790660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76948b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH) # collections.OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['h.0.input_layernorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel, BloomConfig\n",
    "\n",
    "# Initializing a Bloom configuration\n",
    "configuration = BloomConfig(**model_bin)\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = BloomModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(type(configuration))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6758a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_tensor)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1130b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BloomModel, BloomConfig\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "text =  \"Transformers with bnb-Int8 work best on\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bloom-3b\") # - class type is BloomTokenizerFast\n",
    "encoded_input = tokenizer(text, return_tensors='pt').convert_to_tensors()\n",
    "print(type(encoded_input), encoded_input)\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "JSON_PATH=\"./bloom-3b/pytorch_model_json.json\"\n",
    "# write the model to json file\n",
    "with open(JSON_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_bin))\n",
    "    \n",
    "# - load the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(JSON_PATH, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model_8bit = model_8bit.eval()\n",
    "model_8bit.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(model_8bit, dummy_inputs)\n",
    "# Save the TorchScript model\n",
    "traced_model.save(\"./triton-serve/bloom-3b/1/model.pt\")\n",
    "\n",
    "print(\":PyTorch:TorchScript:Model:Saved {}\".format(traced_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module = torch.jit.trace(\n",
    "    model, \n",
    "    [tokens_tensor, segments_tensors] ,\n",
    "    strict=False)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"./triton-serve/bloom-3b/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf56113",
   "metadata": {},
   "source": [
    "## Standard Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063a461",
   "metadata": {},
   "source": [
    "**Single Model from EXACT s3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29097744",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216000\", #\"16777216\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08491c9",
   "metadata": {},
   "source": [
    "**SingleModel end point config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b99ab6",
   "metadata": {},
   "source": [
    "**Finally create the end point -- SINGLE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ec553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8235362",
   "metadata": {},
   "source": [
    "**Now Invoke the Single Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf18445",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed497c3a",
   "metadata": {},
   "source": [
    "**SINGLE Model Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d42612",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dffbf28",
   "metadata": {},
   "source": [
    "## START TRITON MME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8020c99",
   "metadata": {},
   "source": [
    "#### MME container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dcd77",
   "metadata": {},
   "source": [
    "**1. Create The MME container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_mme_model_path,\n",
    "    \"Mode\" : \"MultiModel\",\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ab29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74124c06",
   "metadata": {},
   "source": [
    "**2. Create The MME End Point CONFIG P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfbb18",
   "metadata": {},
   "source": [
    "**3. Create The MME ENDPoint P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e122ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b6262",
   "metadata": {},
   "source": [
    "#### Now invoke the MME end point"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46c7af5a",
   "metadata": {},
   "source": [
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b/model.tar.gz\n",
    "    \n",
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"bloom-3b/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"bloom-3b/model.tar.gz\",\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c2471",
   "metadata": {},
   "source": [
    "## Clean up MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9d50e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86396/885946240.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm_client' is not defined"
     ]
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400850d4",
   "metadata": {},
   "source": [
    "### Now predict on P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f597d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_p5.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b187d",
   "metadata": {},
   "source": [
    "## BOTO 3 way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
