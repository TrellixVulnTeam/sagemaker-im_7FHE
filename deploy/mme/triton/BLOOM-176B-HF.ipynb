{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fdb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip awscli boto3 sagemaker transformers==4.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c11fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3808d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23956c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/bigscience/bloom-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faab9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p bloom-176b/code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92b23cee",
   "metadata": {},
   "source": [
    "Large model loading\n",
    "\n",
    "In Transformers 4.20.0, the from_pretrained() method has been reworked to accommodate large models using Accelerate. This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0. \n",
    "Instead of creating the full model, then loading the pretrained weights inside it \n",
    "\n",
    "otherwise --\n",
    "Loading weights\n",
    "\n",
    "The second tool ðŸ¤— Accelerate introduces is a function load_checkpoint_and_dispatch(), that will allow you to load a checkpoint inside your empty model. This supports full checkpoints (a single file containing the whole state dict) as well as sharded checkpoints. It will also automatically dispatch those weights across the devices you have available (GPUs, CPU RAM), so if you are loadin\n",
    "\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"EleutherAI/gpt-j-6B\"\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, \"sharded-gpt-j-6B\", device_map=\"auto\", no_split_module_classes=[\"GPTJBlock\"]\n",
    ")\n",
    "\n",
    "\n",
    "for inference script in sagemaker extending a sagemaker container -- \n",
    "# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\n",
    "ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
    "\n",
    "# this environment variable is used by the SageMaker PyTorch container to determine our program entry point\n",
    "# for training and serving.\n",
    "# For more information: https://github.com/aws/sagemaker-pytorch-container\n",
    "ENV SAGEMAKER_PROGRAM cifar10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27436dc",
   "metadata": {},
   "source": [
    "#### Structure follows - https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06fa2ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bloom-176b/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-176b/code/inference.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "def download_files(files, model_dir):\n",
    "    \n",
    "    def _download_file(s3_path, model_dir):\n",
    "        \n",
    "        global s3_client\n",
    "        \n",
    "        local_file_path = os.path.join(model_dir, s3_path.split(\"/\")[-1])\n",
    "        \n",
    "        bucket, *key = s3_path.split(\"/\")\n",
    "        key = \"/\".join(key)\n",
    "        \n",
    "        try:\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            s3_client.download_file(bucket, key, local_file_path)\n",
    "\n",
    "        return local_file_path\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(_download_file, file, input_path) for file in files]\n",
    "        for future in as_completed(futures):\n",
    "            print (f\"Bloom:176:LLM:downloaded: {future.result()}\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    bucket = os.environ.get(\"MODEL_S3_BUCKET\")\n",
    "    key_prefix = os.environ.get(\"MODEL_S3_PREFIX\")\n",
    "    print(f\"Bloom:176:LLM:bucket={bucket}::key={key_prefix}\")\n",
    "    model_dir = \"/tmp/model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    s3_objects = s3_client.list_objects(Bucket=bucket, Prefix=key_prefix)[\"Contents\"]\n",
    "    s3_paths = [os.path.join(bucket, obj[\"Key\"]) for obj in s3_objects]\n",
    "    print(\"Bloom:176:LLM:downloading files\")\n",
    "    download_files(s3_paths, model_dir)\n",
    "    print(\"Bloom:176:LLM:downloading finished\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    print(\"Bloom:176:LLM:Tokenizer is initialized\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True, \n",
    "        max_memory={\n",
    "            0: \"0GIB\",\n",
    "            1: \"26GIB\",\n",
    "            2: \"26GIB\",\n",
    "            3: \"26GIB\",\n",
    "            4: \"26GIB\",\n",
    "            5: \"26GIB\",\n",
    "            6: \"26GIB\",\n",
    "            7: \"26GIB\",\n",
    "        } \n",
    "    )\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    print(\"Bloom:176:LLM:Loaded the model\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    print(\"Bloom:176:LLM:predict_fn request received\")\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    print(\"Bloom:176:LLM:Input text is \"+ text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    result_length = 50 \n",
    "    #output_sequences = model.generate(input_ids=encoded_input['input_ids'], **data)\n",
    "    output_sequences = model.generate( input_ids=encoded_input[\"input_ids\"], max_length=result_length)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "#Override out of the box input function\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"Bloom:176:LLM:Received the input \" + input_data)\n",
    "    print(\"Bloom:176:LLM:Content type \" + content_type)\n",
    "    if content_type == \"application/json\":\n",
    "        return json.loads(input_data)\n",
    "    return input_data\n",
    "\n",
    "\n",
    "#Override out of the box output function\n",
    "def output_fn(prediction, accept):\n",
    "    print(\"Bloom:176:LLM:Returning the output \" + prediction)\n",
    "    print(\"Bloom:176:LLM:accept type \" + accept)\n",
    "    output = {\"outputs\": prediction}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "188063e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bloom-176b/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile bloom-176b/code/requirements.txt\n",
    "transformers==4.22.0\n",
    "accelerate\n",
    "bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd184cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd bloom-176b && rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ea12ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/requirements.txt\n",
      "code/inference.py\n",
      "notebook.ipynb\n",
      "old_inf.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd bloom-176b  && tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\".ipynb_checkpoints\" -zcvf model.tar.gz *   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8887b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-622343165275/bloom-llm-test/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!cd bloom-176b && aws s3 cp model.tar.gz s3://sagemaker-us-east-1-622343165275/bloom-llm-test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce5b6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32a33fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom:LLM:starting df commands\n",
      "[['Filesystem', 'Size', 'Used', 'Avail', 'Use%', 'Mounted', 'on'], ['devtmpfs', '185G', '0', '185G', '0%', '/dev'], ['tmpfs', '185G', '152K', '185G', '1%', '/dev/shm'], ['tmpfs', '185G', '996K', '185G', '1%', '/run'], ['tmpfs', '185G', '0', '185G', '0%', '/sys/fs/cgroup'], ['/dev/nvme0n1p1', '140G', '93G', '48G', '67%', '/'], ['/dev/nvme5n1', '197G', '51G', '136G', '28%', '/home/ec2-user/SageMaker'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1000'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1002'], ['tmpfs', '37G', '0', '37G', '0%', '/run/user/1001']]\n",
      "Bloom:LLM:ENDING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(\"Bloom:LLM:starting df commands\")\n",
    "    df_output_lines = [s.split() for s in os.popen(\"df -Ph\").read().splitlines()]\n",
    "    print(df_output_lines)\n",
    "    print(\"Bloom:LLM:ENDING\")\n",
    "except:\n",
    "    print(\"error in running df\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3146a94",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "model.tar.gz/\n",
    "|- pytorch_model.bin\n",
    "|- ....\n",
    "|- code/\n",
    "  |- inference.py\n",
    "  |- requirements.txt \n",
    "  \n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c46135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session=sagemaker.Session()\n",
    "role=sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc5e1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-622343165275/bloom-llm-test/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_model_path = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./bloom-176b/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom-llm-test\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "print(s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b60d2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3_model_path='s3://sagemaker-us-east-1-622343165275/bloom/models/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a15017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "model_id='bloom-176b'\n",
    "endpoint_name = name_from_base(f\"{model_id}-bnb\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_path,\n",
    "    role=role,\n",
    "    transformers_version=\"4.17\",\n",
    "    pytorch_version=\"1.10\",\n",
    "    py_version='py38',\n",
    "    entry_point='inference.py',\n",
    "    #source_dir='code',\n",
    "    model_server_workers=1,\n",
    "    env={'MMS_LOG_LEVEL':'1', 'MMS_USE_FRAMEWORK':'PT', \n",
    "         'MODEL_S3_BUCKET':'sagemaker-us-east-1-622343165275', \n",
    "         'MODEL_S3_PREFIX' :'bloom-176B/model.tar.gz'\n",
    "        }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11d7180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloom-176b-bnb-2022-09-22-00-54-02-747'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fed66700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74196d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bloom-176b-bnb-2022-09-21-21-20-29-024'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3fd7c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7f7a7e4c1f10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "predictor_bl = HuggingFacePredictor(\n",
    "    endpoint_name = 'bloom-176b-p4-patched-boto3-1-worker-2022-09-21-17-09-06',\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "predictor_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "53dd307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 'Transformers with bnb-Int8 work best on CPUs with AVX-512 support. The bnb-Int8 format is a fixed-width integer format that is optimized for the Intel AVX-512 instruction set. It is a signed'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "predictor_bl.predict(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74a9110a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 'Transformers with bnb-Int8 work best on CPUs with AVX-512 support. The bnb-Int8 format is a fixed-width integer format that is optimized for the Intel AVX-512 instruction set. It is a signed'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "predictor_bl.predict(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b94563",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa008b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d1fd6",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3164606",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "#predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f352212",
   "metadata": {},
   "source": [
    "## Deploy on p5 instances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a63e51b6",
   "metadata": {},
   "source": [
    "### USING HuggingFaceInferenceToolKit -- https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules\n",
    "\n",
    "it expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    â””â”€â”€ config.pbtxt    \n",
    "    â”œâ”€â”€ 1/\n",
    "    â”‚   â””â”€â”€ pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "\n",
    "transform_fn(model, data, content_type, accept_type): Overrides the default transform function with custom implementation. Customers using this would have to implement preprocess, predict and postprocess steps in the transform_fn. NOTE: This method can't be combined with input_fn, predict_fn or output_fn mentioned below.\n",
    "\n",
    "ml.g5.48xlarge\n",
    "ml.g4dn.xlarge\n",
    "\n",
    "SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\n",
    "SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE. \n",
    "\n",
    "TRITON INFERENCE SERVE -- https://raw.githubusercontent.com/triton-inference-server/server/main/docker/sagemaker/serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile triton-serve/bloom-3b/config.pbtxt\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  },\n",
    "  {\n",
    "    name: \"INPUT__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [512]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [512, 768]\n",
    "  },\n",
    "  {\n",
    "    name: \"1634__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [768]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "   preferred_batch_size: 16\n",
    "   max_queue_delay_microseconds: 1000\n",
    " }\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17a08940",
   "metadata": {},
   "source": [
    "# tar --exclude=\".git\" --exclude=\".gitattributes\" -zcvf model.tar.gz *\n",
    "# tar -tf model.tar.gz\n",
    "# -- /home/ec2-user/SageMaker/bloom-3b\n",
    "\n",
    "# -- TRITON Is at triton_serve\n",
    "# - tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz -C triton-serve/bloom-3b\n",
    "\n",
    "expects -- unable to find '/opt/ml/model/bloom-3b/1/model.pt'\n",
    "model.tar.gz\n",
    "     |\n",
    "    bloom-3b/\n",
    "    |\n",
    "    â””â”€â”€ config.pbtxt    \n",
    "    â”œâ”€â”€ 1/\n",
    "    â”‚   â””â”€â”€ pytorch_model.bin\n",
    "    |--------|\n",
    "    |--------code/\n",
    "    |---------|\n",
    "    |---------| -- inference.py\n",
    "    |---------| -- requirements.txt\n",
    "\n",
    "cd triton-serve\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" -zcvf model.tar.gz bloom-3b\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bloom-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226aa16",
   "metadata": {},
   "source": [
    "#### Load the model to convert to a .pt state for TRITON server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f9a54",
   "metadata": {},
   "source": [
    "#### This saves as PyTorch model we need torchscript model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH)  # --    collections.OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca9356",
   "metadata": {},
   "source": [
    "#### This saves as a PyTorch SCRIPT mode based model which is what we need for the model to load in TritonServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "model_bin = torch.load(PATH) # collections.OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79516bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a144c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_bin['h.0.input_layernorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca266a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel, BloomConfig\n",
    "\n",
    "# Initializing a Bloom configuration\n",
    "configuration = BloomConfig(**model_bin)\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = BloomModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(type(configuration))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc594241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_tensor)\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BloomModel, BloomConfig\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "text =  \"Transformers with bnb-Int8 work best on\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bloom-3b\") # - class type is BloomTokenizerFast\n",
    "encoded_input = tokenizer(text, return_tensors='pt').convert_to_tensors()\n",
    "print(type(encoded_input), encoded_input)\n",
    "\n",
    "PATH=\"./bloom-3b/pytorch_model.bin\"\n",
    "JSON_PATH=\"./bloom-3b/pytorch_model_json.json\"\n",
    "# write the model to json file\n",
    "with open(JSON_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_bin))\n",
    "    \n",
    "# - load the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(JSON_PATH, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model_8bit = model_8bit.eval()\n",
    "model_8bit.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(model_8bit, dummy_inputs)\n",
    "# Save the TorchScript model\n",
    "traced_model.save(\"./triton-serve/bloom-3b/1/model.pt\")\n",
    "\n",
    "print(\":PyTorch:TorchScript:Model:Saved {}\".format(traced_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module = torch.jit.trace(\n",
    "    model, \n",
    "    [tokens_tensor, segments_tensors] ,\n",
    "    strict=False)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(\"./triton-serve/bloom-3b/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dea4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d52631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9745471",
   "metadata": {},
   "source": [
    "## Standard Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a000cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mme_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a2c30",
   "metadata": {},
   "source": [
    "**Single Model from EXACT s3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216000\", #\"16777216\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b26a3",
   "metadata": {},
   "source": [
    "**SingleModel end point config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8271cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad8456",
   "metadata": {},
   "source": [
    "**Finally create the end point -- SINGLE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810dc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381f083",
   "metadata": {},
   "source": [
    "**Now Invoke the Single Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed396af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ec631",
   "metadata": {},
   "source": [
    "**SINGLE Model Clean up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c05f1",
   "metadata": {},
   "source": [
    "## START TRITON MME "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13b7bc",
   "metadata": {},
   "source": [
    "#### MME container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670bb43",
   "metadata": {},
   "source": [
    "**1. Create The MME container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46409494",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-{model_id}-bnb\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_mme_model_path,\n",
    "    \"Mode\" : \"MultiModel\",\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bloom-3b',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc32ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c59020",
   "metadata": {},
   "source": [
    "**2. Create The MME End Point CONFIG P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca41c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee1332",
   "metadata": {},
   "source": [
    "**3. Create The MME ENDPoint P5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5976ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b92a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46742edf",
   "metadata": {},
   "source": [
    "#### Now invoke the MME end point"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83e2d32",
   "metadata": {},
   "source": [
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bloom-3b/model.tar.gz\n",
    "    \n",
    "s3://sagemaker-us-east-1-622343165275/bloom/triton_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af30006",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"bloom-3b/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39277ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"data\": text,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\":200,\n",
    "            #\"min_tokens\": 100,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            \"top_p\": 500,\n",
    "\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"text/csv\", \n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"bloom-3b/model.tar.gz\",\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aadd26",
   "metadata": {},
   "source": [
    "## Clean up MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5ac10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86396/885946240.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msm_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name_p5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm_client' is not defined"
     ]
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7e4cb",
   "metadata": {},
   "source": [
    "### Now predict on P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "data = {\n",
    "    \"inputs\": text,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#body = json.dumps(data)\n",
    "\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"the man worked as a carpenter.\", #\"Tramsformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = {\n",
    "    \"inputs\" : \"Transformers with bnb-Int8 work best on\",\n",
    "    \"do_sample\" : True,\n",
    "    \"temperature\" : 0.5\n",
    "}\n",
    "res = predictor_p5.predict(data=data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd945159",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_p5.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4e5e1",
   "metadata": {},
   "source": [
    "## BOTO 3 way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab525852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\":200,\n",
    "    #\"min_tokens\": 100,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"top_p\": 500,\n",
    "    }\n",
    "\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"text_inputs\": text,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "body = json.dumps(data)\n",
    "\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint( \n",
    "        EndpointName=endpoint_name, \n",
    "        Body = body, \n",
    "        ContentType = 'application/json'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
