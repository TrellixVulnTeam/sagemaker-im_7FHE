{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97688cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c72428",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f0f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "\n",
    "!pip install -qU pip awscli boto3 sagemaker transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970da8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -qU pip awscli boto3 sagemaker transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import boto3\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertConfig\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd04f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm_client = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3_client = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d5e39be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
      "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
      "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
      "       'review_headline', 'review_body', 'review_date'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3524: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 88: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>49033728</td>\n",
       "      <td>R1P1G5KZ05H6RD</td>\n",
       "      <td>6302503213</td>\n",
       "      <td>748506413</td>\n",
       "      <td>The Night They Saved Christmas [VHS]</td>\n",
       "      <td>Video</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Very satisfied!!</td>\n",
       "      <td>Fast shipping. Pleasure to deal with. Would re...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>17857748</td>\n",
       "      <td>R106N066IUN8ZV</td>\n",
       "      <td>B000059PET</td>\n",
       "      <td>478710180</td>\n",
       "      <td>Hamlet / Kline, New York Shakespeare Festival ...</td>\n",
       "      <td>Video</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>The most talented actor ever!</td>\n",
       "      <td>Kevin Kline is the most versatile, multi-talen...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>25551507</td>\n",
       "      <td>R7WTAA1S5O7D9</td>\n",
       "      <td>0788812807</td>\n",
       "      <td>981002815</td>\n",
       "      <td>Nascar Dual Powered Calculator (Solar &amp; Batter...</td>\n",
       "      <td>Video</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>great movie</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>21025041</td>\n",
       "      <td>R32HFMVWLYOYJK</td>\n",
       "      <td>6302509939</td>\n",
       "      <td>333219811</td>\n",
       "      <td>The Man From U.N.C.L.E, Volume 19: The Brain K...</td>\n",
       "      <td>Video</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>i love the martin landau episode</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>40943563</td>\n",
       "      <td>RWT3H6HBVAL6G</td>\n",
       "      <td>B00JENS2BI</td>\n",
       "      <td>538101194</td>\n",
       "      <td>Playboy Video Party Jokes [VHS]</td>\n",
       "      <td>Video</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>HOT women, dumb jokes</td>\n",
       "      <td>Y'know what this reminded me of? Those sketch ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     49033728  R1P1G5KZ05H6RD  6302503213       748506413   \n",
       "1          US     17857748  R106N066IUN8ZV  B000059PET       478710180   \n",
       "2          US     25551507   R7WTAA1S5O7D9  0788812807       981002815   \n",
       "3          US     21025041  R32HFMVWLYOYJK  6302509939       333219811   \n",
       "4          US     40943563   RWT3H6HBVAL6G  B00JENS2BI       538101194   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0               The Night They Saved Christmas [VHS]            Video   \n",
       "1  Hamlet / Kline, New York Shakespeare Festival ...            Video   \n",
       "2  Nascar Dual Powered Calculator (Solar & Batter...            Video   \n",
       "3  The Man From U.N.C.L.E, Volume 19: The Brain K...            Video   \n",
       "4                    Playboy Video Party Jokes [VHS]            Video   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              0            0    N                 Y   \n",
       "1            5              0            0    N                 Y   \n",
       "2            4              0            0    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            3              0            0    N                 N   \n",
       "\n",
       "                 review_headline  \\\n",
       "0               Very satisfied!!   \n",
       "1  The most talented actor ever!   \n",
       "2                     Four Stars   \n",
       "3                     Five Stars   \n",
       "4          HOT women, dumb jokes   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  Fast shipping. Pleasure to deal with. Would re...  2015-08-31  \n",
       "1  Kevin Kline is the most versatile, multi-talen...  2015-08-31  \n",
       "2                                        great movie  2015-08-31  \n",
       "3                   i love the martin landau episode  2015-08-31  \n",
       "4  Y'know what this reminded me of? Those sketch ...  2015-08-31  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='./data/raw/amazon_raw_1000.tsv',\n",
    "    delimiter=\"\\t\",\n",
    "    header=0,\n",
    "    error_bad_lines=False\n",
    ")\n",
    "print(df.columns)\n",
    "\n",
    "review_columns = df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90a4d49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marketplace          string\n",
       "customer_id           int64\n",
       "review_id            string\n",
       "product_id           string\n",
       "product_parent        int64\n",
       "product_title        string\n",
       "product_category     string\n",
       "star_rating           int64\n",
       "helpful_votes         int64\n",
       "total_votes           int64\n",
       "vine                 string\n",
       "verified_purchase    string\n",
       "review_headline      string\n",
       "review_body          string\n",
       "review_date          string\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == \"object\":\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "    return data_frame\n",
    "\n",
    "df = cast_object_to_string(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a2680bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "        #print(\"InputFeatures:created\")\n",
    "        \n",
    "def convert_input(tokenizer, the_input, max_seq_length, run_print=False):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    #\n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "\n",
    "    tokens = tokenizer.tokenize(the_input.text)\n",
    "    tokens.insert(0, '[CLS]')\n",
    "    tokens.append('[SEP]')\n",
    "    if run_print:\n",
    "        print(\"**{} tokens**\\n{}\\n\".format(len(tokens), tokens))\n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(\n",
    "        the_input.text,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "    input_ids = encode_plus_tokens[\"input_ids\"]\n",
    "\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\n",
    "    input_mask = encode_plus_tokens[\"attention_mask\"]\n",
    "\n",
    "    # Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for each training row (`star_rating` 1 through 5)\n",
    "    label_id = label_map[the_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        review_id=the_input.review_id,\n",
    "        date=the_input.date,\n",
    "        label=the_input.label,\n",
    "    )\n",
    "    if run_print:\n",
    "        print(\"**input_ids**\\n{}\\n\".format(features.input_ids))\n",
    "        print(\"**input_mask**\\n{}\\n\".format(features.input_mask))\n",
    "        print(\"**segment_ids**\\n{}\\n\".format(features.segment_ids))\n",
    "        print(\"**label_id**\\n{}\\n\".format(features.label_id))\n",
    "        print(\"**review_id**\\n{}\\n\".format(features.review_id))\n",
    "        print(\"**date**\\n{}\\n\".format(features.date))\n",
    "        print(\"**label**\\n{}\\n\".format(features.label))\n",
    "\n",
    "    return features\n",
    "\n",
    "# We'll need to transform our data into a format that BERT understands.\n",
    "# - `text` is the text we want to classify, which in this case, is the `Request` field in our Dataframe.\n",
    "# - `label` is the star_rating label (1, 2, 3, 4, 5) for our training input data\n",
    "# -- [LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=x[DATE_COLUMN]\n",
    "def transform_inputs_to_tfrecord(tokenizer,inputs, output_file, max_seq_length):\n",
    "    records = []\n",
    "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, the_input) in enumerate(inputs):\n",
    "        if input_idx % 10000 == 0:\n",
    "            print(\"Writing input {} of {}\\n\".format(input_idx, len(inputs)))\n",
    "\n",
    "        features = convert_input(tokenizer,the_input, max_seq_length)\n",
    "\n",
    "        all_features = collections.OrderedDict()\n",
    "\n",
    "        # Create TFRecord With input_ids, input_mask, segment_ids, and label_ids\n",
    "        all_features[\"input_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
    "        all_features[\"input_mask\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
    "        all_features[\"segment_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
    "        all_features[\"label_ids\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
    "\n",
    "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
    "        tf_record_writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Create Record For Feature Store With All Features\n",
    "        records.append(\n",
    "            {  #'tf_record': tf_record.SerializeToString(),\n",
    "                \"input_ids\": features.input_ids,\n",
    "                \"input_mask\": features.input_mask,\n",
    "                \"segment_ids\": features.segment_ids,\n",
    "                \"label_id\": features.label_id,\n",
    "                \"review_id\": the_input.review_id,\n",
    "                \"date\": the_input.date,\n",
    "                \"label\": features.label,\n",
    "                #                        'review_body': features.review_body\n",
    "            }\n",
    "        )\n",
    "\n",
    "    tf_record_writer.close()\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "775100ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['marketplace', 'customer_id', 'review_id', 'product_id',\n",
       "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
       "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
       "       'review_headline', 'review_body', 'review_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c7bba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role arn:aws:iam::622343165275:role/service-role/AmazonSageMaker-ExecutionRole-20220208T115633\n",
      "The DEFAULT BUCKET is sagemaker-us-east-1-622343165275\n"
     ]
    }
   ],
   "source": [
    "print(\"role {}\".format(role))\n",
    "\n",
    "print(\"The DEFAULT BUCKET is {}\".format(bucket))\n",
    "#############################\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "REVIEW_BODY_COLUMN = \"review_body\"\n",
    "REVIEW_ID_COLUMN = \"review_id\"\n",
    "DATE_COLUMN = 'review_date'\n",
    "\n",
    "LABEL_COLUMN = \"star_rating\"\n",
    "LABEL_VALUES = [1, 2, 3, 4, 5]\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "\n",
    "class Input(object):\n",
    "    \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, text, review_id, date, label=None):\n",
    "        \"\"\"Constructs an Input.\n",
    "        Args:\n",
    "          text: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.review_id = review_id\n",
    "        self.date = date\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def _transform_tsv_to_tfrecord(tokenizer, df, file, max_seq_length):\n",
    "    print(\"file {}\".format(file))\n",
    "    print(\"max_seq_length {}\".format(max_seq_length))\n",
    "\n",
    "    #import os.path\n",
    "    filename_without_extension = \"./data/processed/temp.csv\"\n",
    "    train_split_percentage = 0.8\n",
    "\n",
    "    print(\"Shape of dataframe {}\".format(df.shape))\n",
    "\n",
    "\n",
    "    print(\"train split percentage {}\".format(train_split_percentage))\n",
    "\n",
    "    holdout_percentage = 1.00 - train_split_percentage\n",
    "    print(\"holdout percentage {}\".format(holdout_percentage))\n",
    "    \n",
    "    df_train, df_holdout = train_test_split(df, test_size=holdout_percentage, stratify=df[\"star_rating\"])\n",
    "\n",
    "    test_holdout_percentage = 0.4 #train_split_percentage / holdout_percentage\n",
    "    \n",
    "    print(\"test holdout percentage {}\".format(test_holdout_percentage))\n",
    "    \n",
    "    df_validation, df_test = train_test_split(\n",
    "        df_holdout, test_size=test_holdout_percentage, stratify=df_holdout[\"star_rating\"])\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print(\"Shape of train dataframe {}\".format(df_train.shape))\n",
    "    print(\"Shape of validation dataframe {}\".format(df_validation.shape))\n",
    "    print(\"Shape of test dataframe {}\".format(df_test.shape))\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    print(timestamp)\n",
    "\n",
    "    train_inputs = df_train.apply(\n",
    "        lambda x: Input(\n",
    "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=x[DATE_COLUMN]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    validation_inputs = df_validation.apply(\n",
    "        lambda x: Input(\n",
    "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=x[DATE_COLUMN]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    test_inputs = df_test.apply(\n",
    "        lambda x: Input(\n",
    "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    #\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    #\n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    #\n",
    "    train_data = \"./data/processed/train\"\n",
    "    validation_data = \"./data/processed/val\"\n",
    "    test_data = \"./data/processed/test\"\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    train_records = transform_inputs_to_tfrecord(\n",
    "        tokenizer,\n",
    "        train_inputs,\n",
    "        \"{}.tfrecord\".format('./data/processed/train'),\n",
    "        max_seq_length,\n",
    "    )\n",
    "\n",
    "    validation_records = transform_inputs_to_tfrecord(\n",
    "        tokenizer,\n",
    "        validation_inputs,\n",
    "        \"{}.tfrecord\".format('./data/processed/test'),\n",
    "        max_seq_length,\n",
    "    )\n",
    "\n",
    "    test_records = transform_inputs_to_tfrecord(\n",
    "         tokenizer,\n",
    "        test_inputs,\n",
    "        \"{}.tfrecord\".format('./data/processed/val'),\n",
    "        max_seq_length,\n",
    "    )\n",
    "   \n",
    "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
    "    df_train_records[\"split_type\"] = \"train\"\n",
    "    df_train_records.to_csv('./data/processed/train.csv')\n",
    "    \n",
    "    #df_train_records.\n",
    "\n",
    "    #df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
    "    #df_validation_records[\"split_type\"] = \"validation\"\n",
    "    #df_validation_records.head()\n",
    "\n",
    "    #df_test_records = pd.DataFrame.from_dict(test_records)\n",
    "    #df_test_records[\"split_type\"] = \"test\"\n",
    "    #df_test_records.head()\n",
    "\n",
    "    print('...features ingested!')\n",
    "\n",
    "    return df_train_records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12bf55",
   "metadata": {},
   "source": [
    "**Test how to create DistillBERT torchscript model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d47cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:Tokenized:Text:Tokens=['i', 'needed', 'an', 'anti', '##virus', 'application', 'and', 'know', 'the', 'quality', 'of', 'norton', 'products', '.', 'this', 'was', 'a', 'no', 'brain', '##er', 'for', 'me', 'and', 'i', 'am', 'glad', 'it', 'was', 'so', 'simple', 'to', 'get', '.']:::\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenizing input text\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "text = \"i needed an antivirus application and know the quality of Norton products. This was a no brainer for me and i am glad it was so simple to get.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"BERT:Tokenized:Text:Tokens={tokens}:::\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3269996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2734, 2019, 3424, 23350, 4646, 1998, 2113, 1996, 3737, 1997, 10770, 3688, 1012, 2023, 2001, 1037, 2053, 4167, 2121, 2005, 2033, 1998, 1045, 2572, 5580, 2009, 2001, 2061, 3722, 2000, 2131, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH=64\n",
    "encode_plus_tokens = tokenizer.encode_plus(\n",
    "    text,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "encode_plus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4863f7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./data/processed/temp.csv\n",
      "max_seq_length 64\n",
      "Shape of dataframe (988, 15)\n",
      "train split percentage 0.8\n",
      "holdout percentage 0.19999999999999996\n",
      "test holdout percentage 0.4\n",
      "Shape of train dataframe (790, 15)\n",
      "Shape of validation dataframe (118, 15)\n",
      "Shape of test dataframe (80, 15)\n",
      "2022-09-12T17:32:30Z\n",
      "Writing input 0 of 790\n",
      "\n",
      "Writing input 0 of 118\n",
      "\n",
      "Writing input 0 of 80\n",
      "\n",
      "...features ingested!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>split_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1037, 4438, 1012, 102, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>RYLTA7ELLXO88</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 13528, 4679, 1012, 1012, 1012, 2442, 202...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R1QUEJOPDJ9SBK</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 7929, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>R3O6A13I2HL03H</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 2293, 2009, 1012, 2234, 2428, 4248, 2036...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R3C9JMAIMPW5XL</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 2508, 6187, 10177, 2100, 2012, 2010, 219...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R9UL21HHCL77M</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>[101, 2293, 2009, 1010, 2009, 1005, 1055, 3599...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R3TIA53ST8WDA6</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>[101, 1060, 11636, 11636, 2080, 102, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R2ZIP58WM1FXF0</td>\n",
       "      <td>2015-08-30</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>[101, 1045, 2293, 2023, 3185, 999, 102, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R3OC9ISRIT46HR</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>[101, 14229, 24084, 1011, 1011, 2054, 1037, 24...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R7XAB8F457N4O</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>[101, 3435, 7829, 1012, 5165, 2000, 3066, 2007...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>R1P1G5KZ05H6RD</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>790 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [101, 1037, 4438, 1012, 102, 0, 0, 0, 0, 0, 0,...   \n",
       "1    [101, 13528, 4679, 1012, 1012, 1012, 2442, 202...   \n",
       "2    [101, 7929, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3    [101, 2293, 2009, 1012, 2234, 2428, 4248, 2036...   \n",
       "4    [101, 2508, 6187, 10177, 2100, 2012, 2010, 219...   \n",
       "..                                                 ...   \n",
       "785  [101, 2293, 2009, 1010, 2009, 1005, 1055, 3599...   \n",
       "786  [101, 1060, 11636, 11636, 2080, 102, 0, 0, 0, ...   \n",
       "787  [101, 1045, 2293, 2023, 3185, 999, 102, 0, 0, ...   \n",
       "788  [101, 14229, 24084, 1011, 1011, 2054, 1037, 24...   \n",
       "789  [101, 3435, 7829, 1012, 5165, 2000, 3066, 2007...   \n",
       "\n",
       "                                            input_mask  \\\n",
       "0    [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "..                                                 ...   \n",
       "785  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "786  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "787  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "788  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "789  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           segment_ids  label_id  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "..                                                 ...       ...   \n",
       "785  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "786  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "787  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "788  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "789  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         4   \n",
       "\n",
       "          review_id        date  label split_type  \n",
       "0     RYLTA7ELLXO88  2015-08-03      5      train  \n",
       "1    R1QUEJOPDJ9SBK  2015-08-27      5      train  \n",
       "2    R3O6A13I2HL03H  2015-08-30      3      train  \n",
       "3    R3C9JMAIMPW5XL  2015-08-04      5      train  \n",
       "4     R9UL21HHCL77M  2015-08-15      5      train  \n",
       "..              ...         ...    ...        ...  \n",
       "785  R3TIA53ST8WDA6  2015-08-30      5      train  \n",
       "786  R2ZIP58WM1FXF0  2015-08-30      5      train  \n",
       "787  R3OC9ISRIT46HR  2015-08-04      5      train  \n",
       "788   R7XAB8F457N4O  2015-08-27      5      train  \n",
       "789  R1P1G5KZ05H6RD  2015-08-31      5      train  \n",
       "\n",
       "[790 rows x 8 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file = \"./data/processed/temp.csv\"\n",
    "max_seq_length=64\n",
    "_transform_tsv_to_tfrecord(tokenizer, df, file, max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de1a3d",
   "metadata": {},
   "source": [
    "**Ingest the tf records into S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_train_path_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/processed/train.tfrecord\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews/processed/train\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "s3_data_test_path_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/processed/test.tfrecord\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews/processed/test\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "s3_data_val_path_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/processed/val.tfrecord\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews/processed/val\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "s3_data_path_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/raw/amazon_reviews_us_Video_v1_00.tsv\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "s3_data_path_1000_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/raw/amazon_raw_1000.tsv\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(s3_data_path_reviews)\n",
    "print(s3_data_path_1000_reviews)\n",
    "print(s3_data_train_path_reviews)\n",
    "print(s3_data_test_path_reviews)\n",
    "print(s3_data_val_path_reviews)\n",
    "\n",
    "\n",
    "processed_train_data_s3_uri = s3_data_train_path_reviews\n",
    "processed_validation_data_s3_uri = s3_data_val_path_reviews\n",
    "processed_test_data_s3_uri = s3_data_test_path_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12fad38",
   "metadata": {},
   "source": [
    "**Ingest Public aws movies review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc97f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_public_path_tsv=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_v1_00.tsv.gz\"\n",
    "s3_public_path_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27141370",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path_reviews = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./data/raw/amazon_reviews_us_Video_v1_00.tsv\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/data/amazon_reviews\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "s3_data_path_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input_data_s3_uri = s3_data_path_reviews\n",
    "raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d319cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize bert-gptj/preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe57ddb",
   "metadata": {},
   "source": [
    "**Test Tokenizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri\n",
    "!aws s3 ls $processed_validation_data_s3_uri\n",
    "!aws s3 ls $processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_train_data_s3_uri)\n",
    "!aws s3 ls $processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5319569",
   "metadata": {},
   "source": [
    "### Train with BERT and TF Record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7255fc9",
   "metadata": {},
   "source": [
    "**Note**: Amazon SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`. Below is the sample model directory structure\n",
    "\n",
    "```\n",
    "bert-uc\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "769d2f2a",
   "metadata": {},
   "source": [
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz bert-uc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d2068",
   "metadata": {},
   "source": [
    "#### Train using the TF Records\n",
    "**Have to use the same Tokenizer to generate the input to test as BERT uncased**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        \"input_ids\": record[\"input_ids\"],\n",
    "        \"input_mask\": record[\"input_mask\"],\n",
    "        #        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record[\"label_ids\"]\n",
    "\n",
    "    return (x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_dataset_builder(channel, input_filenames, pipe_mode, is_training, drop_remainder):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print(\"***** Using pipe_mode with channel {}\".format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "\n",
    "        dataset = PipeModeDataset(channel=channel, record_format=\"TFRecord\")\n",
    "    else:\n",
    "        print(\"***** Using input_filenames {}\".format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(100)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        #      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "        \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        return tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=8,\n",
    "            drop_remainder=drop_remainder,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(seed=42, buffer_size=10, reshuffle_each_iteration=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dda964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Train data set\n",
    "train_data = \"./data/processed/train.tfrecord\"\n",
    "test_data = \"./data/processed/test.tfrecord\"\n",
    "val_data = \"./data/processed/val.tfrecord\"\n",
    "max_seq_length=64\n",
    "print(\"train_data_filenames {}\".format(train_data))\n",
    "\n",
    "train_dataset = file_based_input_dataset_builder(\n",
    "    channel=\"train\", \n",
    "    input_filenames=train_data, \n",
    "    pipe_mode=False, \n",
    "    is_training=True, \n",
    "    drop_remainder=False\n",
    ").map(select_data_and_label_from_record)\n",
    "\n",
    "# -- Validation data set\n",
    "validation_dataset = file_based_input_dataset_builder(\n",
    "    channel=\"validation\",\n",
    "    input_filenames=val_data,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    ").map(select_data_and_label_from_record)\n",
    "\n",
    "# -- Test data set\n",
    "# -- Validation data set\n",
    "test_dataset = file_based_input_dataset_builder(\n",
    "    channel=\"validation\",\n",
    "    input_filenames=test_data,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    ").map(select_data_and_label_from_record)\n",
    "\n",
    "print(\"loaded Train data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac24e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "steps_per_epoch = 10\n",
    "validation_steps = 10\n",
    "test_steps = 10\n",
    "freeze_bert_layer = True\n",
    "learning_rate = 3e-5\n",
    "epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32643",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [1, 2, 3, 4, 5]\n",
    "\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(CLASSES),\n",
    "    id2label={0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
    "    label2id={1: 0, 2: 1, 3: 2, 4: 3, 5: 4},\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b74bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    config=config\n",
    ")\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\", dtype=\"int32\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_mask\", dtype=\"int32\")\n",
    "\n",
    "embedding_layer = transformer_model.distilbert(input_ids, attention_mask=input_mask)[0]\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be21ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)\n",
    "    )(embedding_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eaa76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(50, activation=\"relu\")(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(len(CLASSES), activation=\"softmax\")(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=X)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable = not freeze_bert_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b33db72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " distilbert (TFDistilBertMainLa  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " yer)                           ast_hidden_state=(N               'input_mask[0][0]']             \n",
      "                                one, 64, 768),                                                    \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 64, 100)     327600      ['distilbert[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 100)         0           ['bidirectional_2[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           5050        ['global_max_pooling1d[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_60 (Dropout)           (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 5)            255         ['dropout_60[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,695,785\n",
      "Trainable params: 332,905\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18aa9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80c3f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./temp-bloom/tensorboard/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c74a0f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 13s 571ms/step - loss: 1.5028 - accuracy: 0.4250 - val_loss: 1.4611 - val_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed4c5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model <keras.engine.functional.Functional object at 0x7f667c22d580>\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained model {}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60ea7961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 164ms/step - loss: 1.4731 - accuracy: 0.4875\n",
      "[1.4730923175811768, 0.48750001192092896]\n"
     ]
    }
   ],
   "source": [
    "test_history = model.evaluate(test_dataset, steps=test_steps, callbacks=callbacks)\n",
    "print(test_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1266b",
   "metadata": {},
   "source": [
    "## Predict with the Model\n",
    "use the data set to predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "sample_review_body = \"This product is terrible.\"\n",
    "\n",
    "encode_plus_tokens = tokenizer.encode_plus(\n",
    "    sample_review_body, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\n",
    "input_ids = encode_plus_tokens[\"input_ids\"]\n",
    "\n",
    "# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\n",
    "input_mask = encode_plus_tokens[\"attention_mask\"]\n",
    "\n",
    "outputs = model.predict(x=(input_ids, input_mask))\n",
    "\n",
    "prediction = [{\"label\": config.id2label[item.argmax()], \"score\": item.max().item()} for item in outputs]\n",
    "\n",
    "print(\"\")\n",
    "print('Predicted star_rating \"{}\" for review_body \"{}\"'.format(prediction[0][\"label\"], sample_review_body))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22b88c",
   "metadata": {},
   "source": [
    "### Create the BERT Model in Torch Script mode -- .pt model\n",
    "use the ore trained and use torchscript flag here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "# If you are instantiating the model with `from_pretrained` you can also easily set the TorchScript flag\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n",
    "\n",
    "# Change to eva lmodel\n",
    "model.eval()\n",
    "\n",
    "# run a dummy prediction of tokens by tensors\n",
    "output = model(tokens_tensor)\n",
    "print(len(output), type(output), type(output[0]))\n",
    "\n",
    "# Creating the trace\n",
    "traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n",
    "torch.jit.save(traced_model, \"./triton-serve/bert-uc/1/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75581d4c",
   "metadata": {},
   "source": [
    "### Create the BERT Model in Torch Script using dummy inputs -- .pt model\n",
    "Create using the dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-large-uncased\", torchscript=True)\n",
    "\n",
    "bs = 1\n",
    "seq_len = 512\n",
    "dummy_inputs = [\n",
    "    torch.randint(1000, (bs, seq_len)).to(device),\n",
    "    torch.zeros(bs, seq_len, dtype=torch.int).to(device),\n",
    "]\n",
    "model = model.eval()\n",
    "model.to(device)\n",
    "\n",
    "traced_model = torch.jit.trace(model, dummy_inputs)\n",
    "torch.jit.save(traced_model, \"./triton-serve/bert-uc/1/model.pt\")\n",
    "\n",
    "print(\"Saved {}\".format(traced_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584d960",
   "metadata": {},
   "source": [
    "**Predict test using the traced model Needs Tokens and Attention mask both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6064c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = traced_model(input_ids=tokens_tensor, attention_mask=segments_tensors)\n",
    "print(len(output), type(output), type(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ccf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f82a2",
   "metadata": {},
   "source": [
    "### UPLOAD of the Model.tar after it has been created correctly by \n",
    "\n",
    "Because we share the same model tar with bloom and with bert-uc\n",
    "rm model.tar.gz in the triton-serve directory\n",
    "\n",
    "tar --exclude=\".git\" --exclude=\".gitattributes\" --exclude=\"model.tar.gz\" --exclude=\"*.bin\" -zcvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d90bcf",
   "metadata": {},
   "source": [
    "**Upload the model.tar.gz to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7779e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=\"./triton-serve/model.tar.gz\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_mme_model_path='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/'\n",
    "print(s3_model_path_triton)\n",
    "print(s3_mme_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed2cfa",
   "metadata": {},
   "source": [
    "#### Start Single Model Triton for starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b5daf",
   "metadata": {},
   "source": [
    "**Triton Image download and sagemaker variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02abaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "import boto3\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.07-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")\n",
    "print(triton_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507f4be",
   "metadata": {},
   "source": [
    "**Model creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5 = name_from_base(f\"p5-bert-uc-\")\n",
    "print(endpoint_name_p5)\n",
    "\n",
    "container_p5 = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_triton,\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'bert-uc',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5, ExecutionRoleArn=role, PrimaryContainer=container_p5\n",
    ")\n",
    "print(create_model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0164e9f",
   "metadata": {},
   "source": [
    "**Endpoint config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eff61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g5.8xlarge\", #\"ml.g4dn.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af264c7",
   "metadata": {},
   "source": [
    "**Endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5, EndpointConfigName=endpoint_name_p5\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f740ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"SINGLE:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Single:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Single:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bb0ea",
   "metadata": {},
   "source": [
    "**Now Invoke The endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fa8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_text(text, enc, max_length=512):\n",
    "    #enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(f\"Tokenize:text:why??::max_length={max_length}::Tokenizer={enc}\")\n",
    "    encoded_text = enc(text, padding=\"max_length\", max_length=max_length)\n",
    "    return encoded_text[\"input_ids\"], encoded_text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "def _get_sample_tokenized_text_binary(text, input_names, output_names, enc, max_length=512):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    inputs.append(httpclient.InferInput(input_names[0], [1, max_length], \"INT32\"))\n",
    "    inputs.append(httpclient.InferInput(input_names[1], [1, max_length], \"INT32\"))\n",
    "    indexed_tokens, attention_mask = tokenize_text(text,enc)\n",
    "\n",
    "    indexed_tokens = np.array(indexed_tokens, dtype=np.int32)\n",
    "    indexed_tokens = np.expand_dims(indexed_tokens, axis=0)\n",
    "    inputs[0].set_data_from_numpy(indexed_tokens, binary_data=True)\n",
    "\n",
    "    attention_mask = np.array(attention_mask, dtype=np.int32)\n",
    "    attention_mask = np.expand_dims(attention_mask, axis=0)\n",
    "    inputs[1].set_data_from_numpy(attention_mask, binary_data=True)\n",
    "\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[0], binary_data=True))\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_names[1], binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_pt(text, enc, max_length=512):\n",
    "    return _get_sample_tokenized_text_binary(\n",
    "        text, [\"INPUT__0\", \"INPUT__1\"], [\"OUTPUT__0\", \"1634__1\"], enc, max_length\n",
    "    )\n",
    "\n",
    "\n",
    "def get_sample_tokenized_text_binary_trt(text, enc):\n",
    "    return _get_sample_tokenized_text_binary(text, [\"token_ids\", \"attn_mask\"], [\"output\", \"1634\"], enc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "max_seq_length=512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0eed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76788d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "max_seq_length=512\n",
    "text_triton = \"\"\"This is a creative writing exercise. Below, you'll be given a prompt. Your story should be based on the prompt.\n",
    "\n",
    "Prompt: A scary story about a haunted mouse\n",
    "Story: On a dark and stormy night, the mouse crept in the shadows. \"\"\"\n",
    "\n",
    "\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "# -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "output_dict.keys()\n",
    "\n",
    "#enc.decode(output_dict['outputs'][0]['data'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids\n",
    "attention_mask \n",
    "\n",
    "# open file in write mode\n",
    "with open(r'./temp-bloom/input_ids.txt', 'w') as fp:\n",
    "    for item in input_ids:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done input_ids')\n",
    "    \n",
    "# open file in write mode\n",
    "with open(r'./temp-bloom/attention_mask.txt', 'w') as fp:\n",
    "    for item in attention_mask:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done attention_mask')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ad0ec",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# triton\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# custom CloudWatch\n",
    "#from cloudwatch import get_endpoint_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it  -v `pwd`/workspace:/workspace nvcr.io/nvidia/pytorch:21.08-py3 /bin/bash generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb7b4c",
   "metadata": {},
   "source": [
    "## START MME for triton "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91572dcc",
   "metadata": {},
   "source": [
    "**Upload first**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a2c8e",
   "metadata": {},
   "source": [
    "### Upload multiple copies for MME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1,100):\n",
    "    s3_model_path_triton_mme = sagemaker.s3.S3Uploader().upload(\n",
    "        local_path=\"./triton-serve/model.tar.gz\",\n",
    "        desired_s3_uri=f\"s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc/model-{ii}\",\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "s3_model_path_mme='s3://sagemaker-us-east-1-622343165275/bloom/triton_models/bert-uc'\n",
    "print(\"MULTIPLE:Uplodas:\")\n",
    "print(s3_model_path_triton_mme)\n",
    "print(s3_model_path_mme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f62d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_mme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a4708",
   "metadata": {},
   "source": [
    "**Create the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5_mme = name_from_base(f\"p5-bert-uc-mme\")\n",
    "print(endpoint_name_p5_mme)\n",
    "\n",
    "container_p5_mme = {\n",
    "    'Image': triton_image_uri,\n",
    "    'ModelDataUrl': s3_model_path_mme,\n",
    "    'Mode':'MultiModel',\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'model-1',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    }\n",
    "}\n",
    "create_model_response_mme = sm_client.create_model(\n",
    "    ModelName=endpoint_name_p5_mme, ExecutionRoleArn=role, PrimaryContainer=container_p5_mme\n",
    ")\n",
    "print(create_model_response_mme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0caa0",
   "metadata": {},
   "source": [
    "**Create the Endpoint config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_config_response_mme = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name_p5_mme,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\", #\"ml.g4dn.xlarge\",ml.g5.8xlarge\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": endpoint_name_p5_mme,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response_mme[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a72bb",
   "metadata": {},
   "source": [
    "**Create the endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response_mme = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, EndpointConfigName=endpoint_name_p5_mme\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response_mme[\"EndpointArn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be52ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"MME:Model:endpoint:Triton:Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"MME:model:triton:Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"MME:model:triton:Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f4bea",
   "metadata": {},
   "source": [
    "**Test the end point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "max_seq_length=512\n",
    "text_triton = \"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"\n",
    "print(f\"Leverage the Tokenizer={enc}::max_seq_length={max_seq_length}:: create above when creating the model \")\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, ContentType=\"application/octet-stream\", Body=json.dumps(payload), TargetModel  = \"/model-9/model.tar.gz\"\n",
    ")\n",
    "\n",
    "output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "# -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "output_dict.keys()\n",
    "\n",
    "enc.decode(output_dict['outputs'][0]['data'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_p5_mme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ed0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name_p5_mme, ContentType=\"text/json\", Body=json.dumps(payload), TargetModel  = \"/model-9/model.tar.gz\"\n",
    ")\n",
    "output_dict = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "\n",
    "# -- output_dict['outputs'][0]['data']  -- has 0 and 1 as 2 indexes in list \n",
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50107f",
   "metadata": {},
   "source": [
    "**set up in S3 payload to be used for inference load testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=512\n",
    "text_triton = \"\"\"\n",
    "                Create payload JSON and upload it on S3. \n",
    "                This will be used by Inference Recommender to run the load test.\n",
    "              \"\"\"\n",
    "\n",
    "input_ids, attention_mask = tokenize_text(text_triton, enc, max_length=max_seq_length)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\"name\": \"INPUT__0\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": input_ids},\n",
    "        {\"name\": \"INPUT__1\", \"shape\": [1, max_seq_length], \"datatype\": \"INT32\", \"data\": attention_mask},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Sample payload to be used with Inference Recommender\")\n",
    "print(payload)\n",
    "\n",
    "payload_location = \"./sample-payload/\"\n",
    "!mkdir -p $payload_location\n",
    "\n",
    "payload_archive_name = \"payload.tar.gz\"\n",
    "\n",
    "with open(payload_location + \"request.json\", \"w\") as f:\n",
    "    json.dump(payload, f)\n",
    "\n",
    "\n",
    "!cd ./sample-payload/ && tar czvf ../payload.tar.gz *\n",
    "\n",
    "print(f\"payload.tar.gz created at {payload_location}/{payload_archive_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a11f1",
   "metadata": {},
   "source": [
    "**Upload sample payload to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ccf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sample_data_path_triton = sagemaker.s3.S3Uploader().upload(\n",
    "    local_path=f\"{payload_archive_name}\",\n",
    "    desired_s3_uri=\"s3://sagemaker-us-east-1-622343165275/bloom/triton_test_data\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "s3_sample_data_path_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac39da",
   "metadata": {},
   "source": [
    "## Inference Load test set up\n",
    "### DOES NOT WORK FOR MME -- SO SKIP this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_domain = \"NATURAL_LANGUAGE_PROCESSING\"\n",
    "ml_task = \"FILL_MASK\"\n",
    "ml_framework = \"PYTORCH\"\n",
    "framework_version = \"1.6.0\"\n",
    "model_tested = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f827043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "sm_model_name = \"pt-triton-benchmark-model-\" + ts\n",
    "model_package_group_name = \"pt-triton-benchmark-model-group-\" + ts\n",
    "advanced_job = \"pt-triton-benchmark-advanced-job-\" + ts\n",
    "\n",
    "print(f\"SageMaker Model Name: {sm_model_name}\")\n",
    "print(f\"SageMaker Mode Package Name: {model_package_group_name}\")\n",
    "print(f\"SageMaker Advanced Job Name: {advanced_job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_path_mme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_infrec_mme = {\n",
    "    'Image': triton_image_uri,\n",
    "    \"NearestModelName\": model_tested, #'model-1',\n",
    "    \"Framework\": ml_framework,\n",
    "    'ModelDataUrl': s3_model_path_mme,\n",
    "    #'Mode':'MultiModel',\n",
    "    'Environment': {\n",
    "        #'SAGEMAKER_PROGRAM' : 'inference.py',\n",
    "        #'SAGEMAKER_SUBMIT_DIRECTORY' : 'code',\n",
    "        'SAGEMAKER_TRITON_DEFAULT_MODEL_NAME': 'model-1',\n",
    "        \"SAGEMAKER_TRITON_BATCH_SIZE\": \"16\",\n",
    "        \"SAGEMAKER_TRITON_MAX_BATCH_DELAY\": \"1000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_DEFAULT_BYTE_SIZE\" : \"16777216\", #\"16777216000\",\n",
    "        \"SAGEMAKER_TRITON_SHM_GROWTH_BYTE_SIZE\": \"1048576\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageGroupDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    ")\n",
    "print(f\"Model Registry package group: {model_pacakge_group_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_version_response = sm_client.create_model_package(\n",
    "    ModelPackageGroupName=str(model_package_group_name),\n",
    "    ModelPackageDescription=\"BERT large uncased Model group for Triton Serving\",\n",
    "    Domain=ml_domain,\n",
    "    Task=ml_task,\n",
    "    SamplePayloadUrl=s3_sample_data_path_triton,\n",
    "    InferenceSpecification={\n",
    "        \"Containers\": [container_infrec_mme],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\n",
    "            \"ml.g4dn.4xlarge\",\n",
    "            \"ml.g4dn.4xlarge\",\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\"application/octet-stream\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"],\n",
    "    },\n",
    ")\n",
    "model_package_version_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_response = sm_client.create_inference_recommendations_job(\n",
    "    JobName=advanced_job,\n",
    "    JobDescription=\"nlp triton Inference Advanced Recommender Job\",\n",
    "    JobType=\"Advanced\",\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        \"ModelPackageVersionArn\": model_package_version_response[\"ModelPackageArn\"],\n",
    "        \"JobDurationInSeconds\": 7200,\n",
    "        \"EndpointConfigurations\": [\n",
    "            #{\"InstanceType\": \"ml.p3.8xlarge\"},\n",
    "            #{\"InstanceType\": \"ml.p3.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.p2.16xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.8xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.4xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.2xlarge\"},\n",
    "            {\"InstanceType\": \"ml.g4dn.12xlarge\"},\n",
    "        ],\n",
    "        \"TrafficPattern\": {\n",
    "            \"TrafficType\": \"PHASES\",\n",
    "            \"Phases\": [\n",
    "                {\n",
    "                    \"InitialNumberOfUsers\": 2,\n",
    "                    \"SpawnRate\": 3,\n",
    "                    \"DurationInSeconds\": 900,\n",
    "                },  # simulating 50 users, 2 initial and 3 new users every minute for 16 minutes\n",
    "            ],  # second phase, we will strt with 50 users, steady traffic for 5 minutes\n",
    "        },\n",
    "        \"ResourceLimit\": {\"MaxNumberOfTests\": 10, \"MaxParallelOfTests\": 5},\n",
    "    },\n",
    "    StoppingConditions={\n",
    "        \"MaxInvocations\": 30000,\n",
    "        \"ModelLatencyThresholds\": [{\"Percentile\": \"P95\", \"ValueInMilliseconds\": 500}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(advanced_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ended = False\n",
    "while not ended:\n",
    "    inference_recommender_job = sm_client.describe_inference_recommendations_job(\n",
    "        JobName=str(advanced_job)\n",
    "    )\n",
    "    if inference_recommender_job[\"Status\"] in [\"COMPLETED\", \"STOPPED\", \"FAILED\"]:\n",
    "        print(f\"Inference recommender job status: {inference_recommender_job['Status']} \")\n",
    "        ended = True\n",
    "    else:\n",
    "        print(\"Inference recommender job in progress\")\n",
    "        time.sleep(300)\n",
    "\n",
    "if inference_recommender_job[\"Status\"] == \"FAILED\":\n",
    "    print(\"Inference recommender job failed \")\n",
    "    print(\"Failed Reason: {}\".inference_recommender_job[\"FailedReason\"])\n",
    "else:\n",
    "    print(\"Inference recommender job completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f84f1",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e4899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49178cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name_p5_mme)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name_p5_mme)\n",
    "sm_client.delete_model(ModelName=endpoint_name_p5_mme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9238a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
