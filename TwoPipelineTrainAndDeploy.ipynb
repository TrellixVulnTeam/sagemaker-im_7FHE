{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Manual to Repeatable Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Training pipeline with SageMaker Pipelines](#Training-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Pipeline inputs](#Pipeline-inputs)\n",
    "    - [SageMaker Processing step](#SageMaker-Processing-step)\n",
    "    - [SageMaker Training step](#SageMaker-Training-step)\n",
    "    - [Model evaluation step](#Model-evaluation-step)\n",
    "    - [Register model in Model Registry step](#Register-model-in-Model-Registry-step)\n",
    "    - [Assemble the training pipeline](#Assemble-the-training-pipeline)\n",
    "    - [Execute the training pipeline](#Execute-the-training-pipeline)\n",
    "- [Deployment pipeline with SageMaker Pipelines](#Deployment-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Assemble the deployment pipeline](#Assemble-the-deployment-pipeline)\n",
    "    - [Execute the deployment pipeline](#Execute-the-deployment-pipeline)\n",
    "    - [Test the SageMaker endpoint](#Test-the-SageMaker-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook which will explore the orchestration stage of ML workflow. Please see the diagram below for the various maturity stages of MLOps\n",
    "\n",
    "Here, we will put on the hat of a `DevOps/MLOps Engineer` and perform the task of orchestration which includes building pipeline steps that include all the various ML Workflows components into one singular entity. This pipeline entity accomplishes a repeatable and reliable orchestration of each step in the ML workflow.\n",
    "\n",
    "![](./images/mlops-maturity.png)\n",
    "\n",
    "For this task we will be using Amazon SageMaker Pipeline capabilities.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing imports\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# SageMaker Pipeline imports\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "\n",
    "# To test the endpoint once it's deployed\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer, CSVDeserializer\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "import pandas as pd\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful SageMaker variables\n",
    "session = PipelineSession()\n",
    "bucket = session.default_bucket()\n",
    "role_arn= sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful variable\n",
    "output_path=f's3://{bucket}/mlops-workshop/data/sm_processed'  \n",
    "model_package_group_name = 'synthetic-housing-models'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Raw Data set\n",
    "Upload the Raw data set for processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload raw data to S3\n",
    "raw_data_s3_prefix = 'mlops_workshop/data/raw'\n",
    "raw_s3 = session.upload_data(path='./data/raw/house_pricing.csv', key_prefix=raw_data_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_iam_role(role_name):\n",
    "    iam = boto3.client(\"iam\")\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "        print(\"Done\")\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_role = create_lambda_iam_role('LambdaSageMakerExecutionRole')\n",
    "print(lambda_role)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Amazon [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) pipeline is a series of interconnected steps that is defined by a JSON pipeline definition. This pipeline definition encodes a pipeline using a directed acyclic graph (DAG). This DAG gives information on the requirements for and relationships between each step of your pipeline. The structure of a pipeline's DAG is determined by the data dependencies between steps. These data dependencies are created when the properties of a step's output are passed as the input to another step. The following image is a pipeline DAG that we'll be creating for our training pipeline:\n",
    "\n",
    "![](./images/sagemaker-pipelines-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can give a pipeline inputs to make it reusable (you'll be able to override these inputs upon executing the pipeline later in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(\n",
    "    name='ProcessingInstanceCount',\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name='ProcessingInstanceType',\n",
    "    default_value='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `ProcessingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p pipeline_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline_scripts/preprocessing.py\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_parameters():\n",
    "    \"\"\"\n",
    "    Read job parameters\n",
    "    Returns:\n",
    "        (Namespace): read parameters\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_size', type=float, default=0.6)\n",
    "    parser.add_argument('--val_size', type=float, default=0.2)\n",
    "    parser.add_argument('--test_size', type=float, default=0.2)\n",
    "    parser.add_argument('--random_state', type=int, default=42)\n",
    "    parser.add_argument('--target_col', type=str, default='PRICE')\n",
    "    params, _ = parser.parse_known_args()\n",
    "    return params\n",
    "\n",
    "\n",
    "def change_target_to_first_col(df, target_col):\n",
    "    # shift column 'PRICE' to first position\n",
    "    first_column = df.pop(target_col)\n",
    "  \n",
    "    # insert column using insert(position,column_name,\n",
    "    # first_column) function\n",
    "    df.insert(0, target_col, first_column)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_dataset(df, train_size, val_size, test_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation and test samples\n",
    "    Args:\n",
    "        df (pandas.DataFrame): input data\n",
    "        train_size (float): ratio of data to use as training dataset\n",
    "        val_size (float): ratio of data to use as validation dataset\n",
    "        test_size (float): ratio of data to use as test dataset\n",
    "        random_state (int): Pass an int for reproducible output across multiple function calls.\n",
    "    Returns:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "    \"\"\"\n",
    "    if (train_size + val_size + test_size) != 1.0:\n",
    "        raise ValueError(\"train_size, val_size and test_size must sum up to 1.0\")\n",
    "    rest_size = 1 - train_size\n",
    "    df_train, df_rest = train_test_split(\n",
    "        df,\n",
    "        test_size=rest_size,\n",
    "        train_size=train_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_rest,\n",
    "        test_size=(test_size / rest_size),\n",
    "        train_size=(val_size / rest_size),\n",
    "        random_state=random_state\n",
    "    )\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_val.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    train_perc = int((len(df_train)/len(df)) * 100)\n",
    "    print(f\"Training size: {len(df_train)} - {train_perc}% of total\")\n",
    "    val_perc = int((len(df_val)/len(df)) * 100)\n",
    "    print(f\"Val size: {len(df_val)} - {val_perc}% of total\")\n",
    "    test_perc = int((len(df_test)/len(df)) * 100)\n",
    "    print(f\"Test size: {len(df_test)} - {test_perc}% of total\")\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def scale_dataset(df_train, df_val, df_test, target_col):\n",
    "    \"\"\"\n",
    "    Fit StandardScaler to df_train and apply it to df_val and df_test\n",
    "    Args:\n",
    "        df_train (pandas.DataFrame): train dataset\n",
    "        df_val (pandas.DataFrame): validation dataset\n",
    "        df_test (pandas.DataFrame): test dataset\n",
    "        target_col (str): target col\n",
    "    Returns:\n",
    "        df_train_transformed (pandas.DataFrame): train data scaled\n",
    "        df_val_transformed (pandas.DataFrame): val data scaled\n",
    "        df_test_transformed (pandas.DataFrame): test data scaled\n",
    "    \"\"\"\n",
    "    scaler_data = StandardScaler()\n",
    "    \n",
    "    # fit scaler to training dataset\n",
    "    print(\"Fitting scaling to training data and transforming dataset...\")\n",
    "    df_train_transformed = pd.DataFrame(\n",
    "        scaler_data.fit_transform(df_train), \n",
    "        columns=df_train.columns\n",
    "    )\n",
    "    df_train_transformed[target_col] = df_train[target_col]\n",
    "    \n",
    "    # apply scaler to validation and test datasets\n",
    "    print(\"Transforming validation and test datasets...\")\n",
    "    df_val_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_val), \n",
    "        columns=df_val.columns\n",
    "    )\n",
    "    df_val_transformed[target_col] = df_val[target_col]\n",
    "    df_test_transformed = pd.DataFrame(\n",
    "        scaler_data.transform(df_test), \n",
    "        columns=df_test.columns\n",
    "    )\n",
    "    df_test_transformed[target_col] = df_test[target_col]\n",
    "    return df_train_transformed, df_val_transformed, df_test_transformed\n",
    "\n",
    "\n",
    "print(f\"===========================================================\")\n",
    "print(f\"Starting pre-processing\")\n",
    "print(f\"Reading parameters\")\n",
    "\n",
    "# reading job parameters\n",
    "args = read_parameters()\n",
    "print(f\"Parameters read: {args}\")\n",
    "\n",
    "# set input and output paths\n",
    "input_data_path = \"/opt/ml/processing/input/house_pricing.csv\"\n",
    "train_data_path = \"/opt/ml/processing/output/train\"\n",
    "val_data_path = \"/opt/ml/processing/output/validation\"\n",
    "test_data_path = \"/opt/ml/processing/output/test\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(train_data_path)\n",
    "    os.makedirs(val_data_path)\n",
    "    os.makedirs(test_data_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# read data input\n",
    "df = pd.read_csv(input_data_path)\n",
    "\n",
    "# move target to first col\n",
    "df = change_target_to_first_col(df, args.target_col)\n",
    "\n",
    "# split dataset into train, validation and test\n",
    "df_train, df_val, df_test = split_dataset(\n",
    "    df,\n",
    "    train_size=args.train_size,\n",
    "    val_size=args.val_size,\n",
    "    test_size=args.test_size,\n",
    "    random_state=args.random_state\n",
    ")\n",
    "\n",
    "# scale datasets\n",
    "df_train_transformed, df_val_transformed, df_test_transformed = scale_dataset(\n",
    "    df_train, \n",
    "    df_val, \n",
    "    df_test,\n",
    "    args.target_col\n",
    ")\n",
    "\n",
    "print(\"Saving data\")\n",
    "df_train_transformed.to_csv(train_data_path+'/train.csv', sep=',', index=False, header=False)\n",
    "df_val_transformed.to_csv(val_data_path+'/validation.csv', sep=',', index=False, header=False)\n",
    "df_test_transformed.to_csv(test_data_path+'/test.csv', sep=',', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ending pre-processing\")\n",
    "print(f\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='preprocess-data',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "preprocess_dataset_step = ProcessingStep(\n",
    "    name='PreprocessData',\n",
    "    code='./pipeline_scripts/preprocessing.py',\n",
    "    processor=preprocess_data_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_s3,\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            destination=f'{output_path}/train',\n",
    "            source='/opt/ml/processing/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            destination=f'{output_path}/validation',\n",
    "            source='/opt/ml/processing/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            destination=f'{output_path}/test',\n",
    "            source='/opt/ml/processing/test'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `TrainingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"7\",\n",
    "    \"gamma\": \"2\",\n",
    "    \"alpha\": \"375\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "    \"eval_metric\": \"mse\"\n",
    "}\n",
    "\n",
    "train_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "#xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role_arn,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    volume_size=5, # 5 GB \n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='TrainModel',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'train'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'validation'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training step in our pipeline, we'll want to then evaluate our model's performance. To do that, we can create a SageMaker Processing Step and pass in some code to do the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline_scripts/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        'regression_metrics': {\n",
    "            'mae': {\n",
    "                'value': mae,\n",
    "                'standard_deviation': std,\n",
    "            },\n",
    "            'mse': {\n",
    "                'value': mse,\n",
    "                'standard_deviation': std,\n",
    "            },\n",
    "            'rmse': {\n",
    "                'value': rmse,\n",
    "                'standard_deviation': std,\n",
    "            },\n",
    "            'r2': {\n",
    "                'value': r2,\n",
    "                'standard_deviation': std,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=xgboost_container,\n",
    "    command=[\"python3\"],\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='evaluation',\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where we'll store the model evaluation results so\n",
    "# that other steps can access those results\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model',\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test',\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation', source='/opt/ml/processing/evaluation'\n",
    "        ),\n",
    "    ],\n",
    "    code='./pipeline_scripts/evaluation.py',\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register model in Model Registry step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've evaluated the model's peformance, we'll want to register the model in a Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri='{}/evaluation.json'.format(\n",
    "            evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output'][\n",
    "                'S3Uri'\n",
    "            ]\n",
    "        ),\n",
    "        content_type='application/json',\n",
    "    )\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=estimator.training_image_uri(),\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    entry_point=estimator.entry_point,\n",
    "    role=role_arn,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "model_registry_args = model.register(\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['application/json'],\n",
    "    inference_instances=['ml.t2.medium', 'ml.m5.xlarge'],\n",
    "    transform_instances=['ml.m5.xlarge'],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status='PendingManualApproval',\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name='RegisterModel',\n",
    "    step_args=model_registry_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we'll only want to register the model if its performance meets a predefined threshold that we set. So let's create a Condition Step that says if our model's MSE values is less than 80000000.0, then we'll registery the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition step for evaluating model quality and branching execution\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path='regression_metrics.mse.value',\n",
    "    ),\n",
    "    right=80000000.0,\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name='CheckEvaluation',\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though easier to reason with, the parameters and steps don't need to be in order. The pipeline DAG will parse it out properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-training-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-training-pipeline'\n",
    "step_list = [preprocess_dataset_step,\n",
    "             training_step,\n",
    "             evaluation_step,\n",
    "             condition_step]\n",
    "\n",
    "training_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        processing_instance_type\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "training_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "#json.loads(training_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = training_pipeline.start(\n",
    "    parameters = {\n",
    "        'ProcessingInstanceType': 'ml.m5.large'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 2 -Deployment pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a separate pipeline that will take the model that was registered in Model Registry and deploy it as a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll specify the input parameters to our deployment pipeline so that we can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline_scripts/lambda_deploy.py\n",
    "'''\n",
    "This Lambda function creates an Endpoint Configuration and deploys a model to an Endpoint. \n",
    "The name of the model to deploy is provided via the `event` argument\n",
    "'''\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(f'boto3 version: {boto3.__version__}')\n",
    "print(f'region: {region}')\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Approve the model package for deployment, create a SageMaker model\n",
    "    \"\"\"\n",
    "    \n",
    "    region = event['region']\n",
    "    aws_account_id = event['aws_account_id']\n",
    "    model_package_group_name = event['model_package_group_name']\n",
    "    instance_count = event['instance_count']\n",
    "    role_arn = event['role_arn']\n",
    "    \n",
    "    # Optional fields\n",
    "    try:\n",
    "        model_package_version = event['model_package_version']\n",
    "    except:\n",
    "        # Get the latest version\n",
    "        model_package_version = sm_client.list_model_packages(ModelPackageGroupName=model_package_group_name)['ModelPackageSummaryList'][0]['ModelPackageVersion']\n",
    "        \n",
    "    try:\n",
    "        model_name = event['model_name']\n",
    "    except:\n",
    "        model_name = f'{model_package_group_name}-model-{model_package_version}'\n",
    "    \n",
    "    model_package_version_arn = f'arn:aws:sagemaker:{region}:{aws_account_id}:model-package/{model_package_group_name}/{model_package_version}'\n",
    "    print(f'Using model package version ARN: {model_package_version_arn}')\n",
    "    model_package_details = sm_client.describe_model_package(ModelPackageName=model_package_version_arn)\n",
    "\n",
    "    realtime_inference_instance_types = model_package_details['InferenceSpecification']['SupportedRealtimeInferenceInstanceTypes']\n",
    "    \n",
    "    container_list = [{'ModelPackageName': model_package_version_arn}]\n",
    "    \n",
    "    # Approve model package to be used as SageMaker model and for deployment\n",
    "    sm_client.update_model_package(ModelPackageArn=model_package_version_arn,\n",
    "                                   ModelApprovalStatus='Approved')\n",
    "    \n",
    "    endpoint_name = f'{model_name}-endpoint'\n",
    "    print(f\"Lambda:Model:NAME:PASSED:{model_name}:\")\n",
    "    \n",
    "    # Delete in case this pipeline runs again\n",
    "    #  We have to keep the model name the same since we do not return the Model name here\n",
    "    try:\n",
    "        sm_client.delete_model(ModelName=model_name)\n",
    "        print(f\"Lambda:Model:{model_name}::DELETED::\")\n",
    "    except:\n",
    "        print(f\"Lambda:IGNORE::ERROR:Delete:model:Probably First RUN:{traceback.format_exc()}:\")\n",
    "    try:\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(f\"Lambda:EndPointConfig:{endpoint_name}::DELETED::\")\n",
    "    except:\n",
    "        print(f\"Lambda:IGNORE::ERROR:Delete:EndpointConfig:Probably First RUN:{traceback.format_exc()}:\")\n",
    "    try:\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Lambda:EndPoint:{endpoint_name}::DELETED:\")\n",
    "        \n",
    "        time.sleep(60) # sleep for 60 seconds to ensure the model end point is indeed deleted if it was there\n",
    "    except:\n",
    "        print(f\"Lambda:IGNORE::ERROR:Delete:EndPoint:Probably First RUN:{traceback.format_exc()}:\")\n",
    "        \n",
    "    print(f\"Lambda:Creating:endpoint:{endpoint_name}\")\n",
    "    sm_client.create_model(ModelName=model_name,\n",
    "                           Containers=container_list,\n",
    "                           ExecutionRoleArn=role_arn)\n",
    "\n",
    "    \n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                'InstanceType': realtime_inference_instance_types[0],\n",
    "                'InitialVariantWeight': 1,\n",
    "                'InitialInstanceCount': instance_count,\n",
    "                'ModelName': model_name,\n",
    "                'VariantName': 'AllTraffic',\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_name)\n",
    "    print(f\"Lambda:end:point:create_endpoint_response={create_endpoint_response}:\")\n",
    "    time.sleep(60) # sleep for 60 seconds to ensure the end point is created\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        #'body': json.dumps('Created Endpoint!')\n",
    "        'body': json.dumps(create_endpoint_response)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize the Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ParameterString(\n",
    "    name='ModelName',\n",
    "    default_value='my-awesome-model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a Lambda function that will pull the specified model (or latest model) from the Model Registry and deploy as a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name = 'sagemaker-pipelines-deploy-model'\n",
    "lambda_role = \"arn:aws:iam::622343165275:role/sagemaker-lambda-exec\"\n",
    "import traceback\n",
    "lambda_function = Lambda(\n",
    "    function_name=lambda_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script='./pipeline_scripts/lambda_deploy.py',\n",
    "    handler='lambda_deploy.lambda_handler',\n",
    "    timeout=600,\n",
    "    memory_size=3000,\n",
    ")\n",
    "try:\n",
    "    lambda_function.delete()\n",
    "except:\n",
    "    print('Lambda function Does Not exits:First run: IGNORE Error')\n",
    "    print(traceback.format_exc())\n",
    "try:\n",
    "    lambda_function_response = lambda_function.create()\n",
    "    lambda_function_arn = lambda_function_response['FunctionArn']\n",
    "    print(f'Lambda function arn: {lambda_function_arn}')\n",
    "except:\n",
    "    print('Lambda function already exists!')\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a Lambda step for our pipeline and associate it with the new Lambda function we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name='statusCode', output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name='body', output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "deploy_lambda_step = LambdaStep(\n",
    "    name='LambdaStepDeploy',\n",
    "    lambda_func=lambda_function,\n",
    "    inputs={\n",
    "        'region': region,\n",
    "        'aws_account_id': aws_account_id,\n",
    "        'model_package_group_name': model_package_group_name,\n",
    "        'model_name': model_name,\n",
    "        'instance_count': 1,\n",
    "        'role_arn': role_arn\n",
    "    },\n",
    "    outputs=[\n",
    "        output_param_1, \n",
    "        output_param_2\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, now we just need to assemble the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-deployment-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-deployment-pipeline'\n",
    "step_list = [deploy_lambda_step]\n",
    "\n",
    "deployment_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        model_name\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "deployment_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "json.loads(deployment_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_name = 'my-xgboost-model'\n",
    "lambda_execution = deployment_pipeline.start(\n",
    "    parameters = {\n",
    "        'ModelName': deployed_model_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the End point to be in service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=f'{deployed_model_name}-endpoint')\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Endpoint:Status: \" + status)\n",
    "\n",
    "sagemaker_client.get_waiter(\"endpoint_in_service\").wait(EndpointName=f'{deployed_model_name}-endpoint')\n",
    "\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=f'{deployed_model_name}-endpoint')\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"EndPoint:Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"EndPoint:Status: \" + status)\n",
    "\n",
    "if status != \"InService\":\n",
    "    raise Exception(\"Endpoint creation did not succeed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now send some data to the endpoint and test it is working properly.\n",
    "\n",
    "For this, we first load our test data from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"./data/test/test.csv\"\n",
    "# read data input\n",
    "df = pd.read_csv(test_data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we query the endpoint once it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "response_status = 'None'\n",
    "while response_status != 'InService':\n",
    "    if response_status != 'None':\n",
    "        time.sleep(120) # wait until endpoint is in service\n",
    "    response = sagemaker_client.describe_endpoint(EndpointName=f'{deployed_model_name}-endpoint')\n",
    "    response_status = response['EndpointStatus']\n",
    "# Attach to the SageMaker endpoint\n",
    "predictor = Predictor(endpoint_name=f'{deployed_model_name}-endpoint',\n",
    "                      sagemaker_session=session,\n",
    "                      serializer=CSVSerializer(),\n",
    "                      deserializer=CSVDeserializer())\n",
    "\n",
    "# Get a real-time prediction\n",
    "predictor.predict(df.drop(columns=[\"PRICE\"]).to_csv(index=False, header=False))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
